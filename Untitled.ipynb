{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb0d8b1-bcee-45cb-876c-e93b4440be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build as build\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73bcdcc1-91ff-474a-9078-eb081ba89a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "api1= \"AIzaSyClj6OEqZ0jR8-Lf8yFuEQI-1I3WA-U7yo\"\n",
    "api2=\"AIzaSyCstjF-sUzZWALRQEtqNPeEy0Z5P6_UvUc\"\n",
    "api_key = api1\n",
    "channel_id = \"UClM7fatYZhgbMqpqyqHmKAw\"\n",
    "youtube = build('youtube','v3',developerKey = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58137f32-4aae-4444-9a2e-d053ffb7fec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ChanneID': 'UClM7fatYZhgbMqpqyqHmKAw',\n",
       " 'channel_name': 'Clever Studies',\n",
       " 'SubscriberCount': '10200',\n",
       " 'ViewCount': '757938',\n",
       " 'VideoCount': '210'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = youtube.channels().list(\n",
    "    id=channel_id,\n",
    "    part='snippet,statistics,contentDetails'\n",
    ")\n",
    "channel_data = response.execute()\n",
    "data = dict(ChanneID=channel_data['items'][0]['id'],\n",
    "            channel_name=channel_data['items'][0]['snippet']['title'],\n",
    "            SubscriberCount=channel_data['items'][0]['statistics']['subscriberCount'],\n",
    "            ViewCount=channel_data['items'][0]['statistics']['viewCount'],\n",
    "            VideoCount=channel_data['items'][0]['statistics']['videoCount']           \n",
    "            )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00f7ea3a-8be3-4320-b641-51bf86dfacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e04c203-a0a7-436b-9628-67aa000d95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playls(ch_id):\n",
    "    chid = ch_id\n",
    "    token = ''\n",
    "    plylist_ids, ch_ls, ti_ls = [], [], []\n",
    "    while token != None:\n",
    "        request = youtube.playlists().list(\n",
    "            part=\"contentDetails,snippet\",\n",
    "            channelId=str(ch_id),\n",
    "            maxResults=10,\n",
    "            pageToken=token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        [plylist_ids.append(x['id']) for x in response['items']]\n",
    "        [ch_ls.append(x['snippet']['channelId']) for x in response['items']]\n",
    "        [ti_ls.append(x['snippet']['title']) for x in response['items']]\n",
    "\n",
    "        try:\n",
    "            token = response['nextPageToken']\n",
    "        except KeyError as e:\n",
    "            token = None\n",
    "    playlist = {'Playlist_id': plylist_ids,\n",
    "                'ChannelID': ch_ls,\n",
    "                'Playlist_name': ti_ls\n",
    "                }\n",
    "    return playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ee63982-d852-41f2-9eab-7f0b539f86ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PLCLE6UVwCOi27kuohUuFjmP2lgN-fcMMW',\n",
       " 'PLCLE6UVwCOi1Kgg2EbiPk02H9Gl6FdSun',\n",
       " 'PLCLE6UVwCOi2O54HUd_TkUvl138qb-fNS',\n",
       " 'PLCLE6UVwCOi3dPoJKZbBJANysR38m-TEl',\n",
       " 'PLCLE6UVwCOi1r-LnSDyM-efVK9NpWQMEt',\n",
       " 'PLCLE6UVwCOi0qixsdEyJTF72ePkODM7RJ',\n",
       " 'PLCLE6UVwCOi2tdBA8Xq0BwqIbuBaocEgL',\n",
       " 'PLCLE6UVwCOi1_4FSzJDv4Ac_mqQYvZRSU',\n",
       " 'PLCLE6UVwCOi39c70e9JXxqIC9irN9oNHP',\n",
       " 'PLCLE6UVwCOi1u9ZVEFxF6lwWfU0OlrrVU',\n",
       " 'PLCLE6UVwCOi1UpKQkFwyUcPaTaoXeRuSQ',\n",
       " 'PLCLE6UVwCOi2uUGY-qeGTFSK5WWU-QEEw',\n",
       " 'PLCLE6UVwCOi0J0SrU2pNdGpACEQh4kqbC',\n",
       " 'PLCLE6UVwCOi0Ia99KOssDrdfXMMCMnUdU',\n",
       " 'PLCLE6UVwCOi1_uZwcpu8AS5_DGpuQwodr',\n",
       " 'PLCLE6UVwCOi3kg3nCtaJgjfEcR8MJdndi',\n",
       " 'PLCLE6UVwCOi3jZp6JP9Q9qIKqJ1sFuaLC',\n",
       " 'PLCLE6UVwCOi1DsZIULI7pRuelN6SaSoGd',\n",
       " 'PLCLE6UVwCOi1FRysr-OA6UM_kl2Suoubn',\n",
       " 'PLCLE6UVwCOi0kbQ8NzLRWBKGBINP8h8qV']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_ids = playls(channel_id)['Playlist_id']\n",
    "playlist_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "167eaaa5-1c74-4792-af79-24c60627acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videodls(plylist_ids):\n",
    "\n",
    "    Video_id, playlist_Id, Video_name, Published_date, View_count, Like_count, Comments_count, Duration = [], [], [], [], [], [], [], []\n",
    "    for playlist in plylist_ids:\n",
    "        token = ''\n",
    "        videoids = []\n",
    "        while token != None:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"contentDetails\",\n",
    "                playlistId=playlist,\n",
    "                maxResults=10,\n",
    "                pageToken=token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            [videoids.append(item['contentDetails']['videoId']) for item in response['items']]\n",
    "            try:\n",
    "                token = response['nextPageToken']\n",
    "            except KeyError as e:\n",
    "                token = None\n",
    "\n",
    "            batch_size = 50\n",
    "            for i in range(0, len(videoids), batch_size):\n",
    "                batch = videoids[i:i + batch_size]\n",
    "\n",
    "                video_response = youtube.videos().list(\n",
    "                    part='snippet,statistics,contentDetails',\n",
    "                    id=batch\n",
    "                ).execute()\n",
    "                video_response\n",
    "\n",
    "                Video_id.extend([item['id'] for item in video_response['items']])\n",
    "                playlist_Id.extend([playlist for item in video_response['items']])\n",
    "                Video_name.extend([remove_emojis(item['snippet']['title']) for item in video_response['items']])\n",
    "                Published_date.extend([dt.strptime(item['snippet']['publishedAt'].replace('T', ' ').replace('Z', ''),\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\").strftime(\"%d/%m/%Y %I:%M %p\")\n",
    "                                       for item in video_response['items']])\n",
    "                View_count.extend([item['statistics'].get('viewCount',0) for item in video_response['items']])\n",
    "                Like_count.extend([item['statistics'].get('likeCount', 0) for item in video_response['items']])\n",
    "                Comments_count.extend([int(item['statistics'].get('commentCount', 0)) for item in video_response['items']])\n",
    "                Duration.extend([item['contentDetails']['duration'].replace('M', ':').replace('H', ':')\n",
    "                                .replace('PT','').replace('S', '') if len(item['contentDetails']['duration']) > 5 else\n",
    "                                 item['contentDetails']['duration'].replace('M', '').replace('PT', '').replace('S', '')\n",
    "                                 for item in video_response['items']])\n",
    "\n",
    "\n",
    "    videoDetail = {'Video_id': Video_id,\n",
    "                   'Playlist_id': playlist_Id,\n",
    "                   'Video_name': Video_name,\n",
    "                   'Published_date': Published_date,\n",
    "                   'View_count': View_count,\n",
    "                   'Like_count': Like_count,\n",
    "                   'Comment_count': Comments_count,\n",
    "                   'Duration': Duration\n",
    "\n",
    "                   }\n",
    "\n",
    "    return videoDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0dab6bc-3273-43cb-b1e1-98f0caa300d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vidsl= videodls(playlist_ids)\n",
    "videoids = vidsl['Video_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "504416f9-a168-499b-8086-aa78cbb427de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j-P2-fJx_qA',\n",
       " '7pEoD2WYekg',\n",
       " 'AmtFLw9Cu0w',\n",
       " 'q9rL_Igpby0',\n",
       " 'vdQDLMHT7-A',\n",
       " 'oB1ohIIhKUA',\n",
       " 'cM8ZdsD5ohY',\n",
       " 'eUTOT4W4dOw',\n",
       " 'LLDGg_41gWE',\n",
       " 'cro3wTg9ma4',\n",
       " 'bTnS1t8X0oc',\n",
       " 'CuNAUituP0I',\n",
       " 'FkOGirOe3FI',\n",
       " 'jytrP1wA_Wc',\n",
       " 'vdQDLMHT7-A',\n",
       " 'oB1ohIIhKUA',\n",
       " 'cM8ZdsD5ohY',\n",
       " 'eUTOT4W4dOw',\n",
       " 'LLDGg_41gWE',\n",
       " 'cro3wTg9ma4',\n",
       " 'bTnS1t8X0oc',\n",
       " 'CuNAUituP0I',\n",
       " 'FkOGirOe3FI',\n",
       " 'jytrP1wA_Wc',\n",
       " 'qzRJUYMPvtI',\n",
       " '-MTRoSVYHVg',\n",
       " 'FE7qXO4792Y',\n",
       " 'oTryp1Q5pR4',\n",
       " 'K7jCVhf69mI',\n",
       " 'wtRtbz2_EuI',\n",
       " 'qU8hFKZFTow',\n",
       " 'AJTVH-SCkvs',\n",
       " '7AcmyGpwJdc',\n",
       " 'bIKDtN7Tb5w',\n",
       " 'ftTHw8PXjy4',\n",
       " 'FE7qXO4792Y',\n",
       " 'oTryp1Q5pR4',\n",
       " 'K7jCVhf69mI',\n",
       " 'wtRtbz2_EuI',\n",
       " 'qU8hFKZFTow',\n",
       " 'AJTVH-SCkvs',\n",
       " '7AcmyGpwJdc',\n",
       " 'bIKDtN7Tb5w',\n",
       " 'ftTHw8PXjy4',\n",
       " '-WfD5WkYuI4',\n",
       " 'scMTPdplCr8',\n",
       " 'DjC_NQXN8jA',\n",
       " 'l6wbwznQv28',\n",
       " 'pgjrB3LhoaU',\n",
       " 'NBRgk_r39pM',\n",
       " '_ziv1ewbZss',\n",
       " 'L_mk4SZgXq8',\n",
       " 'wgJ9WX1ft28',\n",
       " '2GKw0UBGWZc',\n",
       " 'JZqbddpY-xI',\n",
       " 'a5j5mTdSCtY',\n",
       " 'syJ7-IoD9Z8',\n",
       " 'XDCtldW4eLk',\n",
       " 'CMEx0xYn7i8',\n",
       " 'ggEikhQfM6A',\n",
       " 'Xy7N0YRzml8',\n",
       " 'XBDEQdwFyo8',\n",
       " 'wgJ9WX1ft28',\n",
       " '2GKw0UBGWZc',\n",
       " 'JZqbddpY-xI',\n",
       " 'a5j5mTdSCtY',\n",
       " 'syJ7-IoD9Z8',\n",
       " 'XDCtldW4eLk',\n",
       " 'CMEx0xYn7i8',\n",
       " 'ggEikhQfM6A',\n",
       " 'Xy7N0YRzml8',\n",
       " 'XBDEQdwFyo8',\n",
       " '5G_CqUP2Djs',\n",
       " 'TQ0iTDkqL7s',\n",
       " '7yBntXxzfhs',\n",
       " 'g_y7aUy6bNE',\n",
       " 'ZsKWCP-fPEU',\n",
       " 'HRC5NiGcBo4',\n",
       " 'nMB4bU2YZvI',\n",
       " 'ZYxvcPZ9oW8',\n",
       " 'W2GgqXIR-UM',\n",
       " 'CRH47LN3Oew',\n",
       " '5EZSh3A0xL4',\n",
       " 'BbXZ8bgzIwo',\n",
       " 'zP2U4RgeesM',\n",
       " 'gdK5lNhp0PI',\n",
       " 'JEW0sndX_A0',\n",
       " 'fRqmNGip8to',\n",
       " 'dPCdlo3rdGc',\n",
       " 'PeUIq0q3N4s',\n",
       " 'TFFjJczEWfs',\n",
       " 'tcGjl5PZf94',\n",
       " 'VbqRLDfMR1s',\n",
       " '3pmMGhzFCd4',\n",
       " 'dNuKPV4bpPU',\n",
       " '1jbMgaEbIbs',\n",
       " 'yf7c4xrfsoI',\n",
       " 'IET_NrzaJSE',\n",
       " 'yvsa1bQmWJc',\n",
       " 'H9QqDs86_p4',\n",
       " 'tcGjl5PZf94',\n",
       " 'VbqRLDfMR1s',\n",
       " '3pmMGhzFCd4',\n",
       " 'dNuKPV4bpPU',\n",
       " '1jbMgaEbIbs',\n",
       " 'yf7c4xrfsoI',\n",
       " 'IET_NrzaJSE',\n",
       " 'yvsa1bQmWJc',\n",
       " 'H9QqDs86_p4',\n",
       " '7Ocjh-j-oBs',\n",
       " 'xo-0Vrj9ZdY',\n",
       " 'fh8YA7MDVK8',\n",
       " 'QHfxJVl7s5w',\n",
       " 'GI7zgAwEFoE',\n",
       " '0IpCfCnTTEU',\n",
       " 'xZatN-njBUQ',\n",
       " 'fX5BAlIv5Eo',\n",
       " 'CIj7llf_66o',\n",
       " 'AxFSoYVw5c0',\n",
       " 'tcGjl5PZf94',\n",
       " 'VbqRLDfMR1s',\n",
       " '3pmMGhzFCd4',\n",
       " 'dNuKPV4bpPU',\n",
       " '1jbMgaEbIbs',\n",
       " 'yf7c4xrfsoI',\n",
       " 'IET_NrzaJSE',\n",
       " 'yvsa1bQmWJc',\n",
       " 'H9QqDs86_p4',\n",
       " '7Ocjh-j-oBs',\n",
       " 'xo-0Vrj9ZdY',\n",
       " 'fh8YA7MDVK8',\n",
       " 'QHfxJVl7s5w',\n",
       " 'GI7zgAwEFoE',\n",
       " '0IpCfCnTTEU',\n",
       " 'xZatN-njBUQ',\n",
       " 'fX5BAlIv5Eo',\n",
       " 'CIj7llf_66o',\n",
       " 'AxFSoYVw5c0',\n",
       " 'B9eiN3aZjPs',\n",
       " '-4HTlLiPQxY',\n",
       " 'J8Ku16yIVdA',\n",
       " 'rlYm9EWNNhY',\n",
       " 'MS323_BoGAM',\n",
       " 'fSJOdUyIZU8',\n",
       " 'jCnSG78xOgM',\n",
       " 'k3NNWQGgaPU',\n",
       " 'rXhXWqcVvDE',\n",
       " 'awSCxnk39FU',\n",
       " 'tcGjl5PZf94',\n",
       " 'VbqRLDfMR1s',\n",
       " '3pmMGhzFCd4',\n",
       " 'dNuKPV4bpPU',\n",
       " '1jbMgaEbIbs',\n",
       " 'yf7c4xrfsoI',\n",
       " 'IET_NrzaJSE',\n",
       " 'yvsa1bQmWJc',\n",
       " 'H9QqDs86_p4',\n",
       " '7Ocjh-j-oBs',\n",
       " 'xo-0Vrj9ZdY',\n",
       " 'fh8YA7MDVK8',\n",
       " 'QHfxJVl7s5w',\n",
       " 'GI7zgAwEFoE',\n",
       " '0IpCfCnTTEU',\n",
       " 'xZatN-njBUQ',\n",
       " 'fX5BAlIv5Eo',\n",
       " 'CIj7llf_66o',\n",
       " 'AxFSoYVw5c0',\n",
       " 'B9eiN3aZjPs',\n",
       " '-4HTlLiPQxY',\n",
       " 'J8Ku16yIVdA',\n",
       " 'rlYm9EWNNhY',\n",
       " 'MS323_BoGAM',\n",
       " 'fSJOdUyIZU8',\n",
       " 'jCnSG78xOgM',\n",
       " 'k3NNWQGgaPU',\n",
       " 'rXhXWqcVvDE',\n",
       " 'awSCxnk39FU',\n",
       " 'J62lYtIq0M8',\n",
       " 'BoVCp7gSOdg',\n",
       " 'T1F-kxzBhas',\n",
       " '0xeAMeTEsH0',\n",
       " 'D5bgfxmSiMA',\n",
       " 'gobJnbR2RxE',\n",
       " 'LIqtZxoT8Wk',\n",
       " 'ClMPxvVGQHE',\n",
       " 'v4mylTmx4bo',\n",
       " 'XHQNvonBrHE',\n",
       " 'rEfmsK7hrOw',\n",
       " 'kkZCf9yykCo',\n",
       " 'nZSCysb4iAE',\n",
       " '4wIP3e00rfo',\n",
       " '_wkIl-WWbTM',\n",
       " 'LC8V_Ngvgpo',\n",
       " 'WklGzNPskPU',\n",
       " 'CTVimIcxcY8',\n",
       " 'q2MVrPOotmU',\n",
       " 'r8-DOtbYR0U',\n",
       " 'raCn3ttAbFY',\n",
       " 'VWeDdU7tsPE',\n",
       " 'DQ9DXZs_k54',\n",
       " 'oeqEh0zEaDo',\n",
       " '_wkIl-WWbTM',\n",
       " 'LC8V_Ngvgpo',\n",
       " 'WklGzNPskPU',\n",
       " 'CTVimIcxcY8',\n",
       " 'q2MVrPOotmU',\n",
       " 'r8-DOtbYR0U',\n",
       " 'raCn3ttAbFY',\n",
       " 'VWeDdU7tsPE',\n",
       " 'DQ9DXZs_k54',\n",
       " 'oeqEh0zEaDo',\n",
       " 'l5Hlqs1puos',\n",
       " 'l55v_ylbMKc',\n",
       " 'LEpYRRKFF9U',\n",
       " 'dehVOFDcEHc',\n",
       " 'fH4iTrZVdRQ',\n",
       " 'hRGpUd_44gI',\n",
       " '5mQjfnnfOrI',\n",
       " '952tUb8LOJE',\n",
       " 'P6TET6mu7rM',\n",
       " 'fV6gEX9oeHs',\n",
       " '_wkIl-WWbTM',\n",
       " 'LC8V_Ngvgpo',\n",
       " 'WklGzNPskPU',\n",
       " 'CTVimIcxcY8',\n",
       " 'q2MVrPOotmU',\n",
       " 'r8-DOtbYR0U',\n",
       " 'raCn3ttAbFY',\n",
       " 'VWeDdU7tsPE',\n",
       " 'DQ9DXZs_k54',\n",
       " 'oeqEh0zEaDo',\n",
       " 'l5Hlqs1puos',\n",
       " 'l55v_ylbMKc',\n",
       " 'LEpYRRKFF9U',\n",
       " 'dehVOFDcEHc',\n",
       " 'fH4iTrZVdRQ',\n",
       " 'hRGpUd_44gI',\n",
       " '5mQjfnnfOrI',\n",
       " '952tUb8LOJE',\n",
       " 'P6TET6mu7rM',\n",
       " 'fV6gEX9oeHs',\n",
       " 'a6_ABY1EJVk',\n",
       " 'mPXgTA9Hdn4',\n",
       " 'yn9jxQznkO0',\n",
       " 'tOJCLyrGax8',\n",
       " 'AzbPeJmbw2U',\n",
       " 'kCtOL0B5sqE',\n",
       " 'bpLHVIymBQE',\n",
       " 'h3Yv_krSYsA',\n",
       " 'WKNC_OGDhUQ',\n",
       " 'wJyg_5Q6i4o',\n",
       " 'Yt2pMfd3ntU',\n",
       " '_QoElbb5gl8',\n",
       " '4ZK4ycjjzT8',\n",
       " 'TufmIudjfM0',\n",
       " 'f70w07SyOrU',\n",
       " 'fsijMWQkFpo',\n",
       " 'lG4v9g4rq80',\n",
       " 'i7LtEm07mpg',\n",
       " 'VPSciRX4aeQ',\n",
       " 'ibJOiAfxTBw',\n",
       " 'wMu3WeU6yGc',\n",
       " 'Uz6GZlxT24c',\n",
       " 'dpT2kTl7IPw',\n",
       " '9HY8KG2FWF0',\n",
       " 'n7SIhJPt70s',\n",
       " 'unE9lmV7FvU',\n",
       " 'UbTrAUCzmgg',\n",
       " 'w2HoNIhYtg4',\n",
       " 'p-J-vHFZ564',\n",
       " 'MZ372d1HEvg',\n",
       " '6iSh62GB8TU',\n",
       " 'Zm0fiUuvkek',\n",
       " 'dpT2kTl7IPw',\n",
       " '9HY8KG2FWF0',\n",
       " 'n7SIhJPt70s',\n",
       " 'unE9lmV7FvU',\n",
       " 'UbTrAUCzmgg',\n",
       " 'w2HoNIhYtg4',\n",
       " 'p-J-vHFZ564',\n",
       " 'MZ372d1HEvg',\n",
       " '6iSh62GB8TU',\n",
       " 'Zm0fiUuvkek',\n",
       " 'GD3FL7pRc3U',\n",
       " 'wf9_nuztNaQ',\n",
       " '8kpvI8Wupd0',\n",
       " 'RWsBOSvl-2k',\n",
       " 'VIfZlD1o6sQ',\n",
       " 'HQ4Jk7Y9VQw',\n",
       " '6DGfyjfE52U',\n",
       " 'ZhvpG5yXz7I',\n",
       " 'kCgzjpHh0yI',\n",
       " 'ijL7BDkAYzs',\n",
       " 'dpT2kTl7IPw',\n",
       " '9HY8KG2FWF0',\n",
       " 'n7SIhJPt70s',\n",
       " 'unE9lmV7FvU',\n",
       " 'UbTrAUCzmgg',\n",
       " 'w2HoNIhYtg4',\n",
       " 'p-J-vHFZ564',\n",
       " 'MZ372d1HEvg',\n",
       " '6iSh62GB8TU',\n",
       " 'Zm0fiUuvkek',\n",
       " 'GD3FL7pRc3U',\n",
       " 'wf9_nuztNaQ',\n",
       " '8kpvI8Wupd0',\n",
       " 'RWsBOSvl-2k',\n",
       " 'VIfZlD1o6sQ',\n",
       " 'HQ4Jk7Y9VQw',\n",
       " '6DGfyjfE52U',\n",
       " 'ZhvpG5yXz7I',\n",
       " 'kCgzjpHh0yI',\n",
       " 'ijL7BDkAYzs',\n",
       " '47Xm8Gibue8',\n",
       " 'MA2SAUXpMKU',\n",
       " '9UqKfvAqX_E',\n",
       " '4bC7pwsdg7k',\n",
       " 'vhXJP_yKkmM',\n",
       " 'Cvv6phjd8-8',\n",
       " 'gm55FkVERfc',\n",
       " 'MKrIV6ZNSwA',\n",
       " 'p3TA4vMHq4c',\n",
       " 'XaKhSGio8tY',\n",
       " 'L4BO3y-3sA8',\n",
       " 'fh4LGuzRTd4',\n",
       " '4VWA4JLkWFM',\n",
       " 'x-xtpzrT65U',\n",
       " 'efXXC8503eg',\n",
       " '1hneF335l1E',\n",
       " 'bd4CU5l3lac',\n",
       " 'durdKVQbTUc',\n",
       " 'XjKX3Tq9hHM',\n",
       " 'WYoR7gjR3LM',\n",
       " 'L4BO3y-3sA8',\n",
       " 'fh4LGuzRTd4',\n",
       " '4VWA4JLkWFM',\n",
       " 'x-xtpzrT65U',\n",
       " 'efXXC8503eg',\n",
       " '1hneF335l1E',\n",
       " 'bd4CU5l3lac',\n",
       " 'durdKVQbTUc',\n",
       " 'XjKX3Tq9hHM',\n",
       " 'WYoR7gjR3LM',\n",
       " 'DqtqP9v_J6E',\n",
       " 'FxKSW3KFDMI',\n",
       " 'Gd5tAPIJRMg',\n",
       " 'uHY-oj3Ti4A',\n",
       " '-IgmcrDOTxM',\n",
       " '1KDWAxTCpIk',\n",
       " '6S4gR2qkQwM',\n",
       " 'RFEPmHZs6D8',\n",
       " 'AnotBUVgmMA',\n",
       " 'L4BO3y-3sA8',\n",
       " 'fh4LGuzRTd4',\n",
       " '4VWA4JLkWFM',\n",
       " 'x-xtpzrT65U',\n",
       " 'efXXC8503eg',\n",
       " '1hneF335l1E',\n",
       " 'bd4CU5l3lac',\n",
       " 'durdKVQbTUc',\n",
       " 'XjKX3Tq9hHM',\n",
       " 'WYoR7gjR3LM',\n",
       " 'DqtqP9v_J6E',\n",
       " 'FxKSW3KFDMI',\n",
       " 'Gd5tAPIJRMg',\n",
       " 'uHY-oj3Ti4A',\n",
       " '-IgmcrDOTxM',\n",
       " '1KDWAxTCpIk',\n",
       " '6S4gR2qkQwM',\n",
       " 'RFEPmHZs6D8',\n",
       " 'AnotBUVgmMA',\n",
       " '5d8p6BLNkCo',\n",
       " 'RZrSloPfFuY',\n",
       " '7cW81Et56ys',\n",
       " 'jp9q5_4xalY',\n",
       " 'r65yrHGZZvw',\n",
       " 'HXaztJoSZ-8',\n",
       " 'o7O-q3JTy3M',\n",
       " 'MVkf_s5eyw8',\n",
       " 'pHOs90cqOdk',\n",
       " 'el8YOcT6GaA',\n",
       " 'L4BO3y-3sA8',\n",
       " 'fh4LGuzRTd4',\n",
       " '4VWA4JLkWFM',\n",
       " 'x-xtpzrT65U',\n",
       " 'efXXC8503eg',\n",
       " '1hneF335l1E',\n",
       " 'bd4CU5l3lac',\n",
       " 'durdKVQbTUc',\n",
       " 'XjKX3Tq9hHM',\n",
       " 'WYoR7gjR3LM',\n",
       " 'DqtqP9v_J6E',\n",
       " 'FxKSW3KFDMI',\n",
       " 'Gd5tAPIJRMg',\n",
       " 'uHY-oj3Ti4A',\n",
       " '-IgmcrDOTxM',\n",
       " '1KDWAxTCpIk',\n",
       " '6S4gR2qkQwM',\n",
       " 'RFEPmHZs6D8',\n",
       " 'AnotBUVgmMA',\n",
       " '5d8p6BLNkCo',\n",
       " 'RZrSloPfFuY',\n",
       " '7cW81Et56ys',\n",
       " 'jp9q5_4xalY',\n",
       " 'r65yrHGZZvw',\n",
       " 'HXaztJoSZ-8',\n",
       " 'o7O-q3JTy3M',\n",
       " 'MVkf_s5eyw8',\n",
       " 'pHOs90cqOdk',\n",
       " 'el8YOcT6GaA',\n",
       " 'iKyaZLvVbU4',\n",
       " 'Yjp1F3k-11o',\n",
       " 'I8GJD5uO7c8',\n",
       " 'OduvMRpAAn8',\n",
       " 'uZ_w6ubB1Zk',\n",
       " 'AmH0UEwKwuU']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5802a9c2-0e43-4d6d-b157-7b2fae7d165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Commentsd(vids):\n",
    "    # print(vids)\n",
    "    comment_id, Video_id, Comment_text, Comment_author, Comment_date = [], [], [], [], []\n",
    "    for x in vids:\n",
    "        token = ' '\n",
    "        while token != None:\n",
    "            try:\n",
    "                token = reponse['nextPageToken']\n",
    "            except:\n",
    "                token = None\n",
    "\n",
    "            cmt_response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=x,\n",
    "                maxResults=50,\n",
    "                pageToken=token\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                reponse = cmt_response.execute()\n",
    "            except:\n",
    "                reponse = None\n",
    "            if not reponse:\n",
    "                None\n",
    "            else:                \n",
    "                comment_id.extend([x.get('id', None) for x in reponse['items']]),\n",
    "                Video_id.extend([x['snippet']['videoId'] for x in reponse['items']]),\n",
    "                Comment_text.extend(\n",
    "                    [remove_emojis(x['snippet']['topLevelComment']['snippet'].get('textOriginal', None)) for x in\n",
    "                     reponse['items']]),\n",
    "                Comment_author.extend(\n",
    "                    [x['snippet']['topLevelComment']['snippet'].get('authorDisplayName', None) for x in reponse['items']]),\n",
    "                Comment_date.extend(\n",
    "                    [dt.strptime(x['snippet']['topLevelComment']['snippet'].get('publishedAt', None)\n",
    "                    .replace('T', ' ').replace('Z', ''),\"%Y-%m-%d %H:%M:%S\").strftime(\"%d/%m/%Y %I:%M %p\")\n",
    "                    for x in reponse['items']]\n",
    "                                    )\n",
    "\n",
    "\n",
    "    cmt = {'Comment_id': comment_id,\n",
    "           'Video_id': Video_id,\n",
    "           'Comment_text': Comment_text,\n",
    "           'Comment_author': Comment_author,\n",
    "           'Comment_date': Comment_date\n",
    "           }\n",
    "\n",
    "    return cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1be7066-dab4-4306-86c8-2515c542d4a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Comment_id': ['UgwmOE429Z4p7Ykw0vJ4AaABAg',\n",
       "  'UgyMcyPEuNlxRl0C_ml4AaABAg',\n",
       "  'UgzIazmbI3dY6MwPowB4AaABAg',\n",
       "  'Ugw_Jkg5Ls6U257-cHh4AaABAg',\n",
       "  'UgyFdWy4SXKQ2wO89uN4AaABAg',\n",
       "  'UgzBZxnJONGLw9DkC5F4AaABAg',\n",
       "  'UgweEM50sgnp1j9V6Ll4AaABAg',\n",
       "  'UgybZBvFsz8OXeTt2Zd4AaABAg',\n",
       "  'UgxD74-XYI0Nffban3N4AaABAg',\n",
       "  'UgxdFYvnQ_BrKOa3PxF4AaABAg',\n",
       "  'UgwKDfQRCTeqNASOihB4AaABAg',\n",
       "  'Ugx7lQ4zl7a1gsqT3YR4AaABAg',\n",
       "  'UgyCBZN6KMNZAKr8Cgt4AaABAg',\n",
       "  'Ugy82wJSqgR4Lab6j0V4AaABAg',\n",
       "  'Ugy3CziFic5TOYTld9l4AaABAg',\n",
       "  'UgxY9UnvmCTayIdAh-t4AaABAg',\n",
       "  'UgzitiLXPyTJh-OZVj54AaABAg',\n",
       "  'UgyNWiS7BNF4vV_cHzx4AaABAg',\n",
       "  'UgzBf7GZT4HZJuDAA814AaABAg',\n",
       "  'UgwHDDasqnD1sNv_NGl4AaABAg',\n",
       "  'UgwJHahYIscwSOVI2v54AaABAg',\n",
       "  'UgyLQywuk0RLHKu5iH54AaABAg',\n",
       "  'Ugwg9toIdjFZwOfMgfZ4AaABAg',\n",
       "  'Ugyd22kHp5_7kczfy394AaABAg',\n",
       "  'Ugx7wd4vo3Uv6WdUkdF4AaABAg',\n",
       "  'Ugz_lk_2SyZgGLTJODZ4AaABAg',\n",
       "  'UgznNRy9-cmddJxW1PF4AaABAg',\n",
       "  'UgwAAQPJfH9fSkEgHe94AaABAg',\n",
       "  'Ugwxsb0XWwe7iqnDb654AaABAg',\n",
       "  'UgwhVeQ4MSzV10nR-2V4AaABAg',\n",
       "  'UgxoEZH2YZCNHaz-X_l4AaABAg',\n",
       "  'Ugwr9t6-L1Qdax_HwYV4AaABAg',\n",
       "  'UgzT4_ONgDetpLEgcQB4AaABAg',\n",
       "  'UgwNYuQtU9XzfoCXh_V4AaABAg',\n",
       "  'Ugwg9toIdjFZwOfMgfZ4AaABAg',\n",
       "  'Ugyd22kHp5_7kczfy394AaABAg',\n",
       "  'Ugx7wd4vo3Uv6WdUkdF4AaABAg',\n",
       "  'Ugz_lk_2SyZgGLTJODZ4AaABAg',\n",
       "  'UgznNRy9-cmddJxW1PF4AaABAg',\n",
       "  'UgwAAQPJfH9fSkEgHe94AaABAg',\n",
       "  'Ugwxsb0XWwe7iqnDb654AaABAg',\n",
       "  'UgwhVeQ4MSzV10nR-2V4AaABAg',\n",
       "  'UgxoEZH2YZCNHaz-X_l4AaABAg',\n",
       "  'Ugwr9t6-L1Qdax_HwYV4AaABAg',\n",
       "  'UgzT4_ONgDetpLEgcQB4AaABAg',\n",
       "  'UgwNYuQtU9XzfoCXh_V4AaABAg',\n",
       "  'UgyLTqa2_rbrr29jDhN4AaABAg',\n",
       "  'Ugx7ePpxLBnc4l64DPh4AaABAg',\n",
       "  'Ugx1wwkAhLPzHBEx3394AaABAg',\n",
       "  'UgzqZZXR-o3gRfeG6Bd4AaABAg',\n",
       "  'Ugyt2usZKQLa-D-nEgd4AaABAg',\n",
       "  'UgyO_YrM5jurRwsbru54AaABAg',\n",
       "  'Ugxdxt2oNKKEO8g61YB4AaABAg',\n",
       "  'Ugw4-d6_UBEhJ0lFEYB4AaABAg',\n",
       "  'UgwO5bPtyyvnLReY7m94AaABAg',\n",
       "  'Ugx1wwkAhLPzHBEx3394AaABAg',\n",
       "  'UgzqZZXR-o3gRfeG6Bd4AaABAg',\n",
       "  'Ugyt2usZKQLa-D-nEgd4AaABAg',\n",
       "  'UgyO_YrM5jurRwsbru54AaABAg',\n",
       "  'Ugxdxt2oNKKEO8g61YB4AaABAg',\n",
       "  'Ugw4-d6_UBEhJ0lFEYB4AaABAg',\n",
       "  'UgwO5bPtyyvnLReY7m94AaABAg',\n",
       "  'Ugx5uG_tNXz4lHiw_Fx4AaABAg',\n",
       "  'UgzHlPxm2rkbFok4Vax4AaABAg',\n",
       "  'UgyEicIslzwUVdktnA94AaABAg',\n",
       "  'Ugwk_0XBZ1LWs_sU2Y94AaABAg',\n",
       "  'UgwoN5CSNa3jDUYEHjJ4AaABAg',\n",
       "  'UgyjeHYVL6tj1InNU594AaABAg',\n",
       "  'UgxZTDmirjeeqvnjvGd4AaABAg',\n",
       "  'UgxSZcoEdtdotWkW9L54AaABAg',\n",
       "  'UgwAXl57NpQcLuX8KXJ4AaABAg',\n",
       "  'UgwCNuN2P_JgtKK0NUp4AaABAg',\n",
       "  'UgxnCSKT3BH7ssaWjeh4AaABAg',\n",
       "  'UgywSt4QGA6sMivg8Xd4AaABAg',\n",
       "  'Ugyp1UDSMTI2sOXootl4AaABAg',\n",
       "  'UgzF-q8wbc7H1T5Y5nB4AaABAg',\n",
       "  'Ugz8kYJHsMtd59f1MVF4AaABAg',\n",
       "  'UgyfO2gfb7neq-1OZRN4AaABAg',\n",
       "  'Ugxi0qVApD1WdwIRKQB4AaABAg',\n",
       "  'Ugxelv8U-06JU7-wyXp4AaABAg',\n",
       "  'UgxczoY4CbRjAEkA1SV4AaABAg',\n",
       "  'Ugya1xZyjR5kEJ4LZ2h4AaABAg',\n",
       "  'UgwxpJrgOo5oiCMHPuJ4AaABAg',\n",
       "  'UgzrZwjfts7TslTbubR4AaABAg',\n",
       "  'Ugz4TBw-VMMF2BTspH54AaABAg',\n",
       "  'Ugzo01Wdf5VycQZEZql4AaABAg',\n",
       "  'UgxSdqR0Miy1tz6e2vd4AaABAg',\n",
       "  'UgxjplUFWbutT53vOg94AaABAg',\n",
       "  'UgzQDreoL0luHTP79sV4AaABAg',\n",
       "  'UgyuBfN6yMASoODtKU14AaABAg',\n",
       "  'Ugw5GxR_ARCTxScP4u94AaABAg',\n",
       "  'UgyLZ_RXWO1rqLhp0JV4AaABAg',\n",
       "  'Ugz7YXETK357dqHEv1Z4AaABAg',\n",
       "  'UgzAameb3QZk6uMVEyZ4AaABAg',\n",
       "  'UgxuYI4tyvWSDyXOrpx4AaABAg',\n",
       "  'UgzMg87hVSVtzbS__b14AaABAg',\n",
       "  'UgxMLHM5YqjokXOFJrl4AaABAg',\n",
       "  'UgwgUk3Kd8S-tilxgth4AaABAg',\n",
       "  'UgzmEDdgfOMFh3O90Yh4AaABAg',\n",
       "  'UgzzBW0Mg8nP8QXBJw94AaABAg',\n",
       "  'UgweXM0aLDNl5wpjaXN4AaABAg',\n",
       "  'UgyXy6INBAEeSTQs95Z4AaABAg',\n",
       "  'UgzKkckJMryVg4SSGxF4AaABAg',\n",
       "  'UgwL8PXtJ-kt2zr4Ss14AaABAg',\n",
       "  'UgxQMlQS6m7M-Pg6rfx4AaABAg',\n",
       "  'UgzkuvmKrI-MeQoZ6yB4AaABAg',\n",
       "  'Ugy11f6zU8Bs8zzm0VB4AaABAg',\n",
       "  'UgzUBr6TlShBWYepl414AaABAg',\n",
       "  'UgzijiG5Hf5RGOhrBhF4AaABAg',\n",
       "  'Ugx3MP0T-h2jX7WTjc54AaABAg',\n",
       "  'Ugw4VN1fprGW8MTA5lx4AaABAg',\n",
       "  'Ugx2ixTjDJfAX8gSdxN4AaABAg',\n",
       "  'UgwCNuN2P_JgtKK0NUp4AaABAg',\n",
       "  'UgxnCSKT3BH7ssaWjeh4AaABAg',\n",
       "  'UgywSt4QGA6sMivg8Xd4AaABAg',\n",
       "  'Ugyp1UDSMTI2sOXootl4AaABAg',\n",
       "  'UgzF-q8wbc7H1T5Y5nB4AaABAg',\n",
       "  'Ugz8kYJHsMtd59f1MVF4AaABAg',\n",
       "  'UgyfO2gfb7neq-1OZRN4AaABAg',\n",
       "  'Ugxi0qVApD1WdwIRKQB4AaABAg',\n",
       "  'Ugxelv8U-06JU7-wyXp4AaABAg',\n",
       "  'UgxczoY4CbRjAEkA1SV4AaABAg',\n",
       "  'Ugya1xZyjR5kEJ4LZ2h4AaABAg',\n",
       "  'UgwxpJrgOo5oiCMHPuJ4AaABAg',\n",
       "  'UgzrZwjfts7TslTbubR4AaABAg',\n",
       "  'Ugz4TBw-VMMF2BTspH54AaABAg',\n",
       "  'Ugzo01Wdf5VycQZEZql4AaABAg',\n",
       "  'UgxSdqR0Miy1tz6e2vd4AaABAg',\n",
       "  'UgxjplUFWbutT53vOg94AaABAg',\n",
       "  'UgzQDreoL0luHTP79sV4AaABAg',\n",
       "  'UgyuBfN6yMASoODtKU14AaABAg',\n",
       "  'Ugw5GxR_ARCTxScP4u94AaABAg',\n",
       "  'UgyLZ_RXWO1rqLhp0JV4AaABAg',\n",
       "  'Ugz7YXETK357dqHEv1Z4AaABAg',\n",
       "  'UgzAameb3QZk6uMVEyZ4AaABAg',\n",
       "  'UgxuYI4tyvWSDyXOrpx4AaABAg',\n",
       "  'UgzMg87hVSVtzbS__b14AaABAg',\n",
       "  'UgxMLHM5YqjokXOFJrl4AaABAg',\n",
       "  'UgwgUk3Kd8S-tilxgth4AaABAg',\n",
       "  'UgzmEDdgfOMFh3O90Yh4AaABAg',\n",
       "  'UgzzBW0Mg8nP8QXBJw94AaABAg',\n",
       "  'UgweXM0aLDNl5wpjaXN4AaABAg',\n",
       "  'UgyXy6INBAEeSTQs95Z4AaABAg',\n",
       "  'UgzKkckJMryVg4SSGxF4AaABAg',\n",
       "  'UgwL8PXtJ-kt2zr4Ss14AaABAg',\n",
       "  'UgxQMlQS6m7M-Pg6rfx4AaABAg',\n",
       "  'UgzkuvmKrI-MeQoZ6yB4AaABAg',\n",
       "  'Ugy11f6zU8Bs8zzm0VB4AaABAg',\n",
       "  'UgzUBr6TlShBWYepl414AaABAg',\n",
       "  'UgzijiG5Hf5RGOhrBhF4AaABAg',\n",
       "  'Ugx3MP0T-h2jX7WTjc54AaABAg',\n",
       "  'Ugw4VN1fprGW8MTA5lx4AaABAg',\n",
       "  'Ugx2ixTjDJfAX8gSdxN4AaABAg',\n",
       "  'UgwCJ6rc2JJio8LLDYR4AaABAg',\n",
       "  'UgzaxyT_gFlIojT0loJ4AaABAg',\n",
       "  'UgwV0USs8a-XvaiXRjd4AaABAg',\n",
       "  'UgyeJltYigbtzE3k6ER4AaABAg',\n",
       "  'UgzUMZ-SbtLMUpElqeR4AaABAg',\n",
       "  'UgzmMcLfwWpZ_Hc1EXt4AaABAg',\n",
       "  'UgyuhEEM3jJsBkXVYTF4AaABAg',\n",
       "  'UgxC3xQG0YuqIataXdB4AaABAg',\n",
       "  'Ugx_nd_DCLDZry0BjHR4AaABAg',\n",
       "  'UgzAhg8iCkF2ZjZ5n1l4AaABAg',\n",
       "  'UgzAIw_3ZKvlAUMbEp54AaABAg',\n",
       "  'Ugxreod1slUXebitF_14AaABAg',\n",
       "  'Ugx30mNKYhAtGGZnEdF4AaABAg',\n",
       "  'Ugw-fFideC-s7C6CLW94AaABAg',\n",
       "  'UgzV8RbtnpnD47fPhNh4AaABAg',\n",
       "  'Ugy6GZT3tb3bdKjih054AaABAg',\n",
       "  'UgzaVi7s8WJFNHxuJNF4AaABAg',\n",
       "  'UgwnIKpl1DazLfS1kOV4AaABAg',\n",
       "  'Ugyb_Kv0NvXEE8dxcyZ4AaABAg',\n",
       "  'UgztxT-qQzz0s5FX85h4AaABAg',\n",
       "  'UgxNg5KzEvRsKuo1hfp4AaABAg',\n",
       "  'UgzKoO6sdMoi91p9Skt4AaABAg',\n",
       "  'UgyQCIJrHrOi1htCCbJ4AaABAg',\n",
       "  'Ugwc3IhchI2cw4NB5694AaABAg',\n",
       "  'UgwwJCyVT_Z-SsI_s7x4AaABAg',\n",
       "  'UgzNimDO3FFvrP4pdQp4AaABAg',\n",
       "  'Ugyy4bMBWffGpwizZG94AaABAg',\n",
       "  'UgxTxz1-crfxssgklvF4AaABAg',\n",
       "  'UgzCwCztrIYiZMrCGgJ4AaABAg',\n",
       "  'UgzxXoUynGKl2KsDvQJ4AaABAg',\n",
       "  'UgyAAjn1g4QbpTcWQH94AaABAg',\n",
       "  'UgwG44BxDSHXrQbBjmp4AaABAg',\n",
       "  'UgzjkUE4JnVQISYa_Sl4AaABAg',\n",
       "  'UgwSUfwpOWukiq3BmnZ4AaABAg',\n",
       "  'UgwAgrs5J3Jes0c2n5t4AaABAg',\n",
       "  'Ugy8dfFeqSJfO575JQt4AaABAg',\n",
       "  'UgzI2i9SKrEzjJQ2A4J4AaABAg',\n",
       "  'Ugxh_atFblM_vIL3pK94AaABAg',\n",
       "  'UgwG44BxDSHXrQbBjmp4AaABAg',\n",
       "  'UgzjkUE4JnVQISYa_Sl4AaABAg',\n",
       "  'UgwSUfwpOWukiq3BmnZ4AaABAg',\n",
       "  'UgwAgrs5J3Jes0c2n5t4AaABAg',\n",
       "  'Ugy8dfFeqSJfO575JQt4AaABAg',\n",
       "  'UgzI2i9SKrEzjJQ2A4J4AaABAg',\n",
       "  'Ugxh_atFblM_vIL3pK94AaABAg',\n",
       "  'Ugy7DInKA14wt_07tzx4AaABAg',\n",
       "  'UgxCvyHghJFQnWVw-Dt4AaABAg',\n",
       "  'UgybRFPFzPIASH97H4h4AaABAg',\n",
       "  'UgxQaETfnlozwn1o6GZ4AaABAg',\n",
       "  'UgxddEG1XW0d70KIDg14AaABAg',\n",
       "  'Ugyq9M241ig82LXYoih4AaABAg',\n",
       "  'UgwG44BxDSHXrQbBjmp4AaABAg',\n",
       "  'UgzjkUE4JnVQISYa_Sl4AaABAg',\n",
       "  'UgwSUfwpOWukiq3BmnZ4AaABAg',\n",
       "  'UgwAgrs5J3Jes0c2n5t4AaABAg',\n",
       "  'Ugy8dfFeqSJfO575JQt4AaABAg',\n",
       "  'UgzI2i9SKrEzjJQ2A4J4AaABAg',\n",
       "  'Ugxh_atFblM_vIL3pK94AaABAg',\n",
       "  'Ugy7DInKA14wt_07tzx4AaABAg',\n",
       "  'UgxCvyHghJFQnWVw-Dt4AaABAg',\n",
       "  'UgybRFPFzPIASH97H4h4AaABAg',\n",
       "  'UgxQaETfnlozwn1o6GZ4AaABAg',\n",
       "  'UgxddEG1XW0d70KIDg14AaABAg',\n",
       "  'Ugyq9M241ig82LXYoih4AaABAg',\n",
       "  'UgxMFmdG2FlI1xvoEIJ4AaABAg',\n",
       "  'Ugz4Q0QjBrLKyhBoyMx4AaABAg',\n",
       "  'Ugxm2rT_QRltdy3lvN94AaABAg',\n",
       "  'UgwXlS3f3kQdvfVDkON4AaABAg',\n",
       "  'UgwZMRLyABCOdtbd9_54AaABAg',\n",
       "  'UgxZ_Pb2QtSmZOwuEPh4AaABAg',\n",
       "  'UgwG44BxDSHXrQbBjmp4AaABAg',\n",
       "  'UgzjkUE4JnVQISYa_Sl4AaABAg',\n",
       "  'UgwSUfwpOWukiq3BmnZ4AaABAg',\n",
       "  'UgwAgrs5J3Jes0c2n5t4AaABAg',\n",
       "  'Ugy8dfFeqSJfO575JQt4AaABAg',\n",
       "  'UgzI2i9SKrEzjJQ2A4J4AaABAg',\n",
       "  'Ugxh_atFblM_vIL3pK94AaABAg',\n",
       "  'Ugy7DInKA14wt_07tzx4AaABAg',\n",
       "  'UgxCvyHghJFQnWVw-Dt4AaABAg',\n",
       "  'UgybRFPFzPIASH97H4h4AaABAg',\n",
       "  'UgxQaETfnlozwn1o6GZ4AaABAg',\n",
       "  'UgxddEG1XW0d70KIDg14AaABAg',\n",
       "  'Ugyq9M241ig82LXYoih4AaABAg',\n",
       "  'UgxMFmdG2FlI1xvoEIJ4AaABAg',\n",
       "  'Ugz4Q0QjBrLKyhBoyMx4AaABAg',\n",
       "  'Ugxm2rT_QRltdy3lvN94AaABAg',\n",
       "  'UgwXlS3f3kQdvfVDkON4AaABAg',\n",
       "  'UgwZMRLyABCOdtbd9_54AaABAg',\n",
       "  'UgxZ_Pb2QtSmZOwuEPh4AaABAg',\n",
       "  'Ugzw4DaOHKwRRaa6J154AaABAg',\n",
       "  'UgzOXo_q5cUU8JWZWYJ4AaABAg',\n",
       "  'UgxQEuOdmNV3vCqSrvh4AaABAg',\n",
       "  'Ugx9yhx-NKsU0fd9UFd4AaABAg',\n",
       "  'UgynTvuWxj_I60Coz0t4AaABAg',\n",
       "  'UgxqL04q7G3OFY-5ehd4AaABAg',\n",
       "  'UgyAWIMSI5cWmTL3nTB4AaABAg',\n",
       "  'UgwZrEsKclw7cnIFhEh4AaABAg',\n",
       "  'Ugzm0yeAF8qn8_6igCp4AaABAg',\n",
       "  'UgzvvlRCP9xG7eP4gbJ4AaABAg',\n",
       "  'UgyRrEjxfc3WH0UJNOZ4AaABAg',\n",
       "  'UgwI1K1eUgnIbkwxBtV4AaABAg',\n",
       "  'UgyLqjs1dAgFxvkM-dp4AaABAg',\n",
       "  'UgzAToawh1gyirNYuVd4AaABAg',\n",
       "  'UgxAqx3SlJJKSktDYu94AaABAg',\n",
       "  'Ugx7XYJKGnS42sms8HN4AaABAg',\n",
       "  'UgxWNuldXf2abcv5HEx4AaABAg',\n",
       "  'UgwEp51zyAj6y5Hfp9J4AaABAg',\n",
       "  'UgxGaPwMKuLLob81CgZ4AaABAg',\n",
       "  'Ugxee-uQhtJrmqAc8QR4AaABAg',\n",
       "  'Ugxv3kYK7mpwgsZtkwN4AaABAg',\n",
       "  'UgxLzGxSaTjoOZRDj1d4AaABAg',\n",
       "  'Ugw2d7ymhcorQrDyXCV4AaABAg',\n",
       "  'Ugy8O6PnU695C3FoP7p4AaABAg',\n",
       "  'UgySaOZNorG7m61C1rZ4AaABAg',\n",
       "  'UgyfPwTHL1TJUqKDoJF4AaABAg',\n",
       "  'Ugy9hAyEacmXXR393RR4AaABAg',\n",
       "  'UgwugrnFI6oUKpVYXnB4AaABAg',\n",
       "  'Ugy8Ikdt7uiCHVg_bb94AaABAg',\n",
       "  'Ugw8NR0Vy9Vf5JeZDr14AaABAg',\n",
       "  'UgzlY8FDOjUXMycNiPl4AaABAg',\n",
       "  'UgwnXo6_WqK0uR8O__x4AaABAg',\n",
       "  'UgxLNMdhpRONt1uNu-h4AaABAg',\n",
       "  'UgzhqlrZC-eGTnE3oFV4AaABAg',\n",
       "  'Ugxy2uN93xumURFyV9R4AaABAg',\n",
       "  'UgxJBqvjfpRQeYx1ufZ4AaABAg',\n",
       "  'UgxiFrTxqhGxkHfaxTV4AaABAg',\n",
       "  'UgwWWkn8hOZtc0gDWCt4AaABAg',\n",
       "  'Ugwt1YS6KajVSOafG_N4AaABAg',\n",
       "  'Ugyo269EKVPv9i5wt1J4AaABAg',\n",
       "  'UgzGe6W2A3nLmjdpbcd4AaABAg',\n",
       "  'UgxJ7okxl4YxkLNc0xR4AaABAg',\n",
       "  'UgzaevB66SH3SzfO3eV4AaABAg',\n",
       "  'UgwqPb53m0bo2m2Yfcd4AaABAg',\n",
       "  'UgzGCZ_rwuZkqQuDb_14AaABAg',\n",
       "  'Ugzi0JTnZmMXwCMohMF4AaABAg',\n",
       "  'UgzGT8XK5OvEv7aq_X54AaABAg',\n",
       "  'UgxJ0V-Pl1444lagoH14AaABAg',\n",
       "  'Ugy1DCZq1_p0XJTQvRB4AaABAg',\n",
       "  'Ugx2i27nHrq1RwJnsTZ4AaABAg',\n",
       "  'Ugy4_fXFPZLA3uO8d_V4AaABAg',\n",
       "  'UgybmZdMgvvg9pkuO414AaABAg',\n",
       "  'UgwC498tLvDmFDWxP0R4AaABAg',\n",
       "  'UgzOVayW2A8M7GphecB4AaABAg',\n",
       "  'UgzcVOBtALQ4ddV3xt14AaABAg',\n",
       "  'UgxcjbluQKyUIFUz6-Z4AaABAg',\n",
       "  'UgwkkwgRLB6fw3vnHv94AaABAg',\n",
       "  'UgzXkvJtwsZfky6_xSZ4AaABAg',\n",
       "  'Ugz0ybzH6kPi_Qt-MtJ4AaABAg',\n",
       "  'UgwTbh_X5cwVDpcFf9x4AaABAg',\n",
       "  'Ugz36QMFE6Z4EAKNxnR4AaABAg',\n",
       "  'UgwSaYpR9sjmSmuleXZ4AaABAg',\n",
       "  'Ugykde-oNPuyGyq04LR4AaABAg',\n",
       "  'UgxxmV5staMesS_UBRd4AaABAg',\n",
       "  'UgzTUXxvIA2mWbU4y3B4AaABAg',\n",
       "  'UgxJzOx_0LcI3cfH-Gd4AaABAg',\n",
       "  'UgzqCL_jkUS-RdE8Hk94AaABAg',\n",
       "  'UgzrtnqHWdtitLeeR694AaABAg',\n",
       "  'UgzNNlAUJWi0U3gdq1p4AaABAg',\n",
       "  'UgzoaB8eUiHuvt1mwe94AaABAg',\n",
       "  'UgypPcHawkbxwlIjg4p4AaABAg',\n",
       "  'UgxbHJNomNdU43LevI54AaABAg',\n",
       "  'UgzWQlG7VmhMkQiPjtl4AaABAg',\n",
       "  'Ugwg_o-iBESFZ7DA1CJ4AaABAg',\n",
       "  'Ugz7zVnWZRMm7PzHpjZ4AaABAg',\n",
       "  'UgylaplqLW3WdX41rqF4AaABAg',\n",
       "  'UgwyCqJcWLiC_6YY6_J4AaABAg',\n",
       "  'UgxBtsdrM-BfUyPHlxd4AaABAg',\n",
       "  'UgyqJZB3gH4qUQBKlUh4AaABAg',\n",
       "  'UgwfYmoOIPnl7uoX5Ud4AaABAg',\n",
       "  'UgwHEwb_ZdDxdVjiJ6t4AaABAg',\n",
       "  'UgwWuusXlMkITNhB-9d4AaABAg',\n",
       "  'UgzXrUmMhWwsyYLEIgR4AaABAg',\n",
       "  'Ugws9d5UG94LwS6_Gel4AaABAg',\n",
       "  'UgwDoINunMTZ0-SPtKF4AaABAg',\n",
       "  'UgyzNJn-v-uoQP4UjyR4AaABAg',\n",
       "  'UgwWpOWphXezYLRqh-t4AaABAg',\n",
       "  'UgxnuYDN46og7L3tN9B4AaABAg',\n",
       "  'UgyJ6CGZFI6GBbXwayB4AaABAg',\n",
       "  'UgzjqUZBfXT5stD5B6Z4AaABAg',\n",
       "  'UgwF4JTAY34b8TKC8-x4AaABAg',\n",
       "  'Ugy-h1wBe7-iPZegMP14AaABAg',\n",
       "  'UgxTXeUb9qKfwO4Lnu94AaABAg',\n",
       "  'Ugz0ybzH6kPi_Qt-MtJ4AaABAg',\n",
       "  'UgwTbh_X5cwVDpcFf9x4AaABAg',\n",
       "  'Ugz36QMFE6Z4EAKNxnR4AaABAg',\n",
       "  'UgwSaYpR9sjmSmuleXZ4AaABAg',\n",
       "  'Ugykde-oNPuyGyq04LR4AaABAg',\n",
       "  'UgxxmV5staMesS_UBRd4AaABAg',\n",
       "  'UgzTUXxvIA2mWbU4y3B4AaABAg',\n",
       "  'UgxJzOx_0LcI3cfH-Gd4AaABAg',\n",
       "  'UgzqCL_jkUS-RdE8Hk94AaABAg',\n",
       "  'UgzrtnqHWdtitLeeR694AaABAg',\n",
       "  'UgzNNlAUJWi0U3gdq1p4AaABAg',\n",
       "  'UgzoaB8eUiHuvt1mwe94AaABAg',\n",
       "  'UgypPcHawkbxwlIjg4p4AaABAg',\n",
       "  'UgxbHJNomNdU43LevI54AaABAg',\n",
       "  'UgzWQlG7VmhMkQiPjtl4AaABAg',\n",
       "  'Ugwg_o-iBESFZ7DA1CJ4AaABAg',\n",
       "  'Ugz7zVnWZRMm7PzHpjZ4AaABAg',\n",
       "  'UgylaplqLW3WdX41rqF4AaABAg',\n",
       "  'UgwyCqJcWLiC_6YY6_J4AaABAg',\n",
       "  'UgxBtsdrM-BfUyPHlxd4AaABAg',\n",
       "  'UgyqJZB3gH4qUQBKlUh4AaABAg',\n",
       "  'UgwfYmoOIPnl7uoX5Ud4AaABAg',\n",
       "  'UgwHEwb_ZdDxdVjiJ6t4AaABAg',\n",
       "  'UgwWuusXlMkITNhB-9d4AaABAg',\n",
       "  'UgzXrUmMhWwsyYLEIgR4AaABAg',\n",
       "  'Ugws9d5UG94LwS6_Gel4AaABAg',\n",
       "  'UgwDoINunMTZ0-SPtKF4AaABAg',\n",
       "  'UgyzNJn-v-uoQP4UjyR4AaABAg',\n",
       "  'UgwWpOWphXezYLRqh-t4AaABAg',\n",
       "  'UgxnuYDN46og7L3tN9B4AaABAg',\n",
       "  'UgyJ6CGZFI6GBbXwayB4AaABAg',\n",
       "  'UgzjqUZBfXT5stD5B6Z4AaABAg',\n",
       "  'UgwF4JTAY34b8TKC8-x4AaABAg',\n",
       "  'Ugy-h1wBe7-iPZegMP14AaABAg',\n",
       "  'UgxTXeUb9qKfwO4Lnu94AaABAg',\n",
       "  'UgyXlmLGsVqv0bugqyt4AaABAg',\n",
       "  'Ugx8VrB1IFLKTBzLmCp4AaABAg',\n",
       "  'UgzTxhSWJ9-DAY_l3EF4AaABAg',\n",
       "  'Ugy_WQ5-1_k-sExJpGp4AaABAg',\n",
       "  'UgwmNQflD_ZiTD1efN14AaABAg',\n",
       "  'UgzrQYLKdUDiFd9ztc54AaABAg',\n",
       "  'Ugwm-Mo8xcVZN872HBV4AaABAg',\n",
       "  'Ugx7h-8jB3K3BnvAyIN4AaABAg',\n",
       "  'UgxwOqfwDr_ZA81KmKR4AaABAg',\n",
       "  'UgxbVacVcgAWGMr2BJZ4AaABAg',\n",
       "  'UgyHk6_MMF0ls75vYMF4AaABAg',\n",
       "  'UgxtKYo7XoVKUgInCH14AaABAg',\n",
       "  'UgzIWrFzRcqGM5TBRst4AaABAg',\n",
       "  'UgwfGJSNlOOd0SrYP3d4AaABAg',\n",
       "  'Ugws3y76WHB2VWilsB94AaABAg',\n",
       "  'UgxBYPo__lPKfLoM6fd4AaABAg',\n",
       "  'Ugz0ybzH6kPi_Qt-MtJ4AaABAg',\n",
       "  'UgwTbh_X5cwVDpcFf9x4AaABAg',\n",
       "  'Ugz36QMFE6Z4EAKNxnR4AaABAg',\n",
       "  'UgwSaYpR9sjmSmuleXZ4AaABAg',\n",
       "  'Ugykde-oNPuyGyq04LR4AaABAg',\n",
       "  'UgxxmV5staMesS_UBRd4AaABAg',\n",
       "  'UgzTUXxvIA2mWbU4y3B4AaABAg',\n",
       "  'UgxJzOx_0LcI3cfH-Gd4AaABAg',\n",
       "  'UgzqCL_jkUS-RdE8Hk94AaABAg',\n",
       "  'UgzrtnqHWdtitLeeR694AaABAg',\n",
       "  'UgzNNlAUJWi0U3gdq1p4AaABAg',\n",
       "  'UgzoaB8eUiHuvt1mwe94AaABAg',\n",
       "  'UgypPcHawkbxwlIjg4p4AaABAg',\n",
       "  'UgxbHJNomNdU43LevI54AaABAg',\n",
       "  'UgzWQlG7VmhMkQiPjtl4AaABAg',\n",
       "  'Ugwg_o-iBESFZ7DA1CJ4AaABAg',\n",
       "  'Ugz7zVnWZRMm7PzHpjZ4AaABAg',\n",
       "  'UgylaplqLW3WdX41rqF4AaABAg',\n",
       "  'UgwyCqJcWLiC_6YY6_J4AaABAg',\n",
       "  'UgxBtsdrM-BfUyPHlxd4AaABAg',\n",
       "  'UgyqJZB3gH4qUQBKlUh4AaABAg',\n",
       "  'UgwfYmoOIPnl7uoX5Ud4AaABAg',\n",
       "  'UgwHEwb_ZdDxdVjiJ6t4AaABAg',\n",
       "  'UgwWuusXlMkITNhB-9d4AaABAg',\n",
       "  'UgzXrUmMhWwsyYLEIgR4AaABAg',\n",
       "  'Ugws9d5UG94LwS6_Gel4AaABAg',\n",
       "  'UgwDoINunMTZ0-SPtKF4AaABAg',\n",
       "  'UgyzNJn-v-uoQP4UjyR4AaABAg',\n",
       "  'UgwWpOWphXezYLRqh-t4AaABAg',\n",
       "  'UgxnuYDN46og7L3tN9B4AaABAg',\n",
       "  'UgyJ6CGZFI6GBbXwayB4AaABAg',\n",
       "  'UgzjqUZBfXT5stD5B6Z4AaABAg',\n",
       "  'UgwF4JTAY34b8TKC8-x4AaABAg',\n",
       "  'Ugy-h1wBe7-iPZegMP14AaABAg',\n",
       "  'UgxTXeUb9qKfwO4Lnu94AaABAg',\n",
       "  'UgyXlmLGsVqv0bugqyt4AaABAg',\n",
       "  'Ugx8VrB1IFLKTBzLmCp4AaABAg',\n",
       "  'UgzTxhSWJ9-DAY_l3EF4AaABAg',\n",
       "  'Ugy_WQ5-1_k-sExJpGp4AaABAg',\n",
       "  'UgwmNQflD_ZiTD1efN14AaABAg',\n",
       "  'UgzrQYLKdUDiFd9ztc54AaABAg',\n",
       "  'Ugwm-Mo8xcVZN872HBV4AaABAg',\n",
       "  'Ugx7h-8jB3K3BnvAyIN4AaABAg',\n",
       "  'UgxwOqfwDr_ZA81KmKR4AaABAg',\n",
       "  'UgxbVacVcgAWGMr2BJZ4AaABAg',\n",
       "  'UgyHk6_MMF0ls75vYMF4AaABAg',\n",
       "  'UgxtKYo7XoVKUgInCH14AaABAg',\n",
       "  'UgzIWrFzRcqGM5TBRst4AaABAg',\n",
       "  'UgwfGJSNlOOd0SrYP3d4AaABAg',\n",
       "  'Ugws3y76WHB2VWilsB94AaABAg',\n",
       "  'UgxBYPo__lPKfLoM6fd4AaABAg',\n",
       "  'UgxR62v109TXwRS8m5Z4AaABAg',\n",
       "  'Ugxj2eeqbcPTpKF2oQV4AaABAg',\n",
       "  'Ugwll5jv56TlpdjyQZl4AaABAg',\n",
       "  'UgxUxYs-Vcf-DnFjzI94AaABAg',\n",
       "  'UgwYf4LQ_D5F3NsstTl4AaABAg',\n",
       "  'UgyYHQp_rhbJS3mkm1x4AaABAg',\n",
       "  'UgwXCFuP03-hzXEQ8HZ4AaABAg',\n",
       "  'UgwNgCjEI0U0T7iUVoZ4AaABAg',\n",
       "  'Ugxp4HZqoj6mqTTDq4J4AaABAg',\n",
       "  'UgwunkeGrkaKYP5rkcp4AaABAg',\n",
       "  'UgzjkOVEFImhMYnj8T94AaABAg',\n",
       "  'Ugz1cQUdDAVU8MuBzQR4AaABAg',\n",
       "  'UgwPvCum8OLLO0dT4ZR4AaABAg',\n",
       "  'UgzwscuB6odbn7t4sR94AaABAg',\n",
       "  'UgyHP1Ew3qKVf5fAKY14AaABAg',\n",
       "  'Ugw_BOgK28DkTrNaEUV4AaABAg',\n",
       "  'UgzoTz-T2Tw_jTQl70V4AaABAg',\n",
       "  'UgygnXmpCCNMGiouTWF4AaABAg',\n",
       "  'Ugyvw5I5Ud3cyyrd5w14AaABAg',\n",
       "  'UgxvVDDIX5Nd-txoUjB4AaABAg',\n",
       "  'UgycwWcwZ7cvrnNz1ft4AaABAg',\n",
       "  'Ugw30gtpctw8SWr5ijJ4AaABAg',\n",
       "  'UgwdkcmwuDVqKAznnnN4AaABAg',\n",
       "  'UgwCGVtWhgovmgqhGO14AaABAg',\n",
       "  'Ugy_uyDbut89M-BFd5F4AaABAg',\n",
       "  'UgxPTYgMjBZTYUukAGp4AaABAg',\n",
       "  'Ugylx75dnEmalfNlwOh4AaABAg',\n",
       "  'UgxnP7YxPVBoiR2DYFh4AaABAg',\n",
       "  'UgyLVzw2n74CjcrxDzZ4AaABAg',\n",
       "  'UgxL_XirCmLmW0JNiaB4AaABAg',\n",
       "  'Ugxd4CIZyfj7yaxsWLd4AaABAg',\n",
       "  'UgxtcSlYJcz1HKZ7h5F4AaABAg',\n",
       "  'UgxftPVEzVvf6KuW11R4AaABAg',\n",
       "  'Ugy6EY0mtFcvjKlxMPl4AaABAg',\n",
       "  'UgycCXBe9HmD6T4jGcR4AaABAg',\n",
       "  'UgxuWC2J9QlNyurmYP14AaABAg',\n",
       "  'UgxlUMjhiqP_rw2rBFV4AaABAg',\n",
       "  'Ugy5HIie0MqgdOTXbrN4AaABAg',\n",
       "  'UgxaekspKxt7Z2uZk_x4AaABAg',\n",
       "  'UgzOYiWXQ0KfUs9j_nV4AaABAg',\n",
       "  'Ugx-13nzzmorUPojRVZ4AaABAg',\n",
       "  'Ugzmp0iTzxsqs-3sCiN4AaABAg',\n",
       "  'UgzF1wWMpy2QSOgyhCt4AaABAg',\n",
       "  'UgzCaSs0oWT8nPA47o14AaABAg',\n",
       "  'Ugwu_w8qlN45QWrQ3CN4AaABAg',\n",
       "  'UgxLIJk1h_lT4LpaLCh4AaABAg',\n",
       "  'Ugxhx3eCjUrV54XJkkt4AaABAg',\n",
       "  'UgxlBhrTuw2IHTKo7394AaABAg',\n",
       "  'UgyUboSy-nJfftrubIl4AaABAg',\n",
       "  'Ugx0eXr0k66sRa2-sDZ4AaABAg',\n",
       "  'UgwqUFRwEP1yT68ZEx94AaABAg',\n",
       "  'UgxcN-3B5b4EpD2z0ol4AaABAg',\n",
       "  'UgzbYoq7G5HKY_FdKrd4AaABAg',\n",
       "  'UgxSAQV-ZFGzfR3_upp4AaABAg',\n",
       "  'UgxZBLvqN1iGhpGtBFN4AaABAg',\n",
       "  'UgwoHd9_lmiB8Jg-bFp4AaABAg',\n",
       "  'UgykUyEAPyWGbT4zUXB4AaABAg',\n",
       "  'UgzofgRV8y_oWnt-WhN4AaABAg',\n",
       "  'UgxXRqrRbB5VQS0ggXV4AaABAg',\n",
       "  'UgxyFkTRUSeI3RMNtxl4AaABAg',\n",
       "  'UgziSpbyioxcWOF2R_x4AaABAg',\n",
       "  'UgzcBcxQucUyXZTRAbB4AaABAg',\n",
       "  'UgytuREVLK3X5bjsx9R4AaABAg',\n",
       "  'UgxExJ_56cpRpx9lKKt4AaABAg',\n",
       "  'UgwvkBLg6pKAc_Op8aV4AaABAg',\n",
       "  'UgyqWVpXmHRtfGXN9FV4AaABAg',\n",
       "  'UgxXRRtpZGaL1io1Lf14AaABAg',\n",
       "  'UgzpEaSK92a9MleQ2LB4AaABAg',\n",
       "  'UgwD7Ynt3D8Ifjv45jd4AaABAg',\n",
       "  'UgxXGyysBtefDcAPCTV4AaABAg',\n",
       "  'UgycwWcwZ7cvrnNz1ft4AaABAg',\n",
       "  'Ugw30gtpctw8SWr5ijJ4AaABAg',\n",
       "  'UgwdkcmwuDVqKAznnnN4AaABAg',\n",
       "  'UgwCGVtWhgovmgqhGO14AaABAg',\n",
       "  'Ugy_uyDbut89M-BFd5F4AaABAg',\n",
       "  'UgxPTYgMjBZTYUukAGp4AaABAg',\n",
       "  'Ugylx75dnEmalfNlwOh4AaABAg',\n",
       "  'UgxnP7YxPVBoiR2DYFh4AaABAg',\n",
       "  'UgyLVzw2n74CjcrxDzZ4AaABAg',\n",
       "  'UgxL_XirCmLmW0JNiaB4AaABAg',\n",
       "  'Ugxd4CIZyfj7yaxsWLd4AaABAg',\n",
       "  'UgxtcSlYJcz1HKZ7h5F4AaABAg',\n",
       "  'UgxftPVEzVvf6KuW11R4AaABAg',\n",
       "  'Ugy6EY0mtFcvjKlxMPl4AaABAg',\n",
       "  'UgycCXBe9HmD6T4jGcR4AaABAg',\n",
       "  'UgxuWC2J9QlNyurmYP14AaABAg',\n",
       "  'UgxlUMjhiqP_rw2rBFV4AaABAg',\n",
       "  'Ugy5HIie0MqgdOTXbrN4AaABAg',\n",
       "  'UgxaekspKxt7Z2uZk_x4AaABAg',\n",
       "  'UgzOYiWXQ0KfUs9j_nV4AaABAg',\n",
       "  'Ugx-13nzzmorUPojRVZ4AaABAg',\n",
       "  'Ugzmp0iTzxsqs-3sCiN4AaABAg',\n",
       "  'UgzF1wWMpy2QSOgyhCt4AaABAg',\n",
       "  'UgzCaSs0oWT8nPA47o14AaABAg',\n",
       "  'Ugwu_w8qlN45QWrQ3CN4AaABAg',\n",
       "  'UgxLIJk1h_lT4LpaLCh4AaABAg',\n",
       "  'Ugxhx3eCjUrV54XJkkt4AaABAg',\n",
       "  'UgxlBhrTuw2IHTKo7394AaABAg',\n",
       "  'UgyUboSy-nJfftrubIl4AaABAg',\n",
       "  'Ugx0eXr0k66sRa2-sDZ4AaABAg',\n",
       "  'UgwqUFRwEP1yT68ZEx94AaABAg',\n",
       "  'UgxcN-3B5b4EpD2z0ol4AaABAg',\n",
       "  'UgzbYoq7G5HKY_FdKrd4AaABAg',\n",
       "  'UgxSAQV-ZFGzfR3_upp4AaABAg',\n",
       "  'UgxZBLvqN1iGhpGtBFN4AaABAg',\n",
       "  'UgwoHd9_lmiB8Jg-bFp4AaABAg',\n",
       "  'UgykUyEAPyWGbT4zUXB4AaABAg',\n",
       "  'UgzofgRV8y_oWnt-WhN4AaABAg',\n",
       "  'UgxXRqrRbB5VQS0ggXV4AaABAg',\n",
       "  'UgxyFkTRUSeI3RMNtxl4AaABAg',\n",
       "  'UgziSpbyioxcWOF2R_x4AaABAg',\n",
       "  'UgzcBcxQucUyXZTRAbB4AaABAg',\n",
       "  'UgytuREVLK3X5bjsx9R4AaABAg',\n",
       "  'UgxExJ_56cpRpx9lKKt4AaABAg',\n",
       "  'UgwvkBLg6pKAc_Op8aV4AaABAg',\n",
       "  'UgyqWVpXmHRtfGXN9FV4AaABAg',\n",
       "  'UgxXRRtpZGaL1io1Lf14AaABAg',\n",
       "  'UgzpEaSK92a9MleQ2LB4AaABAg',\n",
       "  'UgwD7Ynt3D8Ifjv45jd4AaABAg',\n",
       "  'UgxXGyysBtefDcAPCTV4AaABAg',\n",
       "  'Ugwee77V9oKqIq2E62B4AaABAg',\n",
       "  'UgxN7rO-9fyFx6SdMvN4AaABAg',\n",
       "  'UgwR_SRDSl_g9_13Kll4AaABAg',\n",
       "  'UgwiPKFAGJMY0wBEQVp4AaABAg',\n",
       "  'Ugwnq38u5hxEaObM_oR4AaABAg',\n",
       "  'UgwY6G2NDvlJyt5gXu94AaABAg',\n",
       "  'Ugw20gG-SpsLrtRdTXx4AaABAg',\n",
       "  'Ugzq87kkHwffv_QQm9d4AaABAg',\n",
       "  'UgwO9LptkkZ9VEU5Xfd4AaABAg',\n",
       "  'UgyP9i2udN2Bx-RNmqx4AaABAg',\n",
       "  'Ugx2_U6hx4tk1uDppQp4AaABAg',\n",
       "  'UgyOmyu0Mwt_K-kTv354AaABAg',\n",
       "  'UgyvytAh1x6T-PK1St14AaABAg',\n",
       "  'UgwgCxzWj7EnNrxy1Jh4AaABAg',\n",
       "  'UgzQuszgczXz4UbyJTt4AaABAg',\n",
       "  'UgzTniGAqqF2Xu9D8Qp4AaABAg',\n",
       "  'Ugys7iUbwA11TtXbZn54AaABAg',\n",
       "  'UgyuIUeAbOEtb3TTmjl4AaABAg',\n",
       "  'Ugxjr0ucOmiYtl7J5Ix4AaABAg',\n",
       "  'UgwsL7Flqu7yjAAtsap4AaABAg',\n",
       "  'UgxB_LzEdzGg4zVb-ap4AaABAg',\n",
       "  'UgwlHJlTd7hsUw_KMwF4AaABAg',\n",
       "  'UgzEVqTe4xkkEZIDDx14AaABAg',\n",
       "  'Ugx6WpiP_l30zinkXXJ4AaABAg',\n",
       "  'UgwNwF8kCOhYLak6EQV4AaABAg',\n",
       "  'Ugy0eU-wvgv6OulCPnB4AaABAg',\n",
       "  'Ugz8Lh7wywg07u-QOVN4AaABAg',\n",
       "  'UgxINUKHqDwaqN_Ft3l4AaABAg',\n",
       "  'UgzYSqsLnopAqH6Pe6l4AaABAg',\n",
       "  'UgzWXdXSV6as7jqc1Ct4AaABAg',\n",
       "  'Ugw5dakfWk1RHwue1CR4AaABAg',\n",
       "  'UgzzwGrNIySRwsYPvPN4AaABAg',\n",
       "  'UgyppvpYgQAduYaWa2V4AaABAg',\n",
       "  'UgzzenAc7BxHO10LgW54AaABAg',\n",
       "  'Ugy-0B3lys6lo4yKbll4AaABAg',\n",
       "  'UgwhXEy5gnLocZMzmA54AaABAg',\n",
       "  'UgwKZMcdtBq_6EzVfAp4AaABAg',\n",
       "  'Ugxz_MOVTLgGRWEntrF4AaABAg',\n",
       "  'Ugy0t6NMg-vf05uvAQ94AaABAg',\n",
       "  'Ugzwd9aDZo44OMY28314AaABAg',\n",
       "  'UgzZsNYMpB9cGcrmPsJ4AaABAg',\n",
       "  'UgwrwzWXni6RdmdEZk54AaABAg',\n",
       "  'UgxpQvpv9d0ejY_OfRN4AaABAg',\n",
       "  'UgxQtYGItYB2o01R8DR4AaABAg',\n",
       "  'UgwWBz_4B-ZiJoC30EB4AaABAg',\n",
       "  'UgwSoxVpx_sprUTv_1h4AaABAg',\n",
       "  'UgxM1lcwSW9JL6LLDcN4AaABAg',\n",
       "  'UgwNB2wfe1-rJsc7HPh4AaABAg',\n",
       "  'UgyVw1WbxMNR4LL0Vnp4AaABAg',\n",
       "  'UgycwWcwZ7cvrnNz1ft4AaABAg',\n",
       "  'Ugw30gtpctw8SWr5ijJ4AaABAg',\n",
       "  'UgwdkcmwuDVqKAznnnN4AaABAg',\n",
       "  'UgwCGVtWhgovmgqhGO14AaABAg',\n",
       "  'Ugy_uyDbut89M-BFd5F4AaABAg',\n",
       "  'UgxPTYgMjBZTYUukAGp4AaABAg',\n",
       "  'Ugylx75dnEmalfNlwOh4AaABAg',\n",
       "  'UgxnP7YxPVBoiR2DYFh4AaABAg',\n",
       "  'UgyLVzw2n74CjcrxDzZ4AaABAg',\n",
       "  'UgxL_XirCmLmW0JNiaB4AaABAg',\n",
       "  'Ugxd4CIZyfj7yaxsWLd4AaABAg',\n",
       "  'UgxtcSlYJcz1HKZ7h5F4AaABAg',\n",
       "  'UgxftPVEzVvf6KuW11R4AaABAg',\n",
       "  'Ugy6EY0mtFcvjKlxMPl4AaABAg',\n",
       "  'UgycCXBe9HmD6T4jGcR4AaABAg',\n",
       "  'UgxuWC2J9QlNyurmYP14AaABAg',\n",
       "  'UgxlUMjhiqP_rw2rBFV4AaABAg',\n",
       "  'Ugy5HIie0MqgdOTXbrN4AaABAg',\n",
       "  'UgxaekspKxt7Z2uZk_x4AaABAg',\n",
       "  'UgzOYiWXQ0KfUs9j_nV4AaABAg',\n",
       "  'Ugx-13nzzmorUPojRVZ4AaABAg',\n",
       "  'Ugzmp0iTzxsqs-3sCiN4AaABAg',\n",
       "  'UgzF1wWMpy2QSOgyhCt4AaABAg',\n",
       "  'UgzCaSs0oWT8nPA47o14AaABAg',\n",
       "  'Ugwu_w8qlN45QWrQ3CN4AaABAg',\n",
       "  'UgxLIJk1h_lT4LpaLCh4AaABAg',\n",
       "  'Ugxhx3eCjUrV54XJkkt4AaABAg',\n",
       "  'UgxlBhrTuw2IHTKo7394AaABAg',\n",
       "  'UgyUboSy-nJfftrubIl4AaABAg',\n",
       "  'Ugx0eXr0k66sRa2-sDZ4AaABAg',\n",
       "  'UgwqUFRwEP1yT68ZEx94AaABAg',\n",
       "  'UgxcN-3B5b4EpD2z0ol4AaABAg',\n",
       "  'UgzbYoq7G5HKY_FdKrd4AaABAg',\n",
       "  'UgxSAQV-ZFGzfR3_upp4AaABAg',\n",
       "  'UgxZBLvqN1iGhpGtBFN4AaABAg',\n",
       "  'UgwoHd9_lmiB8Jg-bFp4AaABAg',\n",
       "  'UgykUyEAPyWGbT4zUXB4AaABAg',\n",
       "  'UgzofgRV8y_oWnt-WhN4AaABAg',\n",
       "  'UgxXRqrRbB5VQS0ggXV4AaABAg',\n",
       "  'UgxyFkTRUSeI3RMNtxl4AaABAg',\n",
       "  'UgziSpbyioxcWOF2R_x4AaABAg',\n",
       "  'UgzcBcxQucUyXZTRAbB4AaABAg',\n",
       "  'UgytuREVLK3X5bjsx9R4AaABAg',\n",
       "  'UgxExJ_56cpRpx9lKKt4AaABAg',\n",
       "  'UgwvkBLg6pKAc_Op8aV4AaABAg',\n",
       "  'UgyqWVpXmHRtfGXN9FV4AaABAg',\n",
       "  'UgxXRRtpZGaL1io1Lf14AaABAg',\n",
       "  'UgzpEaSK92a9MleQ2LB4AaABAg',\n",
       "  'UgwD7Ynt3D8Ifjv45jd4AaABAg',\n",
       "  'UgxXGyysBtefDcAPCTV4AaABAg',\n",
       "  'Ugwee77V9oKqIq2E62B4AaABAg',\n",
       "  'UgxN7rO-9fyFx6SdMvN4AaABAg',\n",
       "  'UgwR_SRDSl_g9_13Kll4AaABAg',\n",
       "  'UgwiPKFAGJMY0wBEQVp4AaABAg',\n",
       "  'Ugwnq38u5hxEaObM_oR4AaABAg',\n",
       "  'UgwY6G2NDvlJyt5gXu94AaABAg',\n",
       "  'Ugw20gG-SpsLrtRdTXx4AaABAg',\n",
       "  'Ugzq87kkHwffv_QQm9d4AaABAg',\n",
       "  'UgwO9LptkkZ9VEU5Xfd4AaABAg',\n",
       "  'UgyP9i2udN2Bx-RNmqx4AaABAg',\n",
       "  'Ugx2_U6hx4tk1uDppQp4AaABAg',\n",
       "  'UgyOmyu0Mwt_K-kTv354AaABAg',\n",
       "  'UgyvytAh1x6T-PK1St14AaABAg',\n",
       "  'UgwgCxzWj7EnNrxy1Jh4AaABAg',\n",
       "  'UgzQuszgczXz4UbyJTt4AaABAg',\n",
       "  'UgzTniGAqqF2Xu9D8Qp4AaABAg',\n",
       "  'Ugys7iUbwA11TtXbZn54AaABAg',\n",
       "  'UgyuIUeAbOEtb3TTmjl4AaABAg',\n",
       "  'Ugxjr0ucOmiYtl7J5Ix4AaABAg',\n",
       "  'UgwsL7Flqu7yjAAtsap4AaABAg',\n",
       "  'UgxB_LzEdzGg4zVb-ap4AaABAg',\n",
       "  'UgwlHJlTd7hsUw_KMwF4AaABAg',\n",
       "  'UgzEVqTe4xkkEZIDDx14AaABAg',\n",
       "  'Ugx6WpiP_l30zinkXXJ4AaABAg',\n",
       "  'UgwNwF8kCOhYLak6EQV4AaABAg',\n",
       "  'Ugy0eU-wvgv6OulCPnB4AaABAg',\n",
       "  'Ugz8Lh7wywg07u-QOVN4AaABAg',\n",
       "  'UgxINUKHqDwaqN_Ft3l4AaABAg',\n",
       "  'UgzYSqsLnopAqH6Pe6l4AaABAg',\n",
       "  'UgzWXdXSV6as7jqc1Ct4AaABAg',\n",
       "  'Ugw5dakfWk1RHwue1CR4AaABAg',\n",
       "  'UgzzwGrNIySRwsYPvPN4AaABAg',\n",
       "  'UgyppvpYgQAduYaWa2V4AaABAg',\n",
       "  'UgzzenAc7BxHO10LgW54AaABAg',\n",
       "  'Ugy-0B3lys6lo4yKbll4AaABAg',\n",
       "  'UgwhXEy5gnLocZMzmA54AaABAg',\n",
       "  'UgwKZMcdtBq_6EzVfAp4AaABAg',\n",
       "  'Ugxz_MOVTLgGRWEntrF4AaABAg',\n",
       "  'Ugy0t6NMg-vf05uvAQ94AaABAg',\n",
       "  'Ugzwd9aDZo44OMY28314AaABAg',\n",
       "  'UgzZsNYMpB9cGcrmPsJ4AaABAg',\n",
       "  'UgwrwzWXni6RdmdEZk54AaABAg',\n",
       "  'UgxpQvpv9d0ejY_OfRN4AaABAg',\n",
       "  'UgxQtYGItYB2o01R8DR4AaABAg',\n",
       "  'UgwWBz_4B-ZiJoC30EB4AaABAg',\n",
       "  'UgwSoxVpx_sprUTv_1h4AaABAg',\n",
       "  'UgxM1lcwSW9JL6LLDcN4AaABAg',\n",
       "  'UgwNB2wfe1-rJsc7HPh4AaABAg',\n",
       "  'UgyVw1WbxMNR4LL0Vnp4AaABAg',\n",
       "  'UgzgRywrZohLvEIaAf94AaABAg',\n",
       "  'UgySu7j-j53RIp-hJhl4AaABAg',\n",
       "  'UgyW3lLHWFsJI1tYPZl4AaABAg',\n",
       "  'UgxwFTq93oUfPvD2eFZ4AaABAg',\n",
       "  'Ugwu3KqsXe-Kij6dUrp4AaABAg',\n",
       "  'Ugw3OLXEwDS0Oieh_4R4AaABAg',\n",
       "  'UgwKSJUQE6O53rwO-ut4AaABAg',\n",
       "  'Ugy7BBQSGngjdeJ9qYZ4AaABAg',\n",
       "  'UgypEoJnLpP3OPEr01d4AaABAg',\n",
       "  'UgwRg-rLz6mK4-hIbFp4AaABAg',\n",
       "  'UgxLWut9RG9z4kdgB0d4AaABAg',\n",
       "  'UgweJinK6a2d7sZfygR4AaABAg',\n",
       "  'UgwCUkhmn0CvlPPP6mN4AaABAg',\n",
       "  'Ugxo2xq51orilxMxR154AaABAg',\n",
       "  'Ugwm6KwcnM5lNeKOkbh4AaABAg',\n",
       "  'Ugwx6tHbNNO70CRMf1t4AaABAg',\n",
       "  'UgyaogUiVdfZUC7vJbx4AaABAg',\n",
       "  'UgyimMj4cWulc94F9wR4AaABAg',\n",
       "  'UgygmEUBmkQgez264UB4AaABAg',\n",
       "  'UgxeNYhW-mC_1OVWL-F4AaABAg',\n",
       "  'UgxZHGDd9xlFSZ7sNEd4AaABAg',\n",
       "  'UgxIv6EdXdLZWAbPlIJ4AaABAg',\n",
       "  'Ugwdl01lWoasdfWb5D94AaABAg',\n",
       "  'UgyBY5lD1zv6xdT5iQx4AaABAg',\n",
       "  'UgwoK60J4JKZR5ypgjl4AaABAg',\n",
       "  'UgwIrVtMXRw7lbge48l4AaABAg',\n",
       "  'UgzEGTc71FZUhWh4VEd4AaABAg',\n",
       "  'UgxsS4paXm5NJNNY-k54AaABAg',\n",
       "  'UgyFUmGJj8-EX4Ir4yp4AaABAg',\n",
       "  'UgxYU1FAyHcPi1Yj5594AaABAg',\n",
       "  'UgxTerk-Vif4gDqN1Tt4AaABAg',\n",
       "  'Ugz_9Z3UvCH7uedgRyd4AaABAg',\n",
       "  'Ugw4uMvQ9V4R4Ya68jN4AaABAg',\n",
       "  'UgyucxwIFmiAi6xZawd4AaABAg',\n",
       "  'UgyOmNbE3v66UyUuitR4AaABAg',\n",
       "  'Ugxo8e4ICpIG_N3mDrp4AaABAg',\n",
       "  'UgxmuTuelrhfUX8Zz4h4AaABAg',\n",
       "  'UgyW--fMTsKj3U7aKkJ4AaABAg',\n",
       "  'UgxS3f8DVXnv85JOD-54AaABAg',\n",
       "  'Ugw2h4YjCSGXrWfzTVx4AaABAg',\n",
       "  'UgwkDsa1Ok6HGrR80zd4AaABAg',\n",
       "  'UgxaDF-RpIe8Y2eTiMl4AaABAg',\n",
       "  'UgwfZfJuim5vtK0J1MV4AaABAg',\n",
       "  'UgwQ7RTAbvun_bEybUx4AaABAg',\n",
       "  'UgxlGdac7Dsu_AZMUbd4AaABAg',\n",
       "  'UgxyirJ9IYUhzFDl5vd4AaABAg',\n",
       "  'Ugw_zbr7vHv8BLtt-4Z4AaABAg',\n",
       "  'Ugw2pmg1PHIzyKsibB14AaABAg',\n",
       "  'UgzqpqcXZwhvV1ewMat4AaABAg',\n",
       "  'UgxnPyBVzqnzrqzZxF54AaABAg',\n",
       "  'Ugy_7NlzOTXzukxL-Pp4AaABAg',\n",
       "  'UgyRMPfEFkcLliaC9dF4AaABAg',\n",
       "  'UgxBFfEetag765-bKp14AaABAg',\n",
       "  'UgykuH8FUdoI_ZMWty14AaABAg',\n",
       "  'UgxLgjsuaGKKFXDR-oZ4AaABAg',\n",
       "  'Ugzbr9En4zgD8qfnoSV4AaABAg',\n",
       "  'Ugxtkb227yzX409lzwV4AaABAg',\n",
       "  'UgwKQLLDb6D118z84AF4AaABAg',\n",
       "  'Ugx8brK-hQjTlFZoEkZ4AaABAg',\n",
       "  'UgxWwBYNNNRUav1hDZ94AaABAg',\n",
       "  'Ugxx7jFVe0mFKunGj594AaABAg',\n",
       "  'UgzI7DQTETW4p7_Lm-d4AaABAg',\n",
       "  'UgwqHxDm2qTT9I913ul4AaABAg',\n",
       "  'Ugyuyn2PHXbPGM0KQ9N4AaABAg',\n",
       "  'UgwK1HMDiTu9pKH5KxR4AaABAg',\n",
       "  'Ugw6UaHgZHk3Mqb8jpd4AaABAg',\n",
       "  'Ugydy4gSJhvX_XFXLV94AaABAg',\n",
       "  'UgyXKlKcGKVbFOAcCYN4AaABAg',\n",
       "  'UgwWfiwiEZHn8xEhqlt4AaABAg',\n",
       "  'Ugz300J_YT9ifhB0Rxh4AaABAg',\n",
       "  'UgwcVCMERSI0wNxYBxN4AaABAg',\n",
       "  'UgwWv4A32T5tC96z2rB4AaABAg',\n",
       "  'UgwxVId_bNGWaOJRpmx4AaABAg',\n",
       "  'UgxsgavmmcQfNHVCYtl4AaABAg',\n",
       "  'UgwtWVK5einWCl-8GTB4AaABAg',\n",
       "  'UgyG9aDNQIWkqTUZ48x4AaABAg',\n",
       "  'UgxgnJ2LMiQqkxJCLSp4AaABAg',\n",
       "  'UgzopTfNWBwhkC1xZHN4AaABAg',\n",
       "  'UgyzLKoEOQg-_2JiHWt4AaABAg',\n",
       "  'UgwPUQj-Sgdwwz-yKKZ4AaABAg',\n",
       "  'UgyRE4SazeQopXMlLZp4AaABAg',\n",
       "  'UgxUoCLUQUnkft75oWR4AaABAg',\n",
       "  'UgxfFcLuMYpTNmVzvSN4AaABAg',\n",
       "  'Ugx67QXrRqwbbG9pjOd4AaABAg',\n",
       "  'UgwRg-rLz6mK4-hIbFp4AaABAg',\n",
       "  'UgxLWut9RG9z4kdgB0d4AaABAg',\n",
       "  'UgweJinK6a2d7sZfygR4AaABAg',\n",
       "  'UgwCUkhmn0CvlPPP6mN4AaABAg',\n",
       "  'Ugxo2xq51orilxMxR154AaABAg',\n",
       "  'Ugwm6KwcnM5lNeKOkbh4AaABAg',\n",
       "  'Ugwx6tHbNNO70CRMf1t4AaABAg',\n",
       "  'UgyaogUiVdfZUC7vJbx4AaABAg',\n",
       "  'UgyimMj4cWulc94F9wR4AaABAg',\n",
       "  'UgygmEUBmkQgez264UB4AaABAg',\n",
       "  'UgxeNYhW-mC_1OVWL-F4AaABAg',\n",
       "  'UgxZHGDd9xlFSZ7sNEd4AaABAg',\n",
       "  'UgxIv6EdXdLZWAbPlIJ4AaABAg',\n",
       "  'Ugwdl01lWoasdfWb5D94AaABAg',\n",
       "  'UgyBY5lD1zv6xdT5iQx4AaABAg',\n",
       "  'UgwoK60J4JKZR5ypgjl4AaABAg',\n",
       "  'UgwIrVtMXRw7lbge48l4AaABAg',\n",
       "  'UgzEGTc71FZUhWh4VEd4AaABAg',\n",
       "  'UgxsS4paXm5NJNNY-k54AaABAg',\n",
       "  'UgyFUmGJj8-EX4Ir4yp4AaABAg',\n",
       "  'UgxYU1FAyHcPi1Yj5594AaABAg',\n",
       "  'UgxTerk-Vif4gDqN1Tt4AaABAg',\n",
       "  'Ugz_9Z3UvCH7uedgRyd4AaABAg',\n",
       "  'Ugw4uMvQ9V4R4Ya68jN4AaABAg',\n",
       "  'UgyucxwIFmiAi6xZawd4AaABAg',\n",
       "  'UgyOmNbE3v66UyUuitR4AaABAg',\n",
       "  'Ugxo8e4ICpIG_N3mDrp4AaABAg',\n",
       "  'UgxmuTuelrhfUX8Zz4h4AaABAg',\n",
       "  'UgyW--fMTsKj3U7aKkJ4AaABAg',\n",
       "  'UgxS3f8DVXnv85JOD-54AaABAg',\n",
       "  'Ugw2h4YjCSGXrWfzTVx4AaABAg',\n",
       "  'UgwkDsa1Ok6HGrR80zd4AaABAg',\n",
       "  'UgxaDF-RpIe8Y2eTiMl4AaABAg',\n",
       "  'UgwfZfJuim5vtK0J1MV4AaABAg',\n",
       "  'UgwQ7RTAbvun_bEybUx4AaABAg',\n",
       "  'UgxlGdac7Dsu_AZMUbd4AaABAg',\n",
       "  'UgxyirJ9IYUhzFDl5vd4AaABAg',\n",
       "  'Ugw_zbr7vHv8BLtt-4Z4AaABAg',\n",
       "  'Ugw2pmg1PHIzyKsibB14AaABAg',\n",
       "  'UgzqpqcXZwhvV1ewMat4AaABAg',\n",
       "  'UgxnPyBVzqnzrqzZxF54AaABAg',\n",
       "  'Ugy_7NlzOTXzukxL-Pp4AaABAg',\n",
       "  'UgyRMPfEFkcLliaC9dF4AaABAg',\n",
       "  'UgxBFfEetag765-bKp14AaABAg',\n",
       "  'UgykuH8FUdoI_ZMWty14AaABAg',\n",
       "  'UgxLgjsuaGKKFXDR-oZ4AaABAg',\n",
       "  'Ugzbr9En4zgD8qfnoSV4AaABAg',\n",
       "  'Ugxtkb227yzX409lzwV4AaABAg',\n",
       "  'UgwKQLLDb6D118z84AF4AaABAg',\n",
       "  'Ugx8brK-hQjTlFZoEkZ4AaABAg',\n",
       "  'UgxWwBYNNNRUav1hDZ94AaABAg',\n",
       "  'Ugxx7jFVe0mFKunGj594AaABAg',\n",
       "  'UgzI7DQTETW4p7_Lm-d4AaABAg',\n",
       "  'UgwqHxDm2qTT9I913ul4AaABAg',\n",
       "  'Ugyuyn2PHXbPGM0KQ9N4AaABAg',\n",
       "  'UgwK1HMDiTu9pKH5KxR4AaABAg',\n",
       "  'Ugw6UaHgZHk3Mqb8jpd4AaABAg',\n",
       "  'Ugydy4gSJhvX_XFXLV94AaABAg',\n",
       "  'UgyXKlKcGKVbFOAcCYN4AaABAg',\n",
       "  'UgwWfiwiEZHn8xEhqlt4AaABAg',\n",
       "  'Ugz300J_YT9ifhB0Rxh4AaABAg',\n",
       "  'UgwcVCMERSI0wNxYBxN4AaABAg',\n",
       "  'UgwWv4A32T5tC96z2rB4AaABAg',\n",
       "  'UgwxVId_bNGWaOJRpmx4AaABAg',\n",
       "  'UgxsgavmmcQfNHVCYtl4AaABAg',\n",
       "  'UgwtWVK5einWCl-8GTB4AaABAg',\n",
       "  'UgyG9aDNQIWkqTUZ48x4AaABAg',\n",
       "  'UgxgnJ2LMiQqkxJCLSp4AaABAg',\n",
       "  'UgzopTfNWBwhkC1xZHN4AaABAg',\n",
       "  'UgyzLKoEOQg-_2JiHWt4AaABAg',\n",
       "  'UgwPUQj-Sgdwwz-yKKZ4AaABAg',\n",
       "  'UgyRE4SazeQopXMlLZp4AaABAg',\n",
       "  'UgxUoCLUQUnkft75oWR4AaABAg',\n",
       "  'UgxfFcLuMYpTNmVzvSN4AaABAg',\n",
       "  'Ugx67QXrRqwbbG9pjOd4AaABAg',\n",
       "  'UgzS-LE_XJ8xlA0wHDx4AaABAg',\n",
       "  'Ugydo4bhPlsBqArnEbt4AaABAg',\n",
       "  'UgyH2sEQXvpmvQBeWd94AaABAg',\n",
       "  'Ugw_93Xixw3CZgXyaVt4AaABAg',\n",
       "  'UgwgatFbvME-iUMPMj14AaABAg',\n",
       "  'UgyXg6KE3caIsAnFSoR4AaABAg',\n",
       "  'Ugx5M39LX-2zNyfJSv14AaABAg',\n",
       "  'Ugzqu2uE-p4ZTPdt2p94AaABAg',\n",
       "  'UgxV1b0yHGHC4H6VUml4AaABAg',\n",
       "  'UgxRpXVwNvizwmHZDKN4AaABAg',\n",
       "  'Ugxkpa_jj0Gh0kH0WTl4AaABAg',\n",
       "  'UgzMc24C0W4nnXye0uF4AaABAg',\n",
       "  'UgyGHndZvhPLHgOVH2F4AaABAg',\n",
       "  'UgxrPkHq9tF_dS6egp54AaABAg',\n",
       "  'UgxCIdNiAhkJ6byTVPh4AaABAg',\n",
       "  'UgyAkOylQwcxwo1G-9p4AaABAg',\n",
       "  'UgzxbM5nMHdnIhqD3094AaABAg',\n",
       "  'UgxcnGYvjEDkXmtwN-Z4AaABAg',\n",
       "  'UgwsyreRWyRuz4FGhYp4AaABAg',\n",
       "  'Ugykgqh23z3dmnPIOlN4AaABAg',\n",
       "  'UgwRg-rLz6mK4-hIbFp4AaABAg',\n",
       "  'UgxLWut9RG9z4kdgB0d4AaABAg',\n",
       "  'UgweJinK6a2d7sZfygR4AaABAg',\n",
       "  'UgwCUkhmn0CvlPPP6mN4AaABAg',\n",
       "  'Ugxo2xq51orilxMxR154AaABAg',\n",
       "  'Ugwm6KwcnM5lNeKOkbh4AaABAg',\n",
       "  'Ugwx6tHbNNO70CRMf1t4AaABAg',\n",
       "  'UgyaogUiVdfZUC7vJbx4AaABAg',\n",
       "  'UgyimMj4cWulc94F9wR4AaABAg',\n",
       "  'UgygmEUBmkQgez264UB4AaABAg',\n",
       "  'UgxeNYhW-mC_1OVWL-F4AaABAg',\n",
       "  'UgxZHGDd9xlFSZ7sNEd4AaABAg',\n",
       "  'UgxIv6EdXdLZWAbPlIJ4AaABAg',\n",
       "  'Ugwdl01lWoasdfWb5D94AaABAg',\n",
       "  'UgyBY5lD1zv6xdT5iQx4AaABAg',\n",
       "  'UgwoK60J4JKZR5ypgjl4AaABAg',\n",
       "  'UgwIrVtMXRw7lbge48l4AaABAg',\n",
       "  'UgzEGTc71FZUhWh4VEd4AaABAg',\n",
       "  'UgxsS4paXm5NJNNY-k54AaABAg',\n",
       "  'UgyFUmGJj8-EX4Ir4yp4AaABAg',\n",
       "  'UgxYU1FAyHcPi1Yj5594AaABAg',\n",
       "  'UgxTerk-Vif4gDqN1Tt4AaABAg',\n",
       "  'Ugz_9Z3UvCH7uedgRyd4AaABAg',\n",
       "  'Ugw4uMvQ9V4R4Ya68jN4AaABAg',\n",
       "  'UgyucxwIFmiAi6xZawd4AaABAg',\n",
       "  'UgyOmNbE3v66UyUuitR4AaABAg',\n",
       "  'Ugxo8e4ICpIG_N3mDrp4AaABAg',\n",
       "  'UgxmuTuelrhfUX8Zz4h4AaABAg',\n",
       "  'UgyW--fMTsKj3U7aKkJ4AaABAg',\n",
       "  'UgxS3f8DVXnv85JOD-54AaABAg',\n",
       "  'Ugw2h4YjCSGXrWfzTVx4AaABAg',\n",
       "  'UgwkDsa1Ok6HGrR80zd4AaABAg',\n",
       "  'UgxaDF-RpIe8Y2eTiMl4AaABAg',\n",
       "  'UgwfZfJuim5vtK0J1MV4AaABAg',\n",
       "  'UgwQ7RTAbvun_bEybUx4AaABAg',\n",
       "  'UgxlGdac7Dsu_AZMUbd4AaABAg',\n",
       "  'UgxyirJ9IYUhzFDl5vd4AaABAg',\n",
       "  'Ugw_zbr7vHv8BLtt-4Z4AaABAg',\n",
       "  'Ugw2pmg1PHIzyKsibB14AaABAg',\n",
       "  'UgzqpqcXZwhvV1ewMat4AaABAg',\n",
       "  'UgxnPyBVzqnzrqzZxF54AaABAg',\n",
       "  'Ugy_7NlzOTXzukxL-Pp4AaABAg',\n",
       "  'UgyRMPfEFkcLliaC9dF4AaABAg',\n",
       "  'UgxBFfEetag765-bKp14AaABAg',\n",
       "  'UgykuH8FUdoI_ZMWty14AaABAg',\n",
       "  'UgxLgjsuaGKKFXDR-oZ4AaABAg',\n",
       "  'Ugzbr9En4zgD8qfnoSV4AaABAg',\n",
       "  'Ugxtkb227yzX409lzwV4AaABAg',\n",
       "  'UgwKQLLDb6D118z84AF4AaABAg',\n",
       "  'Ugx8brK-hQjTlFZoEkZ4AaABAg',\n",
       "  'UgxWwBYNNNRUav1hDZ94AaABAg',\n",
       "  'Ugxx7jFVe0mFKunGj594AaABAg',\n",
       "  'UgzI7DQTETW4p7_Lm-d4AaABAg',\n",
       "  'UgwqHxDm2qTT9I913ul4AaABAg',\n",
       "  'Ugyuyn2PHXbPGM0KQ9N4AaABAg',\n",
       "  'UgwK1HMDiTu9pKH5KxR4AaABAg',\n",
       "  'Ugw6UaHgZHk3Mqb8jpd4AaABAg',\n",
       "  'Ugydy4gSJhvX_XFXLV94AaABAg',\n",
       "  'UgyXKlKcGKVbFOAcCYN4AaABAg',\n",
       "  'UgwWfiwiEZHn8xEhqlt4AaABAg',\n",
       "  'Ugz300J_YT9ifhB0Rxh4AaABAg',\n",
       "  'UgwcVCMERSI0wNxYBxN4AaABAg',\n",
       "  'UgwWv4A32T5tC96z2rB4AaABAg',\n",
       "  'UgwxVId_bNGWaOJRpmx4AaABAg',\n",
       "  'UgxsgavmmcQfNHVCYtl4AaABAg',\n",
       "  'UgwtWVK5einWCl-8GTB4AaABAg',\n",
       "  'UgyG9aDNQIWkqTUZ48x4AaABAg',\n",
       "  'UgxgnJ2LMiQqkxJCLSp4AaABAg',\n",
       "  'UgzopTfNWBwhkC1xZHN4AaABAg',\n",
       "  'UgyzLKoEOQg-_2JiHWt4AaABAg',\n",
       "  'UgwPUQj-Sgdwwz-yKKZ4AaABAg',\n",
       "  'UgyRE4SazeQopXMlLZp4AaABAg',\n",
       "  'UgxUoCLUQUnkft75oWR4AaABAg',\n",
       "  'UgxfFcLuMYpTNmVzvSN4AaABAg',\n",
       "  'Ugx67QXrRqwbbG9pjOd4AaABAg',\n",
       "  'UgzS-LE_XJ8xlA0wHDx4AaABAg',\n",
       "  'Ugydo4bhPlsBqArnEbt4AaABAg',\n",
       "  'UgyH2sEQXvpmvQBeWd94AaABAg',\n",
       "  'Ugw_93Xixw3CZgXyaVt4AaABAg',\n",
       "  'UgwgatFbvME-iUMPMj14AaABAg',\n",
       "  'UgyXg6KE3caIsAnFSoR4AaABAg',\n",
       "  'Ugx5M39LX-2zNyfJSv14AaABAg',\n",
       "  'Ugzqu2uE-p4ZTPdt2p94AaABAg',\n",
       "  'UgxV1b0yHGHC4H6VUml4AaABAg',\n",
       "  'UgxRpXVwNvizwmHZDKN4AaABAg',\n",
       "  'Ugxkpa_jj0Gh0kH0WTl4AaABAg',\n",
       "  'UgzMc24C0W4nnXye0uF4AaABAg',\n",
       "  'UgyGHndZvhPLHgOVH2F4AaABAg',\n",
       "  'UgxrPkHq9tF_dS6egp54AaABAg',\n",
       "  'UgxCIdNiAhkJ6byTVPh4AaABAg',\n",
       "  'UgyAkOylQwcxwo1G-9p4AaABAg',\n",
       "  'UgzxbM5nMHdnIhqD3094AaABAg',\n",
       "  'UgxcnGYvjEDkXmtwN-Z4AaABAg',\n",
       "  'UgwsyreRWyRuz4FGhYp4AaABAg',\n",
       "  'Ugykgqh23z3dmnPIOlN4AaABAg',\n",
       "  'UgwLQ0T1-b-zqCLStph4AaABAg',\n",
       "  'UgydUx_eBwLxvUXE4cR4AaABAg',\n",
       "  'UgwGUKACZn8xf7oJBrN4AaABAg',\n",
       "  'UgwP-cuwDZJqnmsPiLt4AaABAg',\n",
       "  'UgylPHwWeLWWFV3QVgB4AaABAg',\n",
       "  'UgxZjlY-kgADyJ5Pe554AaABAg',\n",
       "  'UgxXqQgTT__OwPeHjlp4AaABAg',\n",
       "  'UgzVq684dYHaurtpf0h4AaABAg',\n",
       "  'UgzDYBEUWV6YHxlJSiJ4AaABAg',\n",
       "  'UgyeRt8SppyAkxKF7DV4AaABAg',\n",
       "  'Ugz-VZ93xiRym5M8GVB4AaABAg',\n",
       "  'UgyKZCvSZmENit91gUh4AaABAg',\n",
       "  'Ugy9Om2Aj0G_hye4s3N4AaABAg',\n",
       "  'UgwNoXEld7nplmB-t4x4AaABAg',\n",
       "  'UgwvY8FDsEjfkKxTzJx4AaABAg',\n",
       "  'UgwQ6V_6kP-CqcYfCHp4AaABAg',\n",
       "  'Ugx8qaf3FwohoQ5QDnt4AaABAg',\n",
       "  'UgxdDxHstBRzDsPxsWZ4AaABAg',\n",
       "  'Ugw3ADplfiDSGsXSJAZ4AaABAg',\n",
       "  'UgyBmmeyq38PQ16Dk9x4AaABAg',\n",
       "  'UgxuUvHblBYCYn7p_SJ4AaABAg',\n",
       "  ...],\n",
       " 'Video_id': ['j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  'j-P2-fJx_qA',\n",
       "  '7pEoD2WYekg',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'AmtFLw9Cu0w',\n",
       "  'q9rL_Igpby0',\n",
       "  'q9rL_Igpby0',\n",
       "  'q9rL_Igpby0',\n",
       "  'q9rL_Igpby0',\n",
       "  'q9rL_Igpby0',\n",
       "  'vdQDLMHT7-A',\n",
       "  'vdQDLMHT7-A',\n",
       "  'oB1ohIIhKUA',\n",
       "  'oB1ohIIhKUA',\n",
       "  'eUTOT4W4dOw',\n",
       "  'LLDGg_41gWE',\n",
       "  'cro3wTg9ma4',\n",
       "  'CuNAUituP0I',\n",
       "  'FkOGirOe3FI',\n",
       "  'FkOGirOe3FI',\n",
       "  'FkOGirOe3FI',\n",
       "  'jytrP1wA_Wc',\n",
       "  'vdQDLMHT7-A',\n",
       "  'vdQDLMHT7-A',\n",
       "  'oB1ohIIhKUA',\n",
       "  'oB1ohIIhKUA',\n",
       "  'eUTOT4W4dOw',\n",
       "  'LLDGg_41gWE',\n",
       "  'cro3wTg9ma4',\n",
       "  'CuNAUituP0I',\n",
       "  'FkOGirOe3FI',\n",
       "  'FkOGirOe3FI',\n",
       "  'FkOGirOe3FI',\n",
       "  'jytrP1wA_Wc',\n",
       "  'qzRJUYMPvtI',\n",
       "  '-MTRoSVYHVg',\n",
       "  'wtRtbz2_EuI',\n",
       "  'wtRtbz2_EuI',\n",
       "  'wtRtbz2_EuI',\n",
       "  'bIKDtN7Tb5w',\n",
       "  'bIKDtN7Tb5w',\n",
       "  'ftTHw8PXjy4',\n",
       "  'ftTHw8PXjy4',\n",
       "  'wtRtbz2_EuI',\n",
       "  'wtRtbz2_EuI',\n",
       "  'wtRtbz2_EuI',\n",
       "  'bIKDtN7Tb5w',\n",
       "  'bIKDtN7Tb5w',\n",
       "  'ftTHw8PXjy4',\n",
       "  'ftTHw8PXjy4',\n",
       "  '-WfD5WkYuI4',\n",
       "  'scMTPdplCr8',\n",
       "  'scMTPdplCr8',\n",
       "  'DjC_NQXN8jA',\n",
       "  'l6wbwznQv28',\n",
       "  'l6wbwznQv28',\n",
       "  'l6wbwznQv28',\n",
       "  '_ziv1ewbZss',\n",
       "  'L_mk4SZgXq8',\n",
       "  'wgJ9WX1ft28',\n",
       "  'wgJ9WX1ft28',\n",
       "  'wgJ9WX1ft28',\n",
       "  'wgJ9WX1ft28',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  'JZqbddpY-xI',\n",
       "  'JZqbddpY-xI',\n",
       "  'JZqbddpY-xI',\n",
       "  'a5j5mTdSCtY',\n",
       "  'a5j5mTdSCtY',\n",
       "  'a5j5mTdSCtY',\n",
       "  'syJ7-IoD9Z8',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'ggEikhQfM6A',\n",
       "  'ggEikhQfM6A',\n",
       "  'ggEikhQfM6A',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'XBDEQdwFyo8',\n",
       "  'wgJ9WX1ft28',\n",
       "  'wgJ9WX1ft28',\n",
       "  'wgJ9WX1ft28',\n",
       "  'wgJ9WX1ft28',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  '2GKw0UBGWZc',\n",
       "  'JZqbddpY-xI',\n",
       "  'JZqbddpY-xI',\n",
       "  'JZqbddpY-xI',\n",
       "  'a5j5mTdSCtY',\n",
       "  'a5j5mTdSCtY',\n",
       "  'a5j5mTdSCtY',\n",
       "  'syJ7-IoD9Z8',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'XDCtldW4eLk',\n",
       "  'ggEikhQfM6A',\n",
       "  'ggEikhQfM6A',\n",
       "  'ggEikhQfM6A',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'Xy7N0YRzml8',\n",
       "  'XBDEQdwFyo8',\n",
       "  '5G_CqUP2Djs',\n",
       "  '5G_CqUP2Djs',\n",
       "  '5G_CqUP2Djs',\n",
       "  '5G_CqUP2Djs',\n",
       "  '5G_CqUP2Djs',\n",
       "  '5G_CqUP2Djs',\n",
       "  'TQ0iTDkqL7s',\n",
       "  'g_y7aUy6bNE',\n",
       "  'ZsKWCP-fPEU',\n",
       "  'ZsKWCP-fPEU',\n",
       "  'ZsKWCP-fPEU',\n",
       "  'HRC5NiGcBo4',\n",
       "  'HRC5NiGcBo4',\n",
       "  'nMB4bU2YZvI',\n",
       "  'W2GgqXIR-UM',\n",
       "  'W2GgqXIR-UM',\n",
       "  'W2GgqXIR-UM',\n",
       "  'CRH47LN3Oew',\n",
       "  'CRH47LN3Oew',\n",
       "  'CRH47LN3Oew',\n",
       "  'CRH47LN3Oew',\n",
       "  'CRH47LN3Oew',\n",
       "  '5EZSh3A0xL4',\n",
       "  '5EZSh3A0xL4',\n",
       "  '5EZSh3A0xL4',\n",
       "  '5EZSh3A0xL4',\n",
       "  'BbXZ8bgzIwo',\n",
       "  'BbXZ8bgzIwo',\n",
       "  'BbXZ8bgzIwo',\n",
       "  'gdK5lNhp0PI',\n",
       "  'dPCdlo3rdGc',\n",
       "  'tcGjl5PZf94',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'tcGjl5PZf94',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'GI7zgAwEFoE',\n",
       "  '0IpCfCnTTEU',\n",
       "  'xZatN-njBUQ',\n",
       "  'xZatN-njBUQ',\n",
       "  'xZatN-njBUQ',\n",
       "  'CIj7llf_66o',\n",
       "  'tcGjl5PZf94',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'GI7zgAwEFoE',\n",
       "  '0IpCfCnTTEU',\n",
       "  'xZatN-njBUQ',\n",
       "  'xZatN-njBUQ',\n",
       "  'xZatN-njBUQ',\n",
       "  'CIj7llf_66o',\n",
       "  'B9eiN3aZjPs',\n",
       "  '-4HTlLiPQxY',\n",
       "  'J8Ku16yIVdA',\n",
       "  'rlYm9EWNNhY',\n",
       "  'MS323_BoGAM',\n",
       "  'jCnSG78xOgM',\n",
       "  'tcGjl5PZf94',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'VbqRLDfMR1s',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'yf7c4xrfsoI',\n",
       "  'GI7zgAwEFoE',\n",
       "  '0IpCfCnTTEU',\n",
       "  'xZatN-njBUQ',\n",
       "  'xZatN-njBUQ',\n",
       "  'xZatN-njBUQ',\n",
       "  'CIj7llf_66o',\n",
       "  'B9eiN3aZjPs',\n",
       "  '-4HTlLiPQxY',\n",
       "  'J8Ku16yIVdA',\n",
       "  'rlYm9EWNNhY',\n",
       "  'MS323_BoGAM',\n",
       "  'jCnSG78xOgM',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'J62lYtIq0M8',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'BoVCp7gSOdg',\n",
       "  'D5bgfxmSiMA',\n",
       "  'D5bgfxmSiMA',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'LIqtZxoT8Wk',\n",
       "  'ClMPxvVGQHE',\n",
       "  'ClMPxvVGQHE',\n",
       "  'ClMPxvVGQHE',\n",
       "  'v4mylTmx4bo',\n",
       "  'v4mylTmx4bo',\n",
       "  'v4mylTmx4bo',\n",
       "  'XHQNvonBrHE',\n",
       "  'XHQNvonBrHE',\n",
       "  'rEfmsK7hrOw',\n",
       "  'rEfmsK7hrOw',\n",
       "  'rEfmsK7hrOw',\n",
       "  'rEfmsK7hrOw',\n",
       "  'kkZCf9yykCo',\n",
       "  'kkZCf9yykCo',\n",
       "  'kkZCf9yykCo',\n",
       "  'nZSCysb4iAE',\n",
       "  'nZSCysb4iAE',\n",
       "  'nZSCysb4iAE',\n",
       "  'nZSCysb4iAE',\n",
       "  'nZSCysb4iAE',\n",
       "  '4wIP3e00rfo',\n",
       "  '4wIP3e00rfo',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'CTVimIcxcY8',\n",
       "  'CTVimIcxcY8',\n",
       "  'q2MVrPOotmU',\n",
       "  'q2MVrPOotmU',\n",
       "  'r8-DOtbYR0U',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'oeqEh0zEaDo',\n",
       "  'oeqEh0zEaDo',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'CTVimIcxcY8',\n",
       "  'CTVimIcxcY8',\n",
       "  'q2MVrPOotmU',\n",
       "  'q2MVrPOotmU',\n",
       "  'r8-DOtbYR0U',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'oeqEh0zEaDo',\n",
       "  'oeqEh0zEaDo',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'LEpYRRKFF9U',\n",
       "  'LEpYRRKFF9U',\n",
       "  'fH4iTrZVdRQ',\n",
       "  'fH4iTrZVdRQ',\n",
       "  'hRGpUd_44gI',\n",
       "  '5mQjfnnfOrI',\n",
       "  '952tUb8LOJE',\n",
       "  '952tUb8LOJE',\n",
       "  'P6TET6mu7rM',\n",
       "  'P6TET6mu7rM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  '_wkIl-WWbTM',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'LC8V_Ngvgpo',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'WklGzNPskPU',\n",
       "  'CTVimIcxcY8',\n",
       "  'CTVimIcxcY8',\n",
       "  'q2MVrPOotmU',\n",
       "  'q2MVrPOotmU',\n",
       "  'r8-DOtbYR0U',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'VWeDdU7tsPE',\n",
       "  'oeqEh0zEaDo',\n",
       "  'oeqEh0zEaDo',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'l5Hlqs1puos',\n",
       "  'LEpYRRKFF9U',\n",
       "  'LEpYRRKFF9U',\n",
       "  'fH4iTrZVdRQ',\n",
       "  'fH4iTrZVdRQ',\n",
       "  'hRGpUd_44gI',\n",
       "  '5mQjfnnfOrI',\n",
       "  '952tUb8LOJE',\n",
       "  '952tUb8LOJE',\n",
       "  'P6TET6mu7rM',\n",
       "  'P6TET6mu7rM',\n",
       "  'a6_ABY1EJVk',\n",
       "  'yn9jxQznkO0',\n",
       "  'yn9jxQznkO0',\n",
       "  'yn9jxQznkO0',\n",
       "  'tOJCLyrGax8',\n",
       "  'tOJCLyrGax8',\n",
       "  'tOJCLyrGax8',\n",
       "  'tOJCLyrGax8',\n",
       "  'tOJCLyrGax8',\n",
       "  'AzbPeJmbw2U',\n",
       "  'AzbPeJmbw2U',\n",
       "  'kCtOL0B5sqE',\n",
       "  'bpLHVIymBQE',\n",
       "  'bpLHVIymBQE',\n",
       "  'bpLHVIymBQE',\n",
       "  'bpLHVIymBQE',\n",
       "  'Yt2pMfd3ntU',\n",
       "  'Yt2pMfd3ntU',\n",
       "  'lG4v9g4rq80',\n",
       "  'Uz6GZlxT24c',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'unE9lmV7FvU',\n",
       "  'unE9lmV7FvU',\n",
       "  'UbTrAUCzmgg',\n",
       "  'w2HoNIhYtg4',\n",
       "  'p-J-vHFZ564',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'unE9lmV7FvU',\n",
       "  'unE9lmV7FvU',\n",
       "  'UbTrAUCzmgg',\n",
       "  'w2HoNIhYtg4',\n",
       "  'p-J-vHFZ564',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  '8kpvI8Wupd0',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'VIfZlD1o6sQ',\n",
       "  'VIfZlD1o6sQ',\n",
       "  'VIfZlD1o6sQ',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'ijL7BDkAYzs',\n",
       "  'ijL7BDkAYzs',\n",
       "  'ijL7BDkAYzs',\n",
       "  'ijL7BDkAYzs',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  'dpT2kTl7IPw',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  '9HY8KG2FWF0',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'n7SIhJPt70s',\n",
       "  'unE9lmV7FvU',\n",
       "  'unE9lmV7FvU',\n",
       "  'UbTrAUCzmgg',\n",
       "  'w2HoNIhYtg4',\n",
       "  'p-J-vHFZ564',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  'MZ372d1HEvg',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  '6iSh62GB8TU',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  'Zm0fiUuvkek',\n",
       "  '8kpvI8Wupd0',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'RWsBOSvl-2k',\n",
       "  'VIfZlD1o6sQ',\n",
       "  'VIfZlD1o6sQ',\n",
       "  'VIfZlD1o6sQ',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  'HQ4Jk7Y9VQw',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  '6DGfyjfE52U',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'ZhvpG5yXz7I',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'kCgzjpHh0yI',\n",
       "  'ijL7BDkAYzs',\n",
       "  'ijL7BDkAYzs',\n",
       "  'ijL7BDkAYzs',\n",
       "  'ijL7BDkAYzs',\n",
       "  '47Xm8Gibue8',\n",
       "  'MA2SAUXpMKU',\n",
       "  'MA2SAUXpMKU',\n",
       "  '4bC7pwsdg7k',\n",
       "  '4bC7pwsdg7k',\n",
       "  '4bC7pwsdg7k',\n",
       "  'vhXJP_yKkmM',\n",
       "  'Cvv6phjd8-8',\n",
       "  'MKrIV6ZNSwA',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  '4VWA4JLkWFM',\n",
       "  '4VWA4JLkWFM',\n",
       "  '4VWA4JLkWFM',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'WYoR7gjR3LM',\n",
       "  'WYoR7gjR3LM',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  '4VWA4JLkWFM',\n",
       "  '4VWA4JLkWFM',\n",
       "  '4VWA4JLkWFM',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'WYoR7gjR3LM',\n",
       "  'WYoR7gjR3LM',\n",
       "  'DqtqP9v_J6E',\n",
       "  'DqtqP9v_J6E',\n",
       "  'DqtqP9v_J6E',\n",
       "  'Gd5tAPIJRMg',\n",
       "  'Gd5tAPIJRMg',\n",
       "  'Gd5tAPIJRMg',\n",
       "  'Gd5tAPIJRMg',\n",
       "  '1KDWAxTCpIk',\n",
       "  '1KDWAxTCpIk',\n",
       "  'RFEPmHZs6D8',\n",
       "  'RFEPmHZs6D8',\n",
       "  'RFEPmHZs6D8',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'L4BO3y-3sA8',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  'fh4LGuzRTd4',\n",
       "  '4VWA4JLkWFM',\n",
       "  '4VWA4JLkWFM',\n",
       "  '4VWA4JLkWFM',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'x-xtpzrT65U',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  'efXXC8503eg',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  '1hneF335l1E',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'durdKVQbTUc',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'XjKX3Tq9hHM',\n",
       "  'WYoR7gjR3LM',\n",
       "  'WYoR7gjR3LM',\n",
       "  'DqtqP9v_J6E',\n",
       "  'DqtqP9v_J6E',\n",
       "  'DqtqP9v_J6E',\n",
       "  'Gd5tAPIJRMg',\n",
       "  'Gd5tAPIJRMg',\n",
       "  'Gd5tAPIJRMg',\n",
       "  'Gd5tAPIJRMg',\n",
       "  '1KDWAxTCpIk',\n",
       "  '1KDWAxTCpIk',\n",
       "  'RFEPmHZs6D8',\n",
       "  'RFEPmHZs6D8',\n",
       "  'RFEPmHZs6D8',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  'AnotBUVgmMA',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  '5d8p6BLNkCo',\n",
       "  'RZrSloPfFuY',\n",
       "  'RZrSloPfFuY',\n",
       "  'r65yrHGZZvw',\n",
       "  'r65yrHGZZvw',\n",
       "  'HXaztJoSZ-8',\n",
       "  'HXaztJoSZ-8',\n",
       "  'HXaztJoSZ-8',\n",
       "  'HXaztJoSZ-8',\n",
       "  'HXaztJoSZ-8',\n",
       "  'HXaztJoSZ-8',\n",
       "  'HXaztJoSZ-8',\n",
       "  'o7O-q3JTy3M',\n",
       "  'o7O-q3JTy3M',\n",
       "  ...],\n",
       " 'Comment_text': ['CI CD pipeline also included ?.....',\n",
       "  'After completion of these classes,would you help with doubts and interview preparation?',\n",
       "  'Please let me know how to join for live projects.',\n",
       "  'Can you change the timings from 8:30am -10:30am\\nSo that for US people it will be 10pm -12am .',\n",
       "  'If I pay 2000 today so will I get only recorded session or love session link as well?',\n",
       "  'Any contact number Sir',\n",
       "  'Domain sir',\n",
       "  'let us know the duration naresh we trust you will u take daily classes?',\n",
       "  'Well Done thanks!',\n",
       "  'Thanks for the upload',\n",
       "  'what are the system requirements for your project course. and what knowledge one should have prior to join this course.',\n",
       "  'Hi,please tell me timing of class.i want to join class.',\n",
       "  'How to enroll in this project? Is there a contact number so i can contact ans enroll in this class. I am really interested',\n",
       "  'heloo naresh will u include interview preparation as well like how to represent this project? and what thing we hv to take care liek that?',\n",
       "  'Naresh, are you already working on azure in your company???',\n",
       "  'Plz let me know if this project can be implemented within azure free 200$ credit',\n",
       "  \"Sir pls make videos to crack interview bcz we don't have experience in data engineer\",\n",
       "  'Hi Sir, will u do a project on streaming data?',\n",
       "  'Unitycatlog, RBAC etc will ne in 2nd project right??',\n",
       "  'Hi naresh \\nI am unable to join the WhatsApp group it was showing loading state from  longtime',\n",
       "  'Can you pls tell project name sir\\nN how many yoe we can claim',\n",
       "  'will there be discount for second project for the person who enrolled the first project',\n",
       "  'is these videos are enough for data engineering job ?',\n",
       "  'Please Include More Code Programs Than Theory....',\n",
       "  'Promo-SM ',\n",
       "  ' P r o m o s m',\n",
       "  'Very nice',\n",
       "  'Nice explanation bro   ',\n",
       "  'Simply superb ',\n",
       "  'Nice explanation  ',\n",
       "  'Nice explanation   ',\n",
       "  'Very useful',\n",
       "  \"Thanks bro... It's really amazing content video\",\n",
       "  'eagerly waiting for  project announcement',\n",
       "  'is these videos are enough for data engineering job ?',\n",
       "  'Please Include More Code Programs Than Theory....',\n",
       "  'Promo-SM ',\n",
       "  ' P r o m o s m',\n",
       "  'Very nice',\n",
       "  'Nice explanation bro   ',\n",
       "  'Simply superb ',\n",
       "  'Nice explanation  ',\n",
       "  'Nice explanation   ',\n",
       "  'Very useful',\n",
       "  \"Thanks bro... It's really amazing content video\",\n",
       "  'eagerly waiting for  project announcement',\n",
       "  \"Hi Naresh sir,\\nI'm from Andhra Pradesh (Vizianagaram) Please accept my LinkedIn request..I want to talk with you personally ASAP where I'm having an interview with TCS tomorrow... Please sir help me with this situation  please try to understand my intention.. I have some doubts... I don't have any time right now(tomorrow interview) actually... That's why I'm pinging you here..\",\n",
       "  'Hello sir, thanks for such good content. Will you not continue the next episodes for python?',\n",
       "  'when will u upload project?',\n",
       "  'can u please tell me how to join ur session',\n",
       "  'Can you let me know how a person from u.s. can benefit from this?',\n",
       "  'Again dhoka  i was expecting day to day activities during work from home. Can you Please create a individual video?',\n",
       "  \"Sir, I am preparing for data engineer interview from last three to four months. But don't know how to study and what topics to cover. Can you help me prepare for the interview?\",\n",
       "  'Still waiting for ans. What is day to day\\nActivity on work from home',\n",
       "  'can i get the link to the spreadsheet\\nThanks',\n",
       "  'when will u upload project?',\n",
       "  'can u please tell me how to join ur session',\n",
       "  'Can you let me know how a person from u.s. can benefit from this?',\n",
       "  'Again dhoka  i was expecting day to day activities during work from home. Can you Please create a individual video?',\n",
       "  \"Sir, I am preparing for data engineer interview from last three to four months. But don't know how to study and what topics to cover. Can you help me prepare for the interview?\",\n",
       "  'Still waiting for ans. What is day to day\\nActivity on work from home',\n",
       "  'can i get the link to the spreadsheet\\nThanks',\n",
       "  'hello sir ur content is superb also in betbeen theis  ads also quality ads ',\n",
       "  \"Hi Naresh sir,\\nI'm from Andhra Pradesh (Vizianagaram) Please accept my LinkedIn request..I want to talk with you personally ASAP where I'm having an interview with TCS tomorrow... Please sir help me with this situation  please try to understand my intention.. I have some doubts... I don't any time to wait actually... That's why I'm pinging you here..\",\n",
       "  'Hello Sir , can you please let me know how can I join your live sessions and the curriculum for the same . Thank you ',\n",
       "  'is hive outdated or dying??',\n",
       "  'If I am getting data from multiple sources,\\nHow can I direct specific data into particular table of rdbms',\n",
       "  'Thanks for the session',\n",
       "  'Hi Naresh, {Anyone can clear my doubt}\\nWhy did you mentioned 8 partitions= 4Nodes??\\nis 1 partition = 1executor??\\nActually 1partition = 1 Block/Node right??',\n",
       "  'When will next batch starting and project too',\n",
       "  \"Please show us how you made the dashboard, just looking at the dashboard isn't of any use to anyone..\",\n",
       "  'Very informative session',\n",
       "  'Hi sir can you give online training?',\n",
       "  'Please upload next videos',\n",
       "  'Sir when will you upload session 2?',\n",
       "  'By far the best architecture explanation of spark architecture ',\n",
       "  'Do you provide any online classes?',\n",
       "  'Well explained, thanks for that :)',\n",
       "  'really good way of teaching and explaining the things , all the things are understood properly',\n",
       "  \"Worker Node / VM diagram shows 2 executors in 1 virtual machine each with separate set of resources. Shouldn't it be 1 worker node on each virtual machine\",\n",
       "  'Do more videos',\n",
       "  'Sir session 3?',\n",
       "  'excellent explanation.',\n",
       "  'great session',\n",
       "  'Hi sir \\nCan you give online training sir?',\n",
       "  'promosm ',\n",
       "  'Nice explanation need more videos',\n",
       "  'thank u for all the session . could u pls come up with some project also . if possible I can work with u',\n",
       "  'Thank you very much',\n",
       "  'Hello Sir.\\nIs there any video to setup single node cluster in Google cloud',\n",
       "  'Hi!, I got clear explanation about RDD, very thanks for videos.',\n",
       "  'hi, can you show how to install the hadoop and all that things in that gcp vm instance',\n",
       "  'Hi, thanks for the videos.... Pls continue to upload more videos... U r explaining very well...',\n",
       "  'Super sir please post more videos',\n",
       "  'Please upload videos more with examples',\n",
       "  'Continue these videos',\n",
       "  'Nice video',\n",
       "  'If we use combiner then technically it shuffle partial (less) data. so I think it supports partial shuffling not avoid it. isnt it?',\n",
       "  \"Great explanation sir, I've watched many videos but they did'nt cleared this much in detail.\",\n",
       "  'Nice Explanation',\n",
       "  'Thanks for the explanation.\\nBut Sir I need one explanation , why the number of tasks will be 8 if the no of partitions is 2 ,\\nTotal no of tasks = (no of tasks per stage)* no of stages = 2*2 =4',\n",
       "  'thank you sir you explain very efficient way',\n",
       "  'Very good explanation ',\n",
       "  'Was this last session?',\n",
       "  'Hi are u provide spark with scala in-depth course.?',\n",
       "  'Continue the series please',\n",
       "  \"I'm unable to join in WhatsApp group, it says group is full\",\n",
       "  'Excellent sir',\n",
       "  'Superb explanation   ',\n",
       "  'Greate Explanation...Great Intention to inject the concept...',\n",
       "  'Thanks for your efforts to Make us understand',\n",
       "  'Im able to create cluster but unable to find dbfs.',\n",
       "  'Very informative session',\n",
       "  'Hi sir can you give online training?',\n",
       "  'Please upload next videos',\n",
       "  'Sir when will you upload session 2?',\n",
       "  'By far the best architecture explanation of spark architecture ',\n",
       "  'Do you provide any online classes?',\n",
       "  'Well explained, thanks for that :)',\n",
       "  'really good way of teaching and explaining the things , all the things are understood properly',\n",
       "  \"Worker Node / VM diagram shows 2 executors in 1 virtual machine each with separate set of resources. Shouldn't it be 1 worker node on each virtual machine\",\n",
       "  'Do more videos',\n",
       "  'Sir session 3?',\n",
       "  'excellent explanation.',\n",
       "  'great session',\n",
       "  'Hi sir \\nCan you give online training sir?',\n",
       "  'promosm ',\n",
       "  'Nice explanation need more videos',\n",
       "  'thank u for all the session . could u pls come up with some project also . if possible I can work with u',\n",
       "  'Thank you very much',\n",
       "  'Hello Sir.\\nIs there any video to setup single node cluster in Google cloud',\n",
       "  'Hi!, I got clear explanation about RDD, very thanks for videos.',\n",
       "  'hi, can you show how to install the hadoop and all that things in that gcp vm instance',\n",
       "  'Hi, thanks for the videos.... Pls continue to upload more videos... U r explaining very well...',\n",
       "  'Super sir please post more videos',\n",
       "  'Please upload videos more with examples',\n",
       "  'Continue these videos',\n",
       "  'Nice video',\n",
       "  'If we use combiner then technically it shuffle partial (less) data. so I think it supports partial shuffling not avoid it. isnt it?',\n",
       "  \"Great explanation sir, I've watched many videos but they did'nt cleared this much in detail.\",\n",
       "  'Nice Explanation',\n",
       "  'Thanks for the explanation.\\nBut Sir I need one explanation , why the number of tasks will be 8 if the no of partitions is 2 ,\\nTotal no of tasks = (no of tasks per stage)* no of stages = 2*2 =4',\n",
       "  'thank you sir you explain very efficient way',\n",
       "  'Very good explanation ',\n",
       "  'Was this last session?',\n",
       "  'Hi are u provide spark with scala in-depth course.?',\n",
       "  'Continue the series please',\n",
       "  \"I'm unable to join in WhatsApp group, it says group is full\",\n",
       "  'Excellent sir',\n",
       "  'Superb explanation   ',\n",
       "  'Greate Explanation...Great Intention to inject the concept...',\n",
       "  'Thanks for your efforts to Make us understand',\n",
       "  'Im able to create cluster but unable to find dbfs.',\n",
       "  'When will be the program starting again sir?',\n",
       "  'Is it good to dive cloud data engineer instead of bigdata?',\n",
       "  'I am interested in the Azure Data Engineer program.',\n",
       "  'Thank You',\n",
       "  'Thank you',\n",
       "  'Already filled form & registered. Thank you ',\n",
       "  'Sir pls create more application based videos on PySpark like these.',\n",
       "  'What is the use of \\\\$CONDITIONS in running SQL custom query??',\n",
       "  'Can you send your number sir',\n",
       "  'Getting Error like connection refused.',\n",
       "  \"Isn't sqoop depreciated?\",\n",
       "  'Please provide documents',\n",
       "  'In append example we have appended the same data.. then there should be a violation in primary key, but there is not as it got successfully executed..., can i please know why did that happen..',\n",
       "  'Is this channel is present in udemy with name FutureX SKILLS? if so I will buy  A Big Data Hadoop and Spark project for absolute beginners  course on udemy',\n",
       "  'very useful video .thank you sir.\\nsir in interview session you asked one question related mapper  question was why there are by default 4 mapper only. pls tell the ans . \\nand how many bydefault reducer in hadoop',\n",
       "  'hi \\nhow to use mysql in cloudera vm can you please help me',\n",
       "  'Good explanation sir, the doubts which I am having in the creation of sqoop job is clarified.  Thanks',\n",
       "  'thank you for the cloudera vm',\n",
       "  'do I have to pay or join your channel to use virtual machine/ software?',\n",
       "  'Excuse me \\nIs this lhe last version?\\nAnd how can we gets the links?',\n",
       "  'How to change password',\n",
       "  'Hi sir, could you also share how to install using docker.',\n",
       "  'Nice explanation',\n",
       "  'brother please create one website please , so that many people  and friends can join your  live classes india , love u bro  u also do good job like pepcoding and ineuron bangalore',\n",
       "  'very good , your vedios are in a way  we get concept also and also one interview  topics is also covered , well done brother , very less people like you who help our india .  best things you teach  whats required survioce  to the point  no time waste only get the jobs like that',\n",
       "  'Hi. I have such as interview question. Table A  \\n                                                                   year     old_salary   new_salary\\n                                                                   2015    20     50\\n                                                                    2016    30     60\\nNow they want i create Table B  from A  Table B\\n                                                                        year    Salary\\n                                                                        2015   20\\n                                                                        2015    50\\n                                                                        2016    30\\n                                                                        2016     60',\n",
       "  \"please make the drive open to all in 'view' mode so that we can access\",\n",
       "  'thnxs',\n",
       "  'There is an error with me while using lead, lag, rank, row_number functions in mysql workbench, why???',\n",
       "  \"Thanks for uploading this video brother  from 3 weeks I'm learning spark, pyspark and big data videos... it's very helpful.... really your efforts appreciable brother \",\n",
       "  'Very Helpful Sir...It would be great if you do some practical on SCD...\\n\\nThanks a lot...',\n",
       "  'bro i need 100 google cloud accs u can creat it ?, i will buy 14$/ 1 gg cloud accs or 100 azure accs .',\n",
       "  'I am yet to get complete grasp of the applying each persistence strategy, but your explanation of the need of persistence was good. Thank you.',\n",
       "  'Thanks for Clear explanation,',\n",
       "  'Can you create a video on SCD2 implementation in spark using python',\n",
       "  'How to decide the number for repartition and coalesce?',\n",
       "  'Hi..I have sent an email.. could you please enable access',\n",
       "  'Hi Team clever studies, can you please tell me how to join these Classes',\n",
       "  'bro i need 100 google cloud accs u can creat it ?, i will buy 14$/ 1 gg cloud accs or 100 azure accs .',\n",
       "  'I am yet to get complete grasp of the applying each persistence strategy, but your explanation of the need of persistence was good. Thank you.',\n",
       "  'Thanks for Clear explanation,',\n",
       "  'Can you create a video on SCD2 implementation in spark using python',\n",
       "  'How to decide the number for repartition and coalesce?',\n",
       "  'Hi..I have sent an email.. could you please enable access',\n",
       "  'Hi Team clever studies, can you please tell me how to join these Classes',\n",
       "  'Complete project from scratch to final report required',\n",
       "  'Thanks',\n",
       "  'Topic starts from 10:00\\nGood to understand the previous part. But not a mandate.',\n",
       "  'Doubt: tasks are assigned to the executor by driver or cluster manager..?',\n",
       "  'thanks for uploading this... when can we expect continuation of this topic sir??',\n",
       "  'Please use noise cancellation mic',\n",
       "  'bro i need 100 google cloud accs u can creat it ?, i will buy 14$/ 1 gg cloud accs or 100 azure accs .',\n",
       "  'I am yet to get complete grasp of the applying each persistence strategy, but your explanation of the need of persistence was good. Thank you.',\n",
       "  'Thanks for Clear explanation,',\n",
       "  'Can you create a video on SCD2 implementation in spark using python',\n",
       "  'How to decide the number for repartition and coalesce?',\n",
       "  'Hi..I have sent an email.. could you please enable access',\n",
       "  'Hi Team clever studies, can you please tell me how to join these Classes',\n",
       "  'Complete project from scratch to final report required',\n",
       "  'Thanks',\n",
       "  'Topic starts from 10:00\\nGood to understand the previous part. But not a mandate.',\n",
       "  'Doubt: tasks are assigned to the executor by driver or cluster manager..?',\n",
       "  'thanks for uploading this... when can we expect continuation of this topic sir??',\n",
       "  'Please use noise cancellation mic',\n",
       "  'I have 5 years of exp in SQL and Plsql ...I want to switch on hdfs with spark ...how to start here learning one by one...pls suggest me. .',\n",
       "  'please make more videos on Azure data engineering projects',\n",
       "  'the datset path is empty, pls check and upload',\n",
       "  'I know pyspark very well...is there any way to provide me job',\n",
       "  'I have recently started using Community Edition for learning purpose and I am not able to create Cluster since yesterday. \\n\\nLast time I remember it took around 30 minutes to create a Cluster, afterwards the Cluster is not creating even though after an hour.\\n\\nIs there any limit for Community Edition version? or it is just a Down Time? \\n\\nI am from India  btw',\n",
       "  'Where can we find session1?\\nThanks in advance',\n",
       "  'bro i need 100 google cloud accs u can creat it ?, i will buy 14$/ 1 gg cloud accs or 100 azure accs .',\n",
       "  'I am yet to get complete grasp of the applying each persistence strategy, but your explanation of the need of persistence was good. Thank you.',\n",
       "  'Thanks for Clear explanation,',\n",
       "  'Can you create a video on SCD2 implementation in spark using python',\n",
       "  'How to decide the number for repartition and coalesce?',\n",
       "  'Hi..I have sent an email.. could you please enable access',\n",
       "  'Hi Team clever studies, can you please tell me how to join these Classes',\n",
       "  'Complete project from scratch to final report required',\n",
       "  'Thanks',\n",
       "  'Topic starts from 10:00\\nGood to understand the previous part. But not a mandate.',\n",
       "  'Doubt: tasks are assigned to the executor by driver or cluster manager..?',\n",
       "  'thanks for uploading this... when can we expect continuation of this topic sir??',\n",
       "  'Please use noise cancellation mic',\n",
       "  'I have 5 years of exp in SQL and Plsql ...I want to switch on hdfs with spark ...how to start here learning one by one...pls suggest me. .',\n",
       "  'please make more videos on Azure data engineering projects',\n",
       "  'the datset path is empty, pls check and upload',\n",
       "  'I know pyspark very well...is there any way to provide me job',\n",
       "  'I have recently started using Community Edition for learning purpose and I am not able to create Cluster since yesterday. \\n\\nLast time I remember it took around 30 minutes to create a Cluster, afterwards the Cluster is not creating even though after an hour.\\n\\nIs there any limit for Community Edition version? or it is just a Down Time? \\n\\nI am from India  btw',\n",
       "  'Where can we find session1?\\nThanks in advance',\n",
       "  'your Download dataset link: https://drive.google.com/drive/folder... is empty , can you send me that dataset ,please',\n",
       "  'The mentioned drive is Empty there is no file!! Can you please check ?',\n",
       "  'Hi Sir, Could you please upload both file as curranty the mentioned folder is showing empty in google drive',\n",
       "  'I need raw data from you this retail project,  can you send me please',\n",
       "  'Hi, the GDRIVE folder is empty? can you check please?',\n",
       "  'the folder in GDRIVE is empty? can you check please?',\n",
       "  'Is dataset uploaded here ?',\n",
       "  'apart from changing the header (schema) what else is taught in this session, literally nothing wasting my time',\n",
       "  'create video on data pipeline',\n",
       "  'how many to build data pipeline',\n",
       "  'What is data pipeline',\n",
       "  'The pyspark link in description is not working',\n",
       "  'How can we save the file by max size as say 100GB?',\n",
       "  '7 % of 21 GB is 3 GB ? How',\n",
       "  'Wonderful Explanation, Sir. \\n\\nBut please clarify me one thing: Do we have dedicated Application Master for every worker node or just a single Application master for the entire cluster?? If the latter is true, why are we dedicating one core and 1.5 GB RAM from every worker node?\\n\\nThank you',\n",
       "  '21 of 7% is not 3.how',\n",
       "  'This is the best explanation. Thanks a lot.',\n",
       "  '7% of 21 is not 3GB, even 10% of 21 is 2.1GB, @ 4:15',\n",
       "  'Thanks for the info. We need to discuss cluster creation based on input data size sir.',\n",
       "  'What about driver memory???',\n",
       "  'As per this calculation we have about 540 gb memory available with all 39 executor, does it mean we can not process file larger than this size. If it has to read 1080 gb file, how will it process. Just trying to understand',\n",
       "  'Thank you for the video. please explain how you computed 7% of 21 = 3',\n",
       "  'Thanks for the explanantion, can you please cover about driver memory config also in coming vedios?',\n",
       "  'How to calculate cluster configuration based on our data size?',\n",
       "  'How to decide these parameters based on the data size that need to be processed, as there can be multiple spark applications running in big clusters.',\n",
       "  'Thanks a lot for making such lucid videos....You are doing a great job......\\nAnd I intentionally dont skip the ads on your videos to support you as much as possible...Thanks man.',\n",
       "  \"Hi Very good explanation. You didn't speak about Disk memory?\",\n",
       "  'Do u have any courses for in depth spark.?',\n",
       "  'Can we get entire end to end project course about big data .?',\n",
       "  'thanks a lot sir, for this video.',\n",
       "  'Very informative. Thank you!',\n",
       "  'Very informative.... Amazing Video....',\n",
       "  'Most useful video....great starting all d best sir',\n",
       "  'Thank you so much.. this will help me in my future interviews a lot..',\n",
       "  \"I'm a jee aspirant.I find this very helpful for my future interview and project explaination. Thank you\",\n",
       "  'Very informative video. Thank you for sharing this.',\n",
       "  'Hey, I had written you an email about the study material. Please reply back with the link. Thank you',\n",
       "  'Can you share me contact number of trainer',\n",
       "  'Can you please post videos of each phase using spark',\n",
       "  'Great video!',\n",
       "  'Nice explanation sir thank you',\n",
       "  'Nice explanation sir...',\n",
       "  'Good information',\n",
       "  'Will u start scala',\n",
       "  'Can we use spark for  data processing stage and store data in hdfs after cleansing?',\n",
       "  'Superbly explained ',\n",
       "  'Crystal Clear explanation for all the beginners',\n",
       "  'thanks a lot',\n",
       "  'Keep making content like this!!',\n",
       "  'Sir, you are explaining such complex things so simply. Very nice sir and really helpful for me preparing interview.',\n",
       "  'excellent series....  eagerly waiting for the next part...',\n",
       "  'Useless course I did it. Dont know why it cost 2000 $ . Its very basic. Its just promotion stunt by data brick.',\n",
       "  'Thank you Naresh',\n",
       "  'Thanks',\n",
       "  'Thanks a lot',\n",
       "  'Thanks for the Information..',\n",
       "  'Yes how we can attend sessions \\nAny watsapp group available',\n",
       "  'Hi Team,\\nHow to join the session on Sunday and also please share the material link of Google Drive',\n",
       "  'Nice But I reached here for finding an answer -> what if job run multiple time a day - Date partition is same but as the job is running at append mode so for each date folder we get multiple files which are mostly duplicates. How to get 1 updated file only inside date folder all the time ( irrespective of this job run multiple time in a day )',\n",
       "  \"This approach assumes that the source is sending incrementals. If it's a full file everyday, how do you identify the delta prior to load into spark warehouse\",\n",
       "  'Nice explanation  ',\n",
       "  'Clearly explained thanks for posting',\n",
       "  'same question asked in my interview . I wish if I would have seen this before ',\n",
       "  'Very useful',\n",
       "  'Hi Sir \\ni have around 7 years of exp into oracle sql/pl sql and trying to make transition into big data field .is there any course are any big data course are you providing in online',\n",
       "  'hi i have install hive along with hadoop and I have stud table there but when I am trying to access using pyspark it is showing table is not present what can be solution',\n",
       "  'Hi, nice explanation but I want to know more about the data flow from the hive tables to becoming a spark dataframe',\n",
       "  'Hi sir nice explaination....\\nCould you please create dataframe from hive using intellij ide',\n",
       "  'Very neat explanation',\n",
       "  '',\n",
       "  'Sir, I have one scenario, on s3 we have data file with size of 500gb (csv file), but our total executor memory is only 100GB , so how we can write a code in pyspark  to process the in optimized way.',\n",
       "  'do I need to install spark application in windows machine ?? or just python pip install pyspark is enough ?',\n",
       "  'great',\n",
       "  \"I've hadoop installed in VMware workstation. I'm able to ping the ip from windows cmd but it throws error when i'm running the command to get csv file in jupyter notebook. Maybe 8020 port is not open for me. I've tried several other ports but it throws the same error : n error occurred while calling o52.load.\\r : java.net.ConnectException <windows ip> to <VMware ip>\",\n",
       "  'Excellent',\n",
       "  'Can you please share the link to download this VM that you are using this video.?',\n",
       "  'How to install VMWare on Windows 10 machine? Can you please tell steps or give any link?',\n",
       "  \"Please do more videos on pyspark bro , i have understood very well on spark ,what u have taught , please I'm Beginning bro , please do videos on pyspark\",\n",
       "  'Fine... ',\n",
       "  'this is what exactly I am looking for what defaultParallelism is and why reading the file default return 1 partition.',\n",
       "  'Are you providing pyspark online training if yes please provide your contact number or mail id ..I am willing to take pyspark course',\n",
       "  'students are getting really clever doubts .  is it some institute or colleage ?',\n",
       "  'Thanks a lot bro ,by making the videos , please please please continue to make all the videos bro',\n",
       "  'Wonderful session ! Kudos to the amazing work you do!!',\n",
       "  'Thanks it helped me.\\nmine was microsoft sql server though',\n",
       "  'for microsoft sql server:\\r\\n\\r\\n\\r\\nimport findspark\\r\\nfindspark.init()\\r\\nfrom pyspark import *\\r\\nfrom pyspark.sql import *\\r\\nfrom pyspark.sql.functions import *\\r\\nfrom pyspark.sql.types import *\\r\\n\\r\\nspark=SparkSession.builder.master(\"local[*]\").appName(\"Test Spark\")\\\\\\r\\n      .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:9.2.1.jre11\").getOrCreate()\\r\\nsc=spark.sparkContext\\r\\njdbc_url = \"jdbc:sqlserver://LAPTOP-ABCDEFG:1433;databaseName=db124\"\\r\\nusername = \"alex\"\\r\\npassword = \"alex@1234\"\\r\\ndbtable = \"dbo.tbl_emp\"\\r\\n\\r\\nsource_df = spark.read \\\\\\r\\n    .format(\"jdbc\") \\\\\\r\\n    .option(\"url\", jdbc_url) \\\\\\r\\n    .option(\"dbtable\", dbtable) \\\\\\r\\n    .option(\"user\", username) \\\\\\r\\n    .option(\"password\", password) \\\\\\r\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\\\\r\\n    .load()\\r\\nsource_df.show()\\r\\n\\r\\njust replace host name,db name and table name , username and password and it should work.',\n",
       "  'Hello, thanks for the video , where did you place the  jar file? is there any classpath to set or to add to the path ? thanks in advance',\n",
       "  'Does this work on windows?',\n",
       "  'Good Video, Can you please let me know on how to download or install jdbc driver? and where to keep the jar file?',\n",
       "  'Can you tell me how to fix the drive connector and MySQL connector path so I can solve my problem? Thank you in advance.',\n",
       "  'Good informative video.',\n",
       "  'the first variable of the writed file is corrupted... why? This after correcting %s\\\\n',\n",
       "  'It have very good information, could you please upload code in git hub and share',\n",
       "  'Nice But I reached here for finding an answer -> what if job run multiple time a day - Date partition is same but as the job is running at append mode so for each date folder we get multiple files which are mostly duplicates. How to get 1 updated file only inside date folder all the time ( irrespective of this job run multiple time in a day )',\n",
       "  \"This approach assumes that the source is sending incrementals. If it's a full file everyday, how do you identify the delta prior to load into spark warehouse\",\n",
       "  'Nice explanation  ',\n",
       "  'Clearly explained thanks for posting',\n",
       "  'same question asked in my interview . I wish if I would have seen this before ',\n",
       "  'Very useful',\n",
       "  'Hi Sir \\ni have around 7 years of exp into oracle sql/pl sql and trying to make transition into big data field .is there any course are any big data course are you providing in online',\n",
       "  'hi i have install hive along with hadoop and I have stud table there but when I am trying to access using pyspark it is showing table is not present what can be solution',\n",
       "  'Hi, nice explanation but I want to know more about the data flow from the hive tables to becoming a spark dataframe',\n",
       "  'Hi sir nice explaination....\\nCould you please create dataframe from hive using intellij ide',\n",
       "  'Very neat explanation',\n",
       "  '',\n",
       "  'Sir, I have one scenario, on s3 we have data file with size of 500gb (csv file), but our total executor memory is only 100GB , so how we can write a code in pyspark  to process the in optimized way.',\n",
       "  'do I need to install spark application in windows machine ?? or just python pip install pyspark is enough ?',\n",
       "  'great',\n",
       "  \"I've hadoop installed in VMware workstation. I'm able to ping the ip from windows cmd but it throws error when i'm running the command to get csv file in jupyter notebook. Maybe 8020 port is not open for me. I've tried several other ports but it throws the same error : n error occurred while calling o52.load.\\r : java.net.ConnectException <windows ip> to <VMware ip>\",\n",
       "  'Excellent',\n",
       "  'Can you please share the link to download this VM that you are using this video.?',\n",
       "  'How to install VMWare on Windows 10 machine? Can you please tell steps or give any link?',\n",
       "  \"Please do more videos on pyspark bro , i have understood very well on spark ,what u have taught , please I'm Beginning bro , please do videos on pyspark\",\n",
       "  'Fine... ',\n",
       "  'this is what exactly I am looking for what defaultParallelism is and why reading the file default return 1 partition.',\n",
       "  'Are you providing pyspark online training if yes please provide your contact number or mail id ..I am willing to take pyspark course',\n",
       "  'students are getting really clever doubts .  is it some institute or colleage ?',\n",
       "  'Thanks a lot bro ,by making the videos , please please please continue to make all the videos bro',\n",
       "  'Wonderful session ! Kudos to the amazing work you do!!',\n",
       "  'Thanks it helped me.\\nmine was microsoft sql server though',\n",
       "  'for microsoft sql server:\\r\\n\\r\\n\\r\\nimport findspark\\r\\nfindspark.init()\\r\\nfrom pyspark import *\\r\\nfrom pyspark.sql import *\\r\\nfrom pyspark.sql.functions import *\\r\\nfrom pyspark.sql.types import *\\r\\n\\r\\nspark=SparkSession.builder.master(\"local[*]\").appName(\"Test Spark\")\\\\\\r\\n      .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:9.2.1.jre11\").getOrCreate()\\r\\nsc=spark.sparkContext\\r\\njdbc_url = \"jdbc:sqlserver://LAPTOP-ABCDEFG:1433;databaseName=db124\"\\r\\nusername = \"alex\"\\r\\npassword = \"alex@1234\"\\r\\ndbtable = \"dbo.tbl_emp\"\\r\\n\\r\\nsource_df = spark.read \\\\\\r\\n    .format(\"jdbc\") \\\\\\r\\n    .option(\"url\", jdbc_url) \\\\\\r\\n    .option(\"dbtable\", dbtable) \\\\\\r\\n    .option(\"user\", username) \\\\\\r\\n    .option(\"password\", password) \\\\\\r\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\\\\r\\n    .load()\\r\\nsource_df.show()\\r\\n\\r\\njust replace host name,db name and table name , username and password and it should work.',\n",
       "  'Hello, thanks for the video , where did you place the  jar file? is there any classpath to set or to add to the path ? thanks in advance',\n",
       "  'Does this work on windows?',\n",
       "  'Good Video, Can you please let me know on how to download or install jdbc driver? and where to keep the jar file?',\n",
       "  'Can you tell me how to fix the drive connector and MySQL connector path so I can solve my problem? Thank you in advance.',\n",
       "  'Good informative video.',\n",
       "  'the first variable of the writed file is corrupted... why? This after correcting %s\\\\n',\n",
       "  'It have very good information, could you please upload code in git hub and share',\n",
       "  'Sir how to access hive database in pycharm editor',\n",
       "  'Are we using hive just to save data in hdfs? What if we use combination of Sqoop and spark?',\n",
       "  'one of best video for sparksql Vs Hive',\n",
       "  'Extremely helpful video!!\\nBless you bhai!!!',\n",
       "  'So much informative. Thank you.\\nI think we can also do it same this in scala as you did in pyspark. Thanks in advance for your reply.',\n",
       "  'Nice explaination.....could you pls do complete pyspark playlist..... thank you',\n",
       "  'Excellent explanation! thank you so mch',\n",
       "  \"Clear cut explanation ....I didn't see this much clearity on any other channel.\",\n",
       "  'Nice one. But, if you decide to go with Dataframe directly, just try out this two lines of code which results the same. \\ndf = spark.read.csv(\"sample.txt\", sep= \" \")\\ndf.fillna(\"no data\").show()\\nHope this works.',\n",
       "  'Ausome explanation',\n",
       "  'Good explanation',\n",
       "  'Hi Bro i have one doubt could you please help to me understand us.\\nSpark supports only standalone,yarn,mesos and kubernetes clusters right..and what databricks cluster ? Databricks have any other cluster ???',\n",
       "  'I want to learn the Hadoop Administrator can you please tell me how i can',\n",
       "  'When can get the next to continue video? :)',\n",
       "  'Thank you sir',\n",
       "  'Thanks for the very important spark-submit sessio.',\n",
       "  'Nice But I reached here for finding an answer -> what if job run multiple time a day - Date partition is same but as the job is running at append mode so for each date folder we get multiple files which are mostly duplicates. How to get 1 updated file only inside date folder all the time ( irrespective of this job run multiple time in a day )',\n",
       "  \"This approach assumes that the source is sending incrementals. If it's a full file everyday, how do you identify the delta prior to load into spark warehouse\",\n",
       "  'Nice explanation  ',\n",
       "  'Clearly explained thanks for posting',\n",
       "  'same question asked in my interview . I wish if I would have seen this before ',\n",
       "  'Very useful',\n",
       "  'Hi Sir \\ni have around 7 years of exp into oracle sql/pl sql and trying to make transition into big data field .is there any course are any big data course are you providing in online',\n",
       "  'hi i have install hive along with hadoop and I have stud table there but when I am trying to access using pyspark it is showing table is not present what can be solution',\n",
       "  'Hi, nice explanation but I want to know more about the data flow from the hive tables to becoming a spark dataframe',\n",
       "  'Hi sir nice explaination....\\nCould you please create dataframe from hive using intellij ide',\n",
       "  'Very neat explanation',\n",
       "  '',\n",
       "  'Sir, I have one scenario, on s3 we have data file with size of 500gb (csv file), but our total executor memory is only 100GB , so how we can write a code in pyspark  to process the in optimized way.',\n",
       "  'do I need to install spark application in windows machine ?? or just python pip install pyspark is enough ?',\n",
       "  'great',\n",
       "  \"I've hadoop installed in VMware workstation. I'm able to ping the ip from windows cmd but it throws error when i'm running the command to get csv file in jupyter notebook. Maybe 8020 port is not open for me. I've tried several other ports but it throws the same error : n error occurred while calling o52.load.\\r : java.net.ConnectException <windows ip> to <VMware ip>\",\n",
       "  'Excellent',\n",
       "  'Can you please share the link to download this VM that you are using this video.?',\n",
       "  'How to install VMWare on Windows 10 machine? Can you please tell steps or give any link?',\n",
       "  \"Please do more videos on pyspark bro , i have understood very well on spark ,what u have taught , please I'm Beginning bro , please do videos on pyspark\",\n",
       "  'Fine... ',\n",
       "  'this is what exactly I am looking for what defaultParallelism is and why reading the file default return 1 partition.',\n",
       "  'Are you providing pyspark online training if yes please provide your contact number or mail id ..I am willing to take pyspark course',\n",
       "  'students are getting really clever doubts .  is it some institute or colleage ?',\n",
       "  'Thanks a lot bro ,by making the videos , please please please continue to make all the videos bro',\n",
       "  'Wonderful session ! Kudos to the amazing work you do!!',\n",
       "  'Thanks it helped me.\\nmine was microsoft sql server though',\n",
       "  'for microsoft sql server:\\r\\n\\r\\n\\r\\nimport findspark\\r\\nfindspark.init()\\r\\nfrom pyspark import *\\r\\nfrom pyspark.sql import *\\r\\nfrom pyspark.sql.functions import *\\r\\nfrom pyspark.sql.types import *\\r\\n\\r\\nspark=SparkSession.builder.master(\"local[*]\").appName(\"Test Spark\")\\\\\\r\\n      .config(\"spark.jars.packages\", \"com.microsoft.sqlserver:mssql-jdbc:9.2.1.jre11\").getOrCreate()\\r\\nsc=spark.sparkContext\\r\\njdbc_url = \"jdbc:sqlserver://LAPTOP-ABCDEFG:1433;databaseName=db124\"\\r\\nusername = \"alex\"\\r\\npassword = \"alex@1234\"\\r\\ndbtable = \"dbo.tbl_emp\"\\r\\n\\r\\nsource_df = spark.read \\\\\\r\\n    .format(\"jdbc\") \\\\\\r\\n    .option(\"url\", jdbc_url) \\\\\\r\\n    .option(\"dbtable\", dbtable) \\\\\\r\\n    .option(\"user\", username) \\\\\\r\\n    .option(\"password\", password) \\\\\\r\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\\\\r\\n    .load()\\r\\nsource_df.show()\\r\\n\\r\\njust replace host name,db name and table name , username and password and it should work.',\n",
       "  'Hello, thanks for the video , where did you place the  jar file? is there any classpath to set or to add to the path ? thanks in advance',\n",
       "  'Does this work on windows?',\n",
       "  'Good Video, Can you please let me know on how to download or install jdbc driver? and where to keep the jar file?',\n",
       "  'Can you tell me how to fix the drive connector and MySQL connector path so I can solve my problem? Thank you in advance.',\n",
       "  'Good informative video.',\n",
       "  'the first variable of the writed file is corrupted... why? This after correcting %s\\\\n',\n",
       "  'It have very good information, could you please upload code in git hub and share',\n",
       "  'Sir how to access hive database in pycharm editor',\n",
       "  'Are we using hive just to save data in hdfs? What if we use combination of Sqoop and spark?',\n",
       "  'one of best video for sparksql Vs Hive',\n",
       "  'Extremely helpful video!!\\nBless you bhai!!!',\n",
       "  'So much informative. Thank you.\\nI think we can also do it same this in scala as you did in pyspark. Thanks in advance for your reply.',\n",
       "  'Nice explaination.....could you pls do complete pyspark playlist..... thank you',\n",
       "  'Excellent explanation! thank you so mch',\n",
       "  \"Clear cut explanation ....I didn't see this much clearity on any other channel.\",\n",
       "  'Nice one. But, if you decide to go with Dataframe directly, just try out this two lines of code which results the same. \\ndf = spark.read.csv(\"sample.txt\", sep= \" \")\\ndf.fillna(\"no data\").show()\\nHope this works.',\n",
       "  'Ausome explanation',\n",
       "  'Good explanation',\n",
       "  'Hi Bro i have one doubt could you please help to me understand us.\\nSpark supports only standalone,yarn,mesos and kubernetes clusters right..and what databricks cluster ? Databricks have any other cluster ???',\n",
       "  'I want to learn the Hadoop Administrator can you please tell me how i can',\n",
       "  'When can get the next to continue video? :)',\n",
       "  'Thank you sir',\n",
       "  'Thanks for the very important spark-submit sessio.',\n",
       "  'Hi do you provide any full pyspark course.. I would like to join',\n",
       "  'can you please show SDK way of doing  it witha small pipeline',\n",
       "  'read data from s3 using pyspark with json can u please share any doc',\n",
       "  'Thanks for sharing',\n",
       "  'Hi, \\nQue -  I could not see you when Started  EMR Cluster ?  Or it was alreay upand running ?',\n",
       "  'Hey thanks really helpful, but isnt there a safer way to go pass aws keys?',\n",
       "  'I\\'m using jenkins pipeline to execute pyspark code in ec2 instance. Dont we need this config in that  code \\nos.environ[\\'PYSPARK_SUBMIT_ARGS\\'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"',\n",
       "  'Can u share in a programmatic way',\n",
       "  'nice explanation. Can you plz make a video on EMR.',\n",
       "  \"How to write code with IAM role, if we don't have access key and secrets key\",\n",
       "  \"If I am not mistaken I think you needed to call 'sc' instead of 'spark' for reading the file right?\",\n",
       "  \"from the previous session, you were to show how to write and read s3 data using pyspark in this session, I've followed the rest of the videos, and there is none covering the topic\",\n",
       "  'Is it possible to read the S3 objects using boto3 without using credentials. (If the source is an open connection)',\n",
       "  'Boto can interact will all services',\n",
       "  'Thanks for your all useful videos.',\n",
       "  'Is it possible to create a table on top of a file present in s3? Kindly let me know.',\n",
       "  'very well explained... was looking for such a video.... thnak you',\n",
       "  'hi please add kafka video with habase integration ...with spark streamming',\n",
       "  'Nice\\nExplanation',\n",
       "  'Please make videos on Hadoop admin mock interview...',\n",
       "  'Thank you so much cleared doubt',\n",
       "  'Can anyone say how to retrieve data if driver fails',\n",
       "  'Perfect  sir',\n",
       "  'My question is suppose if i pass data of 1gb on my local machine which is having 2 cores so is 2 partitions will be created? And if i apply transformation of group by based on condition and call .show() action this logic is applied on these 2 partitions and give results?? Or can i create how much ever partitions of my wish?? Pls let me know',\n",
       "  'Clear and crisp sir Really well explained with internals Thanks Please uload more tuts on interview Questions',\n",
       "  'Can one executor with 2 cores can process 2 different partitions respectively ?',\n",
       "  'Great and Simple to understand Explanation. Thank you.',\n",
       "  \"I don't find it correct and relevant.\",\n",
       "  'if I have only one server , which deployment mode is suitable for my spark application ..\\ncan I use YARN with single machine?',\n",
       "  'still confusing',\n",
       "  'Awesome channel for Big dataTq so Much sir...grateful to you.',\n",
       "  'Simply Superb. Loved it. Thanks a bunch',\n",
       "  'So do you mean driver program is generated in Resource manager (RM)',\n",
       "  'Very good efforts Sir, keep it up..rather than cramming  for interview questions...Watching and understanding your video will give much better and sound preparation...Keep it up. I am surprized...people did not notice your video yet...',\n",
       "  'Thanks u for the video.\\nEven experienced doesnt know this difference as said in the video.\\nvery informative.',\n",
       "  'Nice explanation.',\n",
       "  'Sir, \\nI have watched many videos related to this topic, but very few guys were able to explain these concepts the way you did. and this video tempted me to watch full playlist, and I definitely will.\\nthanks for sharing your knowledge and understandings with us.\\n',\n",
       "  'nice catch points explained',\n",
       "  \"Per my understanding driver sends the logic or program to executor to read only given partition of data. My doubt is how driver node creates those instruction as it does not know exactly what data is present in file specifically if it's big text file, there are no columns or keys or indexes. How it make sure that all data is read by different executorand there are no overlaps.\",\n",
       "  'All your Vedios on Spark are good..Can you assign the numbers in the order to watch from first to last?',\n",
       "  'Informative and well explained. Keep posting ',\n",
       "  'Can you please provide the download link for the CDH you are using.???',\n",
       "  'Could you please explain am I getting it right. As I understand partition is a logical division of data in chunks of data (unit of operation that Spark applies).\\r\\nSo basically when for example we create RDD with 4 partitions it means that Driver Node will read data, create partitions, and serialize it, ship those partitions to Worker Nodes (deserialize here)   so that it may make compuations parallelly?',\n",
       "  'Thank you',\n",
       "  'another good efforts for the aspirent of Data engineering job candidates. sound ground for preparing for interview...',\n",
       "  'Really informative.... neat explanation. Thank u',\n",
       "  'Can you explain this question: how to move all partitions in a single node?',\n",
       "  'well explained',\n",
       "  'first time i have come across a valued explanation. happy to see your videos',\n",
       "  'Gret job..learning in bites..keep it up .',\n",
       "  'Nice content. Short and simple',\n",
       "  'Very nice explanation, Please do more videos on the interview questions... Nice content',\n",
       "  'How can i connect with you....any linkedin link can i get please?',\n",
       "  'Very well defined the technality o Df and Rdd..great job',\n",
       "  'how to parse fixed width text file into columns of fixed length',\n",
       "  'Great Explanation. But I have a question \"What is the role of RDD lineage graph? Spark stores its transformations graph in RDD lineage from parent to child. If we have a damaged/lost partitions/RDD, spark can get from RDD lineage graph. Why persistence is required in the case of lost RDD/Partitions?\"',\n",
       "  'can you please use the latest stable version of spark for next videos. very clear and awesome explanation with in minimal time,',\n",
       "  'well explained',\n",
       "  'seriously one the best explanation i have seen...deserves more views',\n",
       "  'Why we need cache() when there is a persist()?',\n",
       "  'Fantastic explaination...',\n",
       "  'Why we say hive and spark run on top of hadoop .....while sqoop run in hadoop',\n",
       "  'Super',\n",
       "  'Simply superb',\n",
       "  'sir hearing about YARN for more than 1 year, but never understood...until now...super explanation.Thank you',\n",
       "  'Sir you are doing a great  service with your marvelous teaching ...hats off to you sir ',\n",
       "  'Thank you for explaining concepts in simpler terms. We could understand the concepts in a different perspective rather than going through the theory.',\n",
       "  'Excellent Information, very helpful for me, Thank you.',\n",
       "  'Nice video, Can i get your contact details',\n",
       "  'Nice explanation.',\n",
       "  'Thank you so much cleared doubt',\n",
       "  'Can anyone say how to retrieve data if driver fails',\n",
       "  'Perfect  sir',\n",
       "  'My question is suppose if i pass data of 1gb on my local machine which is having 2 cores so is 2 partitions will be created? And if i apply transformation of group by based on condition and call .show() action this logic is applied on these 2 partitions and give results?? Or can i create how much ever partitions of my wish?? Pls let me know',\n",
       "  'Clear and crisp sir Really well explained with internals Thanks Please uload more tuts on interview Questions',\n",
       "  'Can one executor with 2 cores can process 2 different partitions respectively ?',\n",
       "  'Great and Simple to understand Explanation. Thank you.',\n",
       "  \"I don't find it correct and relevant.\",\n",
       "  'if I have only one server , which deployment mode is suitable for my spark application ..\\ncan I use YARN with single machine?',\n",
       "  'still confusing',\n",
       "  'Awesome channel for Big dataTq so Much sir...grateful to you.',\n",
       "  'Simply Superb. Loved it. Thanks a bunch',\n",
       "  'So do you mean driver program is generated in Resource manager (RM)',\n",
       "  'Very good efforts Sir, keep it up..rather than cramming  for interview questions...Watching and understanding your video will give much better and sound preparation...Keep it up. I am surprized...people did not notice your video yet...',\n",
       "  'Thanks u for the video.\\nEven experienced doesnt know this difference as said in the video.\\nvery informative.',\n",
       "  'Nice explanation.',\n",
       "  'Sir, \\nI have watched many videos related to this topic, but very few guys were able to explain these concepts the way you did. and this video tempted me to watch full playlist, and I definitely will.\\nthanks for sharing your knowledge and understandings with us.\\n',\n",
       "  'nice catch points explained',\n",
       "  \"Per my understanding driver sends the logic or program to executor to read only given partition of data. My doubt is how driver node creates those instruction as it does not know exactly what data is present in file specifically if it's big text file, there are no columns or keys or indexes. How it make sure that all data is read by different executorand there are no overlaps.\",\n",
       "  'All your Vedios on Spark are good..Can you assign the numbers in the order to watch from first to last?',\n",
       "  'Informative and well explained. Keep posting ',\n",
       "  'Can you please provide the download link for the CDH you are using.???',\n",
       "  'Could you please explain am I getting it right. As I understand partition is a logical division of data in chunks of data (unit of operation that Spark applies).\\r\\nSo basically when for example we create RDD with 4 partitions it means that Driver Node will read data, create partitions, and serialize it, ship those partitions to Worker Nodes (deserialize here)   so that it may make compuations parallelly?',\n",
       "  'Thank you',\n",
       "  'another good efforts for the aspirent of Data engineering job candidates. sound ground for preparing for interview...',\n",
       "  'Really informative.... neat explanation. Thank u',\n",
       "  'Can you explain this question: how to move all partitions in a single node?',\n",
       "  'well explained',\n",
       "  'first time i have come across a valued explanation. happy to see your videos',\n",
       "  'Gret job..learning in bites..keep it up .',\n",
       "  'Nice content. Short and simple',\n",
       "  'Very nice explanation, Please do more videos on the interview questions... Nice content',\n",
       "  'How can i connect with you....any linkedin link can i get please?',\n",
       "  'Very well defined the technality o Df and Rdd..great job',\n",
       "  'how to parse fixed width text file into columns of fixed length',\n",
       "  'Great Explanation. But I have a question \"What is the role of RDD lineage graph? Spark stores its transformations graph in RDD lineage from parent to child. If we have a damaged/lost partitions/RDD, spark can get from RDD lineage graph. Why persistence is required in the case of lost RDD/Partitions?\"',\n",
       "  'can you please use the latest stable version of spark for next videos. very clear and awesome explanation with in minimal time,',\n",
       "  'well explained',\n",
       "  'seriously one the best explanation i have seen...deserves more views',\n",
       "  'Why we need cache() when there is a persist()?',\n",
       "  'Fantastic explaination...',\n",
       "  'Why we say hive and spark run on top of hadoop .....while sqoop run in hadoop',\n",
       "  'Super',\n",
       "  'Simply superb',\n",
       "  'sir hearing about YARN for more than 1 year, but never understood...until now...super explanation.Thank you',\n",
       "  'Sir you are doing a great  service with your marvelous teaching ...hats off to you sir ',\n",
       "  'Thank you for explaining concepts in simpler terms. We could understand the concepts in a different perspective rather than going through the theory.',\n",
       "  'Excellent Information, very helpful for me, Thank you.',\n",
       "  'Nice video, Can i get your contact details',\n",
       "  'Nice explanation.',\n",
       "  'Very well explained. So any clarity between how data is stored among partitions, clusters and different machines/nodes?',\n",
       "  'Cristal cleared',\n",
       "  'what is the scope of the executor process? will it die after completing the given task? or will wait for another task with narrow transformations ?',\n",
       "  'God level explanations !! It all makes sense now to me. Thank you so much for these in-depth concept explanation videos.',\n",
       "  'Thanks its so clear!!!!!',\n",
       "  'Executor can have many tasks. Depends on cores. #cores=#tasks.',\n",
       "  'Small correction...number of tasks need to be equal to number of executors.. if one executors has 5 cores.. number of tasks running on one executor will be 5.',\n",
       "  'What is the use of jar file?',\n",
       "  'Can you please tell what are the versions used in cloudera distribution in real time',\n",
       "  'Could you please explain realtime scanerio of spark performance tuning (its like a optimzation)',\n",
       "  'Very useful',\n",
       "  'Why we are using maven and jar file?',\n",
       "  'I tried the above steps in my windows system with eclipse IDE but while running scala program it showing below error:\\n\\nError: Main method not found in class org.spark.learning.Demo, please define the main method as: public static void main(String[] args) or a JavaFX application class must extend javafx.application.Application\\n\\nLogic:\\ndef main(args:Array[String]) {\\r\\n    val conf = new SparkConf()\\r\\n  conf.set(\"spark-master\",\"local\")\\r\\n  conf.set(\"spark.app.name\",\"SampleApplication\")\\r\\n    \\r\\n    val sc= new SparkContext(conf)\\r\\n    val ran = sc.range(1,100)\\r\\n    ran.collect.foreach(println)\\r\\n    sc.stop()\\r\\n  }',\n",
       "  'What is the same process for pyspark?',\n",
       "  'Where do you provide S3 http endpoint? Usually you need to provide 3 things: access key, secret key and endpoint? How do we point to s3 different than AWS?',\n",
       "  'How does Spark load/treat the S3 data into worker node memory in the context of task/partition?',\n",
       "  'how to send data from emr using pyspark to s3',\n",
       "  'Good explanation',\n",
       "  \"Hi,\\n\\nThanks for the elaborative video.\\nI was trying to achieve the same via PySpark but it's failing due to class not found exceptions.\\nCan you suggest what all jars files we need to include ?\\n\\nAs we are using S3A, we might need to include the jars files for AWS sdk ?\",\n",
       "  'Hi do you offer any courses? I want to use pyspark to get data from AWS S3 and load it to redshift. Plz help me what resources I can use.',\n",
       "  'Again short and sweet .small concept but impotant.Thank you.',\n",
       "  'Thanks for the video.How to write spark-submit if my application jar also in s3.',\n",
       "  \"Is it same with spark latest version? Why are you using 1.6 it's very old\",\n",
       "  'How to upload a file in S3 if it is more than 5 GB?',\n",
       "  'What about Pvm objects, they dont undergo serialization and deserialization',\n",
       "  'very easy to understand ',\n",
       "  'you repeat so much man.... You cold have finished this in like 3 mins.... :(',\n",
       "  'Good Explanation',\n",
       "  'Well explained Please make more tuts on Spark Theory',\n",
       "  'Gadkari',\n",
       "  'Now concept understand...Thank you',\n",
       "  'Nice',\n",
       "  'Detailed information provided. Thank you so much.',\n",
       "  'best explanation!',\n",
       "  'what is JVM?',\n",
       "  'Bhai handwriting bhut gandi hai',\n",
       "  'Very good explanation in very simple words. Easily Understandable',\n",
       "  'Very good explanation.',\n",
       "  'Very good explanation',\n",
       "  'Good explanation. Thanks U',\n",
       "  'Is pyspark support dataset?',\n",
       "  'Very clear explanation.. thank you',\n",
       "  'Thanks a lot for the video..',\n",
       "  'Thank you for this video',\n",
       "  'Thanks for the video... If we use Pyspark, what Serializations we have to use??',\n",
       "  'Great  explanation',\n",
       "  'i forgot what i learned delete this video',\n",
       "  'Hi, can you please share your email id?',\n",
       "  'Can you do video on client mode vs cluster mode?',\n",
       "  'Thank you so much cleared doubt',\n",
       "  'Can anyone say how to retrieve data if driver fails',\n",
       "  'Perfect  sir',\n",
       "  'My question is suppose if i pass data of 1gb on my local machine which is having 2 cores so is 2 partitions will be created? And if i apply transformation of group by based on condition and call .show() action this logic is applied on these 2 partitions and give results?? Or can i create how much ever partitions of my wish?? Pls let me know',\n",
       "  'Clear and crisp sir Really well explained with internals Thanks Please uload more tuts on interview Questions',\n",
       "  'Can one executor with 2 cores can process 2 different partitions respectively ?',\n",
       "  'Great and Simple to understand Explanation. Thank you.',\n",
       "  \"I don't find it correct and relevant.\",\n",
       "  'if I have only one server , which deployment mode is suitable for my spark application ..\\ncan I use YARN with single machine?',\n",
       "  'still confusing',\n",
       "  'Awesome channel for Big dataTq so Much sir...grateful to you.',\n",
       "  'Simply Superb. Loved it. Thanks a bunch',\n",
       "  'So do you mean driver program is generated in Resource manager (RM)',\n",
       "  'Very good efforts Sir, keep it up..rather than cramming  for interview questions...Watching and understanding your video will give much better and sound preparation...Keep it up. I am surprized...people did not notice your video yet...',\n",
       "  'Thanks u for the video.\\nEven experienced doesnt know this difference as said in the video.\\nvery informative.',\n",
       "  'Nice explanation.',\n",
       "  'Sir, \\nI have watched many videos related to this topic, but very few guys were able to explain these concepts the way you did. and this video tempted me to watch full playlist, and I definitely will.\\nthanks for sharing your knowledge and understandings with us.\\n',\n",
       "  'nice catch points explained',\n",
       "  \"Per my understanding driver sends the logic or program to executor to read only given partition of data. My doubt is how driver node creates those instruction as it does not know exactly what data is present in file specifically if it's big text file, there are no columns or keys or indexes. How it make sure that all data is read by different executorand there are no overlaps.\",\n",
       "  'All your Vedios on Spark are good..Can you assign the numbers in the order to watch from first to last?',\n",
       "  'Informative and well explained. Keep posting ',\n",
       "  'Can you please provide the download link for the CDH you are using.???',\n",
       "  'Could you please explain am I getting it right. As I understand partition is a logical division of data in chunks of data (unit of operation that Spark applies).\\r\\nSo basically when for example we create RDD with 4 partitions it means that Driver Node will read data, create partitions, and serialize it, ship those partitions to Worker Nodes (deserialize here)   so that it may make compuations parallelly?',\n",
       "  'Thank you',\n",
       "  'another good efforts for the aspirent of Data engineering job candidates. sound ground for preparing for interview...',\n",
       "  'Really informative.... neat explanation. Thank u',\n",
       "  'Can you explain this question: how to move all partitions in a single node?',\n",
       "  'well explained',\n",
       "  'first time i have come across a valued explanation. happy to see your videos',\n",
       "  'Gret job..learning in bites..keep it up .',\n",
       "  'Nice content. Short and simple',\n",
       "  'Very nice explanation, Please do more videos on the interview questions... Nice content',\n",
       "  'How can i connect with you....any linkedin link can i get please?',\n",
       "  'Very well defined the technality o Df and Rdd..great job',\n",
       "  'how to parse fixed width text file into columns of fixed length',\n",
       "  'Great Explanation. But I have a question \"What is the role of RDD lineage graph? Spark stores its transformations graph in RDD lineage from parent to child. If we have a damaged/lost partitions/RDD, spark can get from RDD lineage graph. Why persistence is required in the case of lost RDD/Partitions?\"',\n",
       "  'can you please use the latest stable version of spark for next videos. very clear and awesome explanation with in minimal time,',\n",
       "  'well explained',\n",
       "  'seriously one the best explanation i have seen...deserves more views',\n",
       "  'Why we need cache() when there is a persist()?',\n",
       "  'Fantastic explaination...',\n",
       "  'Why we say hive and spark run on top of hadoop .....while sqoop run in hadoop',\n",
       "  'Super',\n",
       "  'Simply superb',\n",
       "  'sir hearing about YARN for more than 1 year, but never understood...until now...super explanation.Thank you',\n",
       "  'Sir you are doing a great  service with your marvelous teaching ...hats off to you sir ',\n",
       "  'Thank you for explaining concepts in simpler terms. We could understand the concepts in a different perspective rather than going through the theory.',\n",
       "  'Excellent Information, very helpful for me, Thank you.',\n",
       "  'Nice video, Can i get your contact details',\n",
       "  'Nice explanation.',\n",
       "  'Very well explained. So any clarity between how data is stored among partitions, clusters and different machines/nodes?',\n",
       "  'Cristal cleared',\n",
       "  'what is the scope of the executor process? will it die after completing the given task? or will wait for another task with narrow transformations ?',\n",
       "  'God level explanations !! It all makes sense now to me. Thank you so much for these in-depth concept explanation videos.',\n",
       "  'Thanks its so clear!!!!!',\n",
       "  'Executor can have many tasks. Depends on cores. #cores=#tasks.',\n",
       "  'Small correction...number of tasks need to be equal to number of executors.. if one executors has 5 cores.. number of tasks running on one executor will be 5.',\n",
       "  'What is the use of jar file?',\n",
       "  'Can you please tell what are the versions used in cloudera distribution in real time',\n",
       "  'Could you please explain realtime scanerio of spark performance tuning (its like a optimzation)',\n",
       "  'Very useful',\n",
       "  'Why we are using maven and jar file?',\n",
       "  'I tried the above steps in my windows system with eclipse IDE but while running scala program it showing below error:\\n\\nError: Main method not found in class org.spark.learning.Demo, please define the main method as: public static void main(String[] args) or a JavaFX application class must extend javafx.application.Application\\n\\nLogic:\\ndef main(args:Array[String]) {\\r\\n    val conf = new SparkConf()\\r\\n  conf.set(\"spark-master\",\"local\")\\r\\n  conf.set(\"spark.app.name\",\"SampleApplication\")\\r\\n    \\r\\n    val sc= new SparkContext(conf)\\r\\n    val ran = sc.range(1,100)\\r\\n    ran.collect.foreach(println)\\r\\n    sc.stop()\\r\\n  }',\n",
       "  'What is the same process for pyspark?',\n",
       "  'Where do you provide S3 http endpoint? Usually you need to provide 3 things: access key, secret key and endpoint? How do we point to s3 different than AWS?',\n",
       "  'How does Spark load/treat the S3 data into worker node memory in the context of task/partition?',\n",
       "  'how to send data from emr using pyspark to s3',\n",
       "  'Good explanation',\n",
       "  \"Hi,\\n\\nThanks for the elaborative video.\\nI was trying to achieve the same via PySpark but it's failing due to class not found exceptions.\\nCan you suggest what all jars files we need to include ?\\n\\nAs we are using S3A, we might need to include the jars files for AWS sdk ?\",\n",
       "  'Hi do you offer any courses? I want to use pyspark to get data from AWS S3 and load it to redshift. Plz help me what resources I can use.',\n",
       "  'Again short and sweet .small concept but impotant.Thank you.',\n",
       "  'Thanks for the video.How to write spark-submit if my application jar also in s3.',\n",
       "  \"Is it same with spark latest version? Why are you using 1.6 it's very old\",\n",
       "  'How to upload a file in S3 if it is more than 5 GB?',\n",
       "  'What about Pvm objects, they dont undergo serialization and deserialization',\n",
       "  'very easy to understand ',\n",
       "  'you repeat so much man.... You cold have finished this in like 3 mins.... :(',\n",
       "  'Good Explanation',\n",
       "  'Well explained Please make more tuts on Spark Theory',\n",
       "  'Gadkari',\n",
       "  'Now concept understand...Thank you',\n",
       "  'Nice',\n",
       "  'Detailed information provided. Thank you so much.',\n",
       "  'best explanation!',\n",
       "  'what is JVM?',\n",
       "  'Bhai handwriting bhut gandi hai',\n",
       "  'Very good explanation in very simple words. Easily Understandable',\n",
       "  'Very good explanation.',\n",
       "  'Very good explanation',\n",
       "  'Good explanation. Thanks U',\n",
       "  'Is pyspark support dataset?',\n",
       "  'Very clear explanation.. thank you',\n",
       "  'Thanks a lot for the video..',\n",
       "  'Thank you for this video',\n",
       "  'Thanks for the video... If we use Pyspark, what Serializations we have to use??',\n",
       "  'Great  explanation',\n",
       "  'i forgot what i learned delete this video',\n",
       "  'Hi, can you please share your email id?',\n",
       "  'Can you do video on client mode vs cluster mode?',\n",
       "  'Teach us  a complex data pipeline with Apache Spark..and disparate sources..',\n",
       "  'This is very good. Please upload some intermediate /advanced level use cases..',\n",
       "  'Such a valuable use cases for new aspirants   Thank you so much',\n",
       "  '@Clever Studies Can I directly learn Scala with out going much into learning Java?',\n",
       "  'thank you sir',\n",
       "  'Thanks bro',\n",
       "  'Tq',\n",
       "  'Thx random Internet guy',\n",
       "  'Provide me trainer number',\n",
       "  'Is this a real interview recording ?or just a mock ?',\n",
       "  'Clever Studies, could anyone list down all the questions asked in the interview?',\n",
       "  'Hive pretend the normal dataset as a relational database but it is de-normalized deepdown.So, the queries are not fast and efficient.',\n",
       "  'can we keep it more difficult? as subject interview is for teach lead but the questions is more like for  2-3 year experience guy. Can you please ask more deeper and coding related questions as well....',\n",
       "  'Once again very helpful stuff.. \\nthanks a lot..',\n",
       "  'Did the candidate cleared the interview successfully?',\n",
       "  'We need more like this. This was a great conversation. Thank you!',\n",
       "  'nice sir please do have more real time questions from daily activities what the candidate does',\n",
       "  'Hello sir can you please take my mock interview ...',\n",
       "  'Great interview',\n",
       "  'better for interview preparation.',\n",
       "  'It would be great if u could post the answer of the question.',\n",
       "  'How many years of experience the student has??',\n",
       "  'Hello Sir Can you please take my mock interview on the same ,if yes so please share your mail id where I can share my latest CV. Having total 11 years of IT experience & out of 11 having 4 years of big data & 7 years of Oracle pl SQL experience.',\n",
       "  'Is the preparation good enough for a 5 yr experienced guy?',\n",
       "  'Very nice.. good and fast paced',\n",
       "  'How to build jar files in spark and why?',\n",
       "  \"If we try to increase partitions using Coalesce it will only take the default value which is 200.\\n\\nMost people feel shuffle is bad and we should be using Coalesce more often than repartition. No! \\nSpark works fine when the partitions sizes are even, Coalesce might give an uneven sized partitions and that would impact the performance of the job(not always!). \\n\\nIt's a trade-off and we have chose depend on the need.\",\n",
       "  'He was telling about spark actual memory usage toll in 9th min to 11min). Can someone tell toll the name.',\n",
       "  \"1. exclude tables,  --exclude-tables flag\\n2. splittable colums to split the partitons on intbased columns\\n3. repartion can increase and decrease the columns and does full shuffle. Coalesce avoids full shuffle and better for reduced number of partitions\\n4. If spark.default.parallelism is set, we'll use the value of SparkContext defaultParallelism as the default partitions number, otherwise we'll use the max number of upstream partitions.\\n5.  I guess they are talking about DERBY server.\",\n",
       "  'Excellent.',\n",
       "  'Very Helpful , Excellent',\n",
       "  'Very helpful',\n",
       "  'Good knowledge for 1 year exp',\n",
       "  '@Clever Studies : why is hdfs blocksize is set to 128mb ?',\n",
       "  'Q',\n",
       "  'I just completed my course but I could answer these questions...',\n",
       "  'Awesome questions. By the way I knew all the answers, if these were the questions for my interview, I would have rocked it ',\n",
       "  'Can you please answers also in last ....and what mistake we does in interview will be more help ful  thanks awesome question  love from rajsthan ',\n",
       "  'What are questions for seniors persons ?',\n",
       "  'Even though I have less than 1.5 years of real job experience on HADOOP and SPark, I know each and every question answers in detail that you asked. Oh god, I am heavily underpaid.  .: p. I need a raise lol',\n",
       "  'Could you please schedule mock interv with me?\\nI am better than her!',\n",
       "  'awesome! interview question can you please provide me more real-time questions for the Hadoop admin',\n",
       "  'Awesome',\n",
       "  '\"it will AAstoreeeee\" is the best ',\n",
       "  'Hive does not support multiline comment',\n",
       "  'Kudos brother, this helped me with my interview ',\n",
       "  \"It's wonderful for people who r going to attend interview in spark realy helpful...thanks alot dear\",\n",
       "  'Can you please answer? Why cache when there is persist?',\n",
       "  'Count of like in video ?',\n",
       "  'Can you also please tell the answers ex : in case of Hadoop block size is 128 MB , if Hdfs replaced by s3 as a storage then what will be the block size in s3?',\n",
       "  'Please hide the video as posture is inappropriate. you can cast audio',\n",
       "  'thankyou for posting this video',\n",
       "  'Vectorization explained wrong',\n",
       "  'What the exact ans to the Ques:\\nWhen we do a coalesce to 1 and the data is greater than the block size , then how many files will be written?',\n",
       "  'Can u plz tell me How many years of Experiance does he has?',\n",
       "  'this kind of questions can we expect for 2-3 years of experienced people  or any higher level?',\n",
       "  'Nice one',\n",
       "  'He is blabbering..',\n",
       "  'I would like to attend ur interview I have emailed to you but there is no response. Could you please respond.',\n",
       "  'Awesome interview.',\n",
       "  'this guy is struggling a lot',\n",
       "  'How many years of experience candidate had?',\n",
       "  'Mostly your uploading hadoop development,  can you upload for admin part also , if already uploaded please share link',\n",
       "  'Very nice interview.. cna you schedule mock interview for me as well if possible..?',\n",
       "  'really talented candidate',\n",
       "  'Default blocksize in lfs is 4 kb',\n",
       "  'Wo banda hushar tha.',\n",
       "  'Sir\\n.I have an experience of 2.5 years as a Manual test engineer but rather than automation I am thinking to switching to big data testing therefore I end up doing a certification course as well.How should i apply for interviews.',\n",
       "  'How to apply for mock interview',\n",
       "  'Very nice content',\n",
       "  'Hdfs for 128mb and local for 32mb',\n",
       "  'Good one',\n",
       "  'Use rank functions to delete duplicate',\n",
       "  'could you please provide feedback on this interview?',\n",
       "  'Plz give me answer of  \"100 mb file and read the contents of this file and write it into another file 5 times\" \\nHow we can write 5 times...?',\n",
       "  \"Guys, pls consider only questions but dont trust the answers, 95% answers are worng, It's not criticising anyone just warning the people who are preparing for interviews.\",\n",
       "  'Please post interviews on data integration & Warehousing. I wanted to give demo interview is there any possibility to do that.',\n",
       "  'Is this telephonic interview?',\n",
       "  'very very easy interview till now i hv faced',\n",
       "  'Superb',\n",
       "  'Please send your resume to shareit2904@gmail.com',\n",
       "  'I want to give demo interview for hadoop/spark developer',\n",
       "  'Hlo sir\\nPreparing for interview with 2 years of experience\\nCan i have recording videos for 2 years',\n",
       "  'Goood stuff...may i know the verdict of this interview',\n",
       "  'Is this a real interview recording ?or just a mock ?',\n",
       "  'Clever Studies, could anyone list down all the questions asked in the interview?',\n",
       "  'Hive pretend the normal dataset as a relational database but it is de-normalized deepdown.So, the queries are not fast and efficient.',\n",
       "  'can we keep it more difficult? as subject interview is for teach lead but the questions is more like for  2-3 year experience guy. Can you please ask more deeper and coding related questions as well....',\n",
       "  'Once again very helpful stuff.. \\nthanks a lot..',\n",
       "  'Did the candidate cleared the interview successfully?',\n",
       "  'We need more like this. This was a great conversation. Thank you!',\n",
       "  'nice sir please do have more real time questions from daily activities what the candidate does',\n",
       "  'Hello sir can you please take my mock interview ...',\n",
       "  'Great interview',\n",
       "  'better for interview preparation.',\n",
       "  'It would be great if u could post the answer of the question.',\n",
       "  'How many years of experience the student has??',\n",
       "  'Hello Sir Can you please take my mock interview on the same ,if yes so please share your mail id where I can share my latest CV. Having total 11 years of IT experience & out of 11 having 4 years of big data & 7 years of Oracle pl SQL experience.',\n",
       "  'Is the preparation good enough for a 5 yr experienced guy?',\n",
       "  'Very nice.. good and fast paced',\n",
       "  'How to build jar files in spark and why?',\n",
       "  \"If we try to increase partitions using Coalesce it will only take the default value which is 200.\\n\\nMost people feel shuffle is bad and we should be using Coalesce more often than repartition. No! \\nSpark works fine when the partitions sizes are even, Coalesce might give an uneven sized partitions and that would impact the performance of the job(not always!). \\n\\nIt's a trade-off and we have chose depend on the need.\",\n",
       "  'He was telling about spark actual memory usage toll in 9th min to 11min). Can someone tell toll the name.',\n",
       "  \"1. exclude tables,  --exclude-tables flag\\n2. splittable colums to split the partitons on intbased columns\\n3. repartion can increase and decrease the columns and does full shuffle. Coalesce avoids full shuffle and better for reduced number of partitions\\n4. If spark.default.parallelism is set, we'll use the value of SparkContext defaultParallelism as the default partitions number, otherwise we'll use the max number of upstream partitions.\\n5.  I guess they are talking about DERBY server.\",\n",
       "  'Excellent.',\n",
       "  'Very Helpful , Excellent',\n",
       "  'Very helpful',\n",
       "  'Good knowledge for 1 year exp',\n",
       "  '@Clever Studies : why is hdfs blocksize is set to 128mb ?',\n",
       "  'Q',\n",
       "  'I just completed my course but I could answer these questions...',\n",
       "  'Awesome questions. By the way I knew all the answers, if these were the questions for my interview, I would have rocked it ',\n",
       "  'Can you please answers also in last ....and what mistake we does in interview will be more help ful  thanks awesome question  love from rajsthan ',\n",
       "  'What are questions for seniors persons ?',\n",
       "  'Even though I have less than 1.5 years of real job experience on HADOOP and SPark, I know each and every question answers in detail that you asked. Oh god, I am heavily underpaid.  .: p. I need a raise lol',\n",
       "  'Could you please schedule mock interv with me?\\nI am better than her!',\n",
       "  'awesome! interview question can you please provide me more real-time questions for the Hadoop admin',\n",
       "  'Awesome',\n",
       "  '\"it will AAstoreeeee\" is the best ',\n",
       "  'Hive does not support multiline comment',\n",
       "  'Kudos brother, this helped me with my interview ',\n",
       "  \"It's wonderful for people who r going to attend interview in spark realy helpful...thanks alot dear\",\n",
       "  'Can you please answer? Why cache when there is persist?',\n",
       "  'Count of like in video ?',\n",
       "  'Can you also please tell the answers ex : in case of Hadoop block size is 128 MB , if Hdfs replaced by s3 as a storage then what will be the block size in s3?',\n",
       "  'Please hide the video as posture is inappropriate. you can cast audio',\n",
       "  'thankyou for posting this video',\n",
       "  'Vectorization explained wrong',\n",
       "  'What the exact ans to the Ques:\\nWhen we do a coalesce to 1 and the data is greater than the block size , then how many files will be written?',\n",
       "  'Can u plz tell me How many years of Experiance does he has?',\n",
       "  'this kind of questions can we expect for 2-3 years of experienced people  or any higher level?',\n",
       "  'Nice one',\n",
       "  'He is blabbering..',\n",
       "  'I would like to attend ur interview I have emailed to you but there is no response. Could you please respond.',\n",
       "  'Awesome interview.',\n",
       "  'this guy is struggling a lot',\n",
       "  'How many years of experience candidate had?',\n",
       "  'Mostly your uploading hadoop development,  can you upload for admin part also , if already uploaded please share link',\n",
       "  'Very nice interview.. cna you schedule mock interview for me as well if possible..?',\n",
       "  'really talented candidate',\n",
       "  'Default blocksize in lfs is 4 kb',\n",
       "  'Wo banda hushar tha.',\n",
       "  'Sir\\n.I have an experience of 2.5 years as a Manual test engineer but rather than automation I am thinking to switching to big data testing therefore I end up doing a certification course as well.How should i apply for interviews.',\n",
       "  'How to apply for mock interview',\n",
       "  'Very nice content',\n",
       "  'Hdfs for 128mb and local for 32mb',\n",
       "  'Good one',\n",
       "  'Use rank functions to delete duplicate',\n",
       "  'could you please provide feedback on this interview?',\n",
       "  'Plz give me answer of  \"100 mb file and read the contents of this file and write it into another file 5 times\" \\nHow we can write 5 times...?',\n",
       "  \"Guys, pls consider only questions but dont trust the answers, 95% answers are worng, It's not criticising anyone just warning the people who are preparing for interviews.\",\n",
       "  'Please post interviews on data integration & Warehousing. I wanted to give demo interview is there any possibility to do that.',\n",
       "  'Is this telephonic interview?',\n",
       "  'very very easy interview till now i hv faced',\n",
       "  'Superb',\n",
       "  'Please send your resume to shareit2904@gmail.com',\n",
       "  'I want to give demo interview for hadoop/spark developer',\n",
       "  'Hlo sir\\nPreparing for interview with 2 years of experience\\nCan i have recording videos for 2 years',\n",
       "  'Goood stuff...may i know the verdict of this interview',\n",
       "  'Voice not clear',\n",
       "  'Voice is not clear.',\n",
       "  'Very informative. I love your channel. Thank you so much. Please make a video for latest resume preparation as well.',\n",
       "  'I wish I had the confidence she has.',\n",
       "  'Thank you',\n",
       "  \"She doesn't have real time experience on Hadoop technologies. She have taken coaching and attended interview.\",\n",
       "  'What you think will the interviewee get selected ?',\n",
       "  'There is no snowflake in this interview please change title',\n",
       "  'voice is not audible properly',\n",
       "  'Great work',\n",
       "  'Awesome  Very useful',\n",
       "  'I find that providing the feedback part is more informative, Please continue to share the feedback also along with mock interviews in the next interviews as well. Thanks a lot... really helpful.',\n",
       "  'sir you have snowflake interview please upload it ..please',\n",
       "  'datawarehouse  interviewer needs more practice on how to frame questions and answers',\n",
       "  'Concept is not clear of this guy.',\n",
       "  'was he selected or not?',\n",
       "  'i want to connect with the person who took mock interview. he is really amazing.',\n",
       "  'really useful video',\n",
       "  \"Looks candidate don't have the working experience. He has learned and explained\",\n",
       "  'Thank you Clever Studies.This one  helped to know about   explaining project architecture.Looking for more...',\n",
       "  'Is this a real interview recording ?or just a mock ?',\n",
       "  'Clever Studies, could anyone list down all the questions asked in the interview?',\n",
       "  'Hive pretend the normal dataset as a relational database but it is de-normalized deepdown.So, the queries are not fast and efficient.',\n",
       "  'can we keep it more difficult? as subject interview is for teach lead but the questions is more like for  2-3 year experience guy. Can you please ask more deeper and coding related questions as well....',\n",
       "  'Once again very helpful stuff.. \\nthanks a lot..',\n",
       "  'Did the candidate cleared the interview successfully?',\n",
       "  'We need more like this. This was a great conversation. Thank you!',\n",
       "  'nice sir please do have more real time questions from daily activities what the candidate does',\n",
       "  'Hello sir can you please take my mock interview ...',\n",
       "  'Great interview',\n",
       "  'better for interview preparation.',\n",
       "  'It would be great if u could post the answer of the question.',\n",
       "  'How many years of experience the student has??',\n",
       "  'Hello Sir Can you please take my mock interview on the same ,if yes so please share your mail id where I can share my latest CV. Having total 11 years of IT experience & out of 11 having 4 years of big data & 7 years of Oracle pl SQL experience.',\n",
       "  'Is the preparation good enough for a 5 yr experienced guy?',\n",
       "  'Very nice.. good and fast paced',\n",
       "  'How to build jar files in spark and why?',\n",
       "  \"If we try to increase partitions using Coalesce it will only take the default value which is 200.\\n\\nMost people feel shuffle is bad and we should be using Coalesce more often than repartition. No! \\nSpark works fine when the partitions sizes are even, Coalesce might give an uneven sized partitions and that would impact the performance of the job(not always!). \\n\\nIt's a trade-off and we have chose depend on the need.\",\n",
       "  'He was telling about spark actual memory usage toll in 9th min to 11min). Can someone tell toll the name.',\n",
       "  \"1. exclude tables,  --exclude-tables flag\\n2. splittable colums to split the partitons on intbased columns\\n3. repartion can increase and decrease the columns and does full shuffle. Coalesce avoids full shuffle and better for reduced number of partitions\\n4. If spark.default.parallelism is set, we'll use the value of SparkContext defaultParallelism as the default partitions number, otherwise we'll use the max number of upstream partitions.\\n5.  I guess they are talking about DERBY server.\",\n",
       "  'Excellent.',\n",
       "  'Very Helpful , Excellent',\n",
       "  'Very helpful',\n",
       "  'Good knowledge for 1 year exp',\n",
       "  '@Clever Studies : why is hdfs blocksize is set to 128mb ?',\n",
       "  'Q',\n",
       "  'I just completed my course but I could answer these questions...',\n",
       "  'Awesome questions. By the way I knew all the answers, if these were the questions for my interview, I would have rocked it ',\n",
       "  'Can you please answers also in last ....and what mistake we does in interview will be more help ful  thanks awesome question  love from rajsthan ',\n",
       "  'What are questions for seniors persons ?',\n",
       "  'Even though I have less than 1.5 years of real job experience on HADOOP and SPark, I know each and every question answers in detail that you asked. Oh god, I am heavily underpaid.  .: p. I need a raise lol',\n",
       "  'Could you please schedule mock interv with me?\\nI am better than her!',\n",
       "  'awesome! interview question can you please provide me more real-time questions for the Hadoop admin',\n",
       "  'Awesome',\n",
       "  '\"it will AAstoreeeee\" is the best ',\n",
       "  'Hive does not support multiline comment',\n",
       "  'Kudos brother, this helped me with my interview ',\n",
       "  \"It's wonderful for people who r going to attend interview in spark realy helpful...thanks alot dear\",\n",
       "  'Can you please answer? Why cache when there is persist?',\n",
       "  'Count of like in video ?',\n",
       "  'Can you also please tell the answers ex : in case of Hadoop block size is 128 MB , if Hdfs replaced by s3 as a storage then what will be the block size in s3?',\n",
       "  'Please hide the video as posture is inappropriate. you can cast audio',\n",
       "  'thankyou for posting this video',\n",
       "  'Vectorization explained wrong',\n",
       "  'What the exact ans to the Ques:\\nWhen we do a coalesce to 1 and the data is greater than the block size , then how many files will be written?',\n",
       "  'Can u plz tell me How many years of Experiance does he has?',\n",
       "  'this kind of questions can we expect for 2-3 years of experienced people  or any higher level?',\n",
       "  'Nice one',\n",
       "  'He is blabbering..',\n",
       "  'I would like to attend ur interview I have emailed to you but there is no response. Could you please respond.',\n",
       "  'Awesome interview.',\n",
       "  'this guy is struggling a lot',\n",
       "  'How many years of experience candidate had?',\n",
       "  'Mostly your uploading hadoop development,  can you upload for admin part also , if already uploaded please share link',\n",
       "  'Very nice interview.. cna you schedule mock interview for me as well if possible..?',\n",
       "  'really talented candidate',\n",
       "  'Default blocksize in lfs is 4 kb',\n",
       "  'Wo banda hushar tha.',\n",
       "  'Sir\\n.I have an experience of 2.5 years as a Manual test engineer but rather than automation I am thinking to switching to big data testing therefore I end up doing a certification course as well.How should i apply for interviews.',\n",
       "  'How to apply for mock interview',\n",
       "  'Very nice content',\n",
       "  'Hdfs for 128mb and local for 32mb',\n",
       "  'Good one',\n",
       "  'Use rank functions to delete duplicate',\n",
       "  'could you please provide feedback on this interview?',\n",
       "  'Plz give me answer of  \"100 mb file and read the contents of this file and write it into another file 5 times\" \\nHow we can write 5 times...?',\n",
       "  \"Guys, pls consider only questions but dont trust the answers, 95% answers are worng, It's not criticising anyone just warning the people who are preparing for interviews.\",\n",
       "  'Please post interviews on data integration & Warehousing. I wanted to give demo interview is there any possibility to do that.',\n",
       "  'Is this telephonic interview?',\n",
       "  'very very easy interview till now i hv faced',\n",
       "  'Superb',\n",
       "  'Please send your resume to shareit2904@gmail.com',\n",
       "  'I want to give demo interview for hadoop/spark developer',\n",
       "  'Hlo sir\\nPreparing for interview with 2 years of experience\\nCan i have recording videos for 2 years',\n",
       "  'Goood stuff...may i know the verdict of this interview',\n",
       "  'Voice not clear',\n",
       "  'Voice is not clear.',\n",
       "  'Very informative. I love your channel. Thank you so much. Please make a video for latest resume preparation as well.',\n",
       "  'I wish I had the confidence she has.',\n",
       "  'Thank you',\n",
       "  \"She doesn't have real time experience on Hadoop technologies. She have taken coaching and attended interview.\",\n",
       "  'What you think will the interviewee get selected ?',\n",
       "  'There is no snowflake in this interview please change title',\n",
       "  'voice is not audible properly',\n",
       "  'Great work',\n",
       "  'Awesome  Very useful',\n",
       "  'I find that providing the feedback part is more informative, Please continue to share the feedback also along with mock interviews in the next interviews as well. Thanks a lot... really helpful.',\n",
       "  'sir you have snowflake interview please upload it ..please',\n",
       "  'datawarehouse  interviewer needs more practice on how to frame questions and answers',\n",
       "  'Concept is not clear of this guy.',\n",
       "  'was he selected or not?',\n",
       "  'i want to connect with the person who took mock interview. he is really amazing.',\n",
       "  'really useful video',\n",
       "  \"Looks candidate don't have the working experience. He has learned and explained\",\n",
       "  'Thank you Clever Studies.This one  helped to know about   explaining project architecture.Looking for more...',\n",
       "  'How to register for hadoop,spark  mock interview',\n",
       "  'next interview  1-1 hour 3 interviews but end end concept clear ful biga data , just start basics next part 2--part 3---- full big data concepts interview level finished LOL ITS TRUE CLEVER INTERVIEW FOR CLEVER PROGRAMMERS LOL STISFIED KEEP DOING GOOD WORK',\n",
       "  'thse  2 should be there  in SIR your youtube at any cost 1 sql 2 ds algo trust me if these are 4/5 certainly you can take just candidate takes 2 months beat expereinced developers of 10 years, simple after  KT --- 2months inside company task done whereas if you take without this 2 skills some or other time resource will bottleneck  if and only if  these 2 skills are not good  1 SQL  2 DS ALGO probably java relate with scala and for him python is cakewalk or else please see history who bagged above 40 lacs surely they must be very good at these 2 skills. rest 2 months inside on the job they can learn big data',\n",
       "  'sir humble request please create sql intermediate series a concept and how its asked interview ,this will help as  Source Mishra  youtube channel\\r\\nMost asked questions  \\r\\nImportant DBMS & SQL Query Topics For Data Engineering Interview\\r\\n\\r\\nBasic-Medium\\r\\n\\r\\n1. Relations, Schema Design, E-R Diagrams\\r\\n2. Primary Key, Foriegn Key, Composite Key, Surrogate Key\\r\\n3. Normalization & Denormalization\\r\\n4. ACID Properties\\r\\n5. Transactions\\r\\n6. Transaction Concurrency\\r\\n7. Concurrency Control\\r\\n8. Indexing\\r\\n9. Types of Indexing\\r\\n10. DDL, DML, DCL\\r\\n11. Integrity Constraints\\r\\n12. Difference between Delete & Truncate?\\r\\n13. Which one is faster count(*) or count(1)?\\r\\n14. All type of joins\\r\\n15. Queries related to SELF JOIN\\r\\n16. What is NULL SAFE Join\\r\\n17. Queries related to Group By & Having clause\\r\\n18. Use of Where clause along with Group by\\r\\n19. CASE-WHEN statement\\r\\n20. Group by aggregation with CASE-WHEN statement\\r\\n21. Use of WITH Clause\\r\\n22. Difference between any, all & exists\\r\\n23. Difference between Union & Union All\\r\\n\\r\\nAdvance \\r\\n\\r\\n1. Difference between Views & Materialized Views\\r\\n2. When to use Stored procedures & how can we create dynamic Stored procedures\\r\\n3. Queries related to Window Functions\\r\\n   - Difference between Rank(), Row_Number(), Dense_Rank()\\r\\n   - Over clause with Partition By & Order By\\r\\n   - Aggregation in Window functions like Sum(), AVG(), MIN()\\r\\n     MAX(), FIRST_VALUES(), LAST_VALUE()\\r\\n   - Use of LAG() window function\\r\\n   - Queries for running SUM/AVG etc in Window functions\\r\\n      -> Rows Between\\r\\n      -> Unbounded preceding',\n",
       "  'Interviewer wont allow candidate to explain so much in detail and will be pissed. Answer should be precise and to the point.',\n",
       "  'I believe parquet is not default support by hive we need to extend jar...',\n",
       "  'Request you to make more such videos.',\n",
       "  'waiting for part 2 :)',\n",
       "  'Can you please tell the answer for why we need cache when persist gives the in_memory cache option. Thank you for the content.',\n",
       "  'Nice Interview madam',\n",
       "  'How is it a 3 YOE when the guy said this is his first company?',\n",
       "  'I want to be interviewed',\n",
       "  'Hi..it was really nice interview session. I have a question so could you please answer me if possible... suppose one job is running daily and some day it is taking more than expected time to execute so how can we resolve this issue and what is the couse for this???...plz give me the answer',\n",
       "  'When you submit spark appliction how it will do internally?',\n",
       "  'Interviewer himself lacks some common concepts. He was wrong about coalesce and repartition, In coalesce there is less shuffle, not explaning else the comment would become long. Also he was wrong about predicate pushdown, there are scenarios where filter would happen before join and other scenarios where join would happen before filter.',\n",
       "  'Hi sir, \\nThankyou for uploading these types of videos it is very helpful for us.\\nI have attended some of the big data interviews and from my experience i have observed that most of the question is related to use cases on spark and hive so sir it is my request with can please cover thoses types of questions in the your mock interview so that it will help us',\n",
       "  'sir azure is getting very popular  things same but i am hearing  azure ecosystem getting peopular  , core things are same  so please make vedios on azure data engineering also',\n",
       "  'In case you also take live classes for big data engineering and also  cover azure',\n",
       "  'dear sir thanks you are doing great job , please make vedio what is sequqnce we should follow your playlist to gain expertise in data engineering',\n",
       "  'create schema to create a df from unstructured rdd => val empSchema =StructType(Array(StructField(\"empid\",IntegerType,False)) then we can pass this schema into rdd',\n",
       "  'create and rdd from file => rdd=sc.textFile(\"path\")',\n",
       "  ...],\n",
       " 'Comment_author': ['sathiya jothi',\n",
       "  'RRR',\n",
       "  'Biswajit Buragohain',\n",
       "  'Shailaja',\n",
       "  'Bulldozer',\n",
       "  'Arijit Pal',\n",
       "  'Kailas Mehtre',\n",
       "  'Harshit Agrwal',\n",
       "  'Osman Ege Demiryurek',\n",
       "  'waseem bari',\n",
       "  'Sidd',\n",
       "  'Sunil Rathod',\n",
       "  'Abhishek Shrestha',\n",
       "  'Harshit Agrwal',\n",
       "  'RRR',\n",
       "  'AN Diaries',\n",
       "  'Bulldozer',\n",
       "  'Ankita Santra',\n",
       "  'RRR',\n",
       "  'g sreenivasulu',\n",
       "  'Kailas Mehtre',\n",
       "  'Harshit Agrwal',\n",
       "  'Cherukuri d08',\n",
       "  'suman',\n",
       "  'E Wong',\n",
       "  'Ange Crocker',\n",
       "  'C. P. Rai',\n",
       "  'Sravan Kumar',\n",
       "  'Sravan Kumar',\n",
       "  'Sravan Kumar',\n",
       "  'Sravan Kumar',\n",
       "  'Nagulmeera Shaik',\n",
       "  'Nagulmeera Shaik',\n",
       "  'Vivek Puurkayastha',\n",
       "  'Cherukuri d08',\n",
       "  'suman',\n",
       "  'E Wong',\n",
       "  'Ange Crocker',\n",
       "  'C. P. Rai',\n",
       "  'Sravan Kumar',\n",
       "  'Sravan Kumar',\n",
       "  'Sravan Kumar',\n",
       "  'Sravan Kumar',\n",
       "  'Nagulmeera Shaik',\n",
       "  'Nagulmeera Shaik',\n",
       "  'Vivek Puurkayastha',\n",
       "  'Mohan Sai',\n",
       "  'Sandip Tulsyan',\n",
       "  'Prabhat Gupta',\n",
       "  'Rohit Sharma',\n",
       "  'Akash Chandra',\n",
       "  'mike',\n",
       "  'The Sachinn',\n",
       "  'thakur Mukesh Singh',\n",
       "  'Sumit Dantale',\n",
       "  'Prabhat Gupta',\n",
       "  'Rohit Sharma',\n",
       "  'Akash Chandra',\n",
       "  'mike',\n",
       "  'The Sachinn',\n",
       "  'thakur Mukesh Singh',\n",
       "  'Sumit Dantale',\n",
       "  'Rohit Sharma',\n",
       "  'Mohan Sai',\n",
       "  'Shweta Bhat',\n",
       "  'Bnf Hunterr',\n",
       "  'butchi raju',\n",
       "  'venkata kamaiah',\n",
       "  'Mohan Sai',\n",
       "  'satish boddula',\n",
       "  'Syed Kamran',\n",
       "  'Robin shaw',\n",
       "  'a leela',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Commenter Dek',\n",
       "  'Robin shaw',\n",
       "  'Nikhil Waghalkar',\n",
       "  'Learn Math',\n",
       "  'Ayyub Sayyad',\n",
       "  'Muhammad Bilal',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Commenter Dek',\n",
       "  'undinti shashikumar',\n",
       "  'Atul Bisht',\n",
       "  'a leela',\n",
       "  'Logan Simpson',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'sumanta ghosh',\n",
       "  'suneel kumar',\n",
       "  'Svcc 777',\n",
       "  'prabu E',\n",
       "  'sandesh kurhade',\n",
       "  'Rajeev Hr',\n",
       "  'Pradeep Akkasali',\n",
       "  'Rahul Yeole',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Hiren Patel',\n",
       "  'Shubham Bulbule',\n",
       "  'pradeep kumar',\n",
       "  'Aritra Chatterjee',\n",
       "  'Rohit Bhawle',\n",
       "  'Sonu Kumar',\n",
       "  'Indian Engineer',\n",
       "  'p s',\n",
       "  'Mr Khan',\n",
       "  'Venkat D',\n",
       "  'Abhi Patil',\n",
       "  'Sravan Kumar',\n",
       "  'Indian',\n",
       "  'suman',\n",
       "  'Vamshi Dil',\n",
       "  'Robin shaw',\n",
       "  'a leela',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Commenter Dek',\n",
       "  'Robin shaw',\n",
       "  'Nikhil Waghalkar',\n",
       "  'Learn Math',\n",
       "  'Ayyub Sayyad',\n",
       "  'Muhammad Bilal',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Commenter Dek',\n",
       "  'undinti shashikumar',\n",
       "  'Atul Bisht',\n",
       "  'a leela',\n",
       "  'Logan Simpson',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'sumanta ghosh',\n",
       "  'suneel kumar',\n",
       "  'Svcc 777',\n",
       "  'prabu E',\n",
       "  'sandesh kurhade',\n",
       "  'Rajeev Hr',\n",
       "  'Pradeep Akkasali',\n",
       "  'Rahul Yeole',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Ravulapalli Venkata Gurnadham',\n",
       "  'Hiren Patel',\n",
       "  'Shubham Bulbule',\n",
       "  'pradeep kumar',\n",
       "  'Aritra Chatterjee',\n",
       "  'Rohit Bhawle',\n",
       "  'Sonu Kumar',\n",
       "  'Indian Engineer',\n",
       "  'p s',\n",
       "  'Mr Khan',\n",
       "  'Venkat D',\n",
       "  'Abhi Patil',\n",
       "  'Sravan Kumar',\n",
       "  'Indian',\n",
       "  'suman',\n",
       "  'Vamshi Dil',\n",
       "  'Lata patil',\n",
       "  'Avinash',\n",
       "  'Suhail Shahed Noor',\n",
       "  'Pradeep Akkasali',\n",
       "  'S.',\n",
       "  'Simran Singh',\n",
       "  'Commenter Dek',\n",
       "  'Mohan Sai',\n",
       "  'hari vardhan',\n",
       "  'Subam Sarkar',\n",
       "  'Kaushik P',\n",
       "  'CHEVURI RAMAKANTH',\n",
       "  'Rakesh Reddy',\n",
       "  'ROHITH POKALA',\n",
       "  'Govind Pare',\n",
       "  'Dunna Uday Kumar',\n",
       "  'design the life',\n",
       "  'deepa barki',\n",
       "  'J.V Prajapati',\n",
       "  'Salwa beauty',\n",
       "  'Arvind Kumar',\n",
       "  'ROHITH POKALA',\n",
       "  'pradeep kumar',\n",
       "  'Y1',\n",
       "  'Y1',\n",
       "  'fumus mustafa',\n",
       "  'Shweta Rawat',\n",
       "  'Bikers View',\n",
       "  'Michael Ghaly',\n",
       "  'Vineet Pandey',\n",
       "  'Rahul Pandit',\n",
       "  'quyen vu',\n",
       "  'Ajay Kiran Chundi',\n",
       "  'Siva Ranganath',\n",
       "  'manoj bandaru',\n",
       "  'Sathish kumar',\n",
       "  'Ramya Chinnu',\n",
       "  'Gopichand G',\n",
       "  'quyen vu',\n",
       "  'Ajay Kiran Chundi',\n",
       "  'Siva Ranganath',\n",
       "  'manoj bandaru',\n",
       "  'Sathish kumar',\n",
       "  'Ramya Chinnu',\n",
       "  'Gopichand G',\n",
       "  'CrickFall',\n",
       "  'Suraj Patil',\n",
       "  'Sriram iyer',\n",
       "  'Rakesh Reddy',\n",
       "  'Sree Raam',\n",
       "  'Relaxing Life',\n",
       "  'quyen vu',\n",
       "  'Ajay Kiran Chundi',\n",
       "  'Siva Ranganath',\n",
       "  'manoj bandaru',\n",
       "  'Sathish kumar',\n",
       "  'Ramya Chinnu',\n",
       "  'Gopichand G',\n",
       "  'CrickFall',\n",
       "  'Suraj Patil',\n",
       "  'Sriram iyer',\n",
       "  'Rakesh Reddy',\n",
       "  'Sree Raam',\n",
       "  'Relaxing Life',\n",
       "  'kirankumar reddy',\n",
       "  'reach2puneeths',\n",
       "  'Bala Tamilmani',\n",
       "  'Pradeep Kumar',\n",
       "  ' Patrick Bateman',\n",
       "  'Aniket Jadhav',\n",
       "  'quyen vu',\n",
       "  'Ajay Kiran Chundi',\n",
       "  'Siva Ranganath',\n",
       "  'manoj bandaru',\n",
       "  'Sathish kumar',\n",
       "  'Ramya Chinnu',\n",
       "  'Gopichand G',\n",
       "  'CrickFall',\n",
       "  'Suraj Patil',\n",
       "  'Sriram iyer',\n",
       "  'Rakesh Reddy',\n",
       "  'Sree Raam',\n",
       "  'Relaxing Life',\n",
       "  'kirankumar reddy',\n",
       "  'reach2puneeths',\n",
       "  'Bala Tamilmani',\n",
       "  'Pradeep Kumar',\n",
       "  ' Patrick Bateman',\n",
       "  'Aniket Jadhav',\n",
       "  'ultimo',\n",
       "  'Diksha Chourasiya',\n",
       "  'gopal chavan',\n",
       "  'ultimo',\n",
       "  'Ren Silva',\n",
       "  'Ren Silva',\n",
       "  'VR Geethika',\n",
       "  'Stan Ws',\n",
       "  'Rahul Shandilya',\n",
       "  'Rahul Shandilya',\n",
       "  'Rahul Shandilya',\n",
       "  'Pranav Wagde',\n",
       "  'Pranav Wagde',\n",
       "  'kooljnana',\n",
       "  'Sujoy Das',\n",
       "  'Aman Raj',\n",
       "  'Shruti Sharma',\n",
       "  'Pratyush Bhatt',\n",
       "  'Sathish kumar',\n",
       "  'Alejandro Corpe',\n",
       "  'ashok gupta',\n",
       "  'Ayoo Anand',\n",
       "  'Hari Bachala',\n",
       "  'kundan singh',\n",
       "  'Shreyansh Chougule',\n",
       "  'ExplodeLyrics',\n",
       "  'Ranga Reddy',\n",
       "  'p s',\n",
       "  'p s',\n",
       "  'The Sachinn',\n",
       "  'Sumit Khandwekar',\n",
       "  'A Vs',\n",
       "  'B Chery',\n",
       "  'Teja Kiran',\n",
       "  'Harsha Kiran M',\n",
       "  'Ravi Chander',\n",
       "  'ramanjeet singh',\n",
       "  'harsha royal',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Boetenkloetsen',\n",
       "  'Shital Patil',\n",
       "  'MN chashrey',\n",
       "  'SP BATHUKAMMA SONGS',\n",
       "  'alok sahoo',\n",
       "  'Aniket Jadhav',\n",
       "  'Suraj Patil',\n",
       "  'saikiran chittaluri',\n",
       "  'Rahul Pandit',\n",
       "  'Suraj Patil',\n",
       "  'Surajit Sahoo',\n",
       "  'Rahul Pandit',\n",
       "  'SPARKARTS Distinctions',\n",
       "  'Ramana Swamy',\n",
       "  'Sangramraj Pujari',\n",
       "  'N A',\n",
       "  'Sudarshan VU',\n",
       "  'Prathi Venkata Sai Pavan',\n",
       "  'KaushiK',\n",
       "  'Sagar Singh',\n",
       "  'MalluTornado',\n",
       "  'Sravan Kumar',\n",
       "  'khalid sultani',\n",
       "  'Bugs with Google',\n",
       "  'Prashant Rawat',\n",
       "  'Debasish Choudhury',\n",
       "  'kiran ugalmugale',\n",
       "  'Oumaima Ghamzour',\n",
       "  'p priya',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Ravikumar K',\n",
       "  'kumar shubham',\n",
       "  'dsk lobotus',\n",
       "  'Kundan Kumar',\n",
       "  'Saurabh Soni',\n",
       "  'Ramandeep Virk',\n",
       "  'Dataaholic',\n",
       "  'Swagatika Tripathy',\n",
       "  'Ranga Swamy',\n",
       "  'Datta B.',\n",
       "  'Jade Nguyen',\n",
       "  'Sivaji p',\n",
       "  'neyagapula siddartha',\n",
       "  'Ranga Swamy',\n",
       "  'Billcates',\n",
       "  'Shahbaaz Khan',\n",
       "  'Shahbaaz Khan',\n",
       "  'khalfa alaeddine',\n",
       "  'Jonathan Lampkin',\n",
       "  'Saurabh Soni',\n",
       "  'Lalit Alone',\n",
       "  'TARUN SHARMA',\n",
       "  'Jaime Rodriguez',\n",
       "  'anvesh reddy',\n",
       "  'Sagar Singh',\n",
       "  'MalluTornado',\n",
       "  'Sravan Kumar',\n",
       "  'khalid sultani',\n",
       "  'Bugs with Google',\n",
       "  'Prashant Rawat',\n",
       "  'Debasish Choudhury',\n",
       "  'kiran ugalmugale',\n",
       "  'Oumaima Ghamzour',\n",
       "  'p priya',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Ravikumar K',\n",
       "  'kumar shubham',\n",
       "  'dsk lobotus',\n",
       "  'Kundan Kumar',\n",
       "  'Saurabh Soni',\n",
       "  'Ramandeep Virk',\n",
       "  'Dataaholic',\n",
       "  'Swagatika Tripathy',\n",
       "  'Ranga Swamy',\n",
       "  'Datta B.',\n",
       "  'Jade Nguyen',\n",
       "  'Sivaji p',\n",
       "  'neyagapula siddartha',\n",
       "  'Ranga Swamy',\n",
       "  'Billcates',\n",
       "  'Shahbaaz Khan',\n",
       "  'Shahbaaz Khan',\n",
       "  'khalfa alaeddine',\n",
       "  'Jonathan Lampkin',\n",
       "  'Saurabh Soni',\n",
       "  'Lalit Alone',\n",
       "  'TARUN SHARMA',\n",
       "  'Jaime Rodriguez',\n",
       "  'anvesh reddy',\n",
       "  'Ukraine War Live footages',\n",
       "  'AAYUSH KU SONI',\n",
       "  'RABINDRA JAISWAL',\n",
       "  'Josh Reji',\n",
       "  'dyagala praveen',\n",
       "  \"SuryaTeja's View\",\n",
       "  'TakeItEasy',\n",
       "  'sajeev kumar',\n",
       "  'gnana ganesh Kumar',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Pamujula Rajesh',\n",
       "  'Md Mushtaq Ali',\n",
       "  'Sangramraj Pujari',\n",
       "  'Dhana K',\n",
       "  'Sangramraj Pujari',\n",
       "  'Sagar Singh',\n",
       "  'MalluTornado',\n",
       "  'Sravan Kumar',\n",
       "  'khalid sultani',\n",
       "  'Bugs with Google',\n",
       "  'Prashant Rawat',\n",
       "  'Debasish Choudhury',\n",
       "  'kiran ugalmugale',\n",
       "  'Oumaima Ghamzour',\n",
       "  'p priya',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Ravikumar K',\n",
       "  'kumar shubham',\n",
       "  'dsk lobotus',\n",
       "  'Kundan Kumar',\n",
       "  'Saurabh Soni',\n",
       "  'Ramandeep Virk',\n",
       "  'Dataaholic',\n",
       "  'Swagatika Tripathy',\n",
       "  'Ranga Swamy',\n",
       "  'Datta B.',\n",
       "  'Jade Nguyen',\n",
       "  'Sivaji p',\n",
       "  'neyagapula siddartha',\n",
       "  'Ranga Swamy',\n",
       "  'Billcates',\n",
       "  'Shahbaaz Khan',\n",
       "  'Shahbaaz Khan',\n",
       "  'khalfa alaeddine',\n",
       "  'Jonathan Lampkin',\n",
       "  'Saurabh Soni',\n",
       "  'Lalit Alone',\n",
       "  'TARUN SHARMA',\n",
       "  'Jaime Rodriguez',\n",
       "  'anvesh reddy',\n",
       "  'Ukraine War Live footages',\n",
       "  'AAYUSH KU SONI',\n",
       "  'RABINDRA JAISWAL',\n",
       "  'Josh Reji',\n",
       "  'dyagala praveen',\n",
       "  \"SuryaTeja's View\",\n",
       "  'TakeItEasy',\n",
       "  'sajeev kumar',\n",
       "  'gnana ganesh Kumar',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Gayathri Lakshmi',\n",
       "  'Pamujula Rajesh',\n",
       "  'Md Mushtaq Ali',\n",
       "  'Sangramraj Pujari',\n",
       "  'Dhana K',\n",
       "  'Sangramraj Pujari',\n",
       "  'Animesh Sen',\n",
       "  'sachin',\n",
       "  'Sandeep Sahu',\n",
       "  'raj guru',\n",
       "  'sachin',\n",
       "  'Giovanni De Cillis',\n",
       "  'Vignesh Jaisankar',\n",
       "  'Sandeep Sahu',\n",
       "  'Padala Rajendergoud',\n",
       "  'Tushar Shinde',\n",
       "  'Murtaza Badshah',\n",
       "  'Mwanthi Daniel',\n",
       "  'Santhoshi Mankala',\n",
       "  'Gokul Nath',\n",
       "  'Sangramraj Pujari',\n",
       "  'Sangramraj Pujari',\n",
       "  'Abdul Ahmed',\n",
       "  'Pramod Khandalkar',\n",
       "  'ramakrishna telugu',\n",
       "  'syed owais',\n",
       "  'Prathap Ganesh',\n",
       "  'Shamanth J',\n",
       "  'Tanushree Nagar',\n",
       "  'Akshay Dushyanth',\n",
       "  'Engineer Baaniya',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Drishti Nishchhal Sahu',\n",
       "  'Vishal Sharma',\n",
       "  'Alex Jolly',\n",
       "  'Saurabh Kr Pathak',\n",
       "  'sri ranjani',\n",
       "  'Harish B',\n",
       "  'Kiran Grandhe',\n",
       "  'ksk test',\n",
       "  'A Vs',\n",
       "  'Absar Usain',\n",
       "  'Onkar Londhe',\n",
       "  'Anek Singh',\n",
       "  'ashok gupta',\n",
       "  'Kannadiga in USA',\n",
       "  'Tejes Khandagale',\n",
       "  'Dataaholic',\n",
       "  'seymirpro',\n",
       "  'Senthilselvan',\n",
       "  'ksk test',\n",
       "  'A Vs',\n",
       "  'NIVEDITA',\n",
       "  'Onkar Londhe',\n",
       "  'Pawan Sharma',\n",
       "  'ksk test',\n",
       "  'Nallabotula Anusha',\n",
       "  'ramakrishna telugu',\n",
       "  'Sunny Raut',\n",
       "  'ksk test',\n",
       "  'random postings',\n",
       "  'LOKESH GUPTA',\n",
       "  'ramakrishna telugu',\n",
       "  'Onkar Londhe',\n",
       "  'Brewing Data',\n",
       "  'A Vs',\n",
       "  'vishal chhabra',\n",
       "  'prashanth928',\n",
       "  'Bilal',\n",
       "  'chandi priya',\n",
       "  'Gotham S',\n",
       "  'Ashwini Chandran',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Afsarahmed habib',\n",
       "  'ramakrishna telugu',\n",
       "  'Absar Usain',\n",
       "  'Prathap Ganesh',\n",
       "  'Shamanth J',\n",
       "  'Tanushree Nagar',\n",
       "  'Akshay Dushyanth',\n",
       "  'Engineer Baaniya',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Drishti Nishchhal Sahu',\n",
       "  'Vishal Sharma',\n",
       "  'Alex Jolly',\n",
       "  'Saurabh Kr Pathak',\n",
       "  'sri ranjani',\n",
       "  'Harish B',\n",
       "  'Kiran Grandhe',\n",
       "  'ksk test',\n",
       "  'A Vs',\n",
       "  'Absar Usain',\n",
       "  'Onkar Londhe',\n",
       "  'Anek Singh',\n",
       "  'ashok gupta',\n",
       "  'Kannadiga in USA',\n",
       "  'Tejes Khandagale',\n",
       "  'Dataaholic',\n",
       "  'seymirpro',\n",
       "  'Senthilselvan',\n",
       "  'ksk test',\n",
       "  'A Vs',\n",
       "  'NIVEDITA',\n",
       "  'Onkar Londhe',\n",
       "  'Pawan Sharma',\n",
       "  'ksk test',\n",
       "  'Nallabotula Anusha',\n",
       "  'ramakrishna telugu',\n",
       "  'Sunny Raut',\n",
       "  'ksk test',\n",
       "  'random postings',\n",
       "  'LOKESH GUPTA',\n",
       "  'ramakrishna telugu',\n",
       "  'Onkar Londhe',\n",
       "  'Brewing Data',\n",
       "  'A Vs',\n",
       "  'vishal chhabra',\n",
       "  'prashanth928',\n",
       "  'Bilal',\n",
       "  'chandi priya',\n",
       "  'Gotham S',\n",
       "  'Ashwini Chandran',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Afsarahmed habib',\n",
       "  'ramakrishna telugu',\n",
       "  'Absar Usain',\n",
       "  'Manideep Gupta',\n",
       "  'Sonu Singh',\n",
       "  'gobi s',\n",
       "  'Tanishq Duggal',\n",
       "  'saikiran chittaluri',\n",
       "  'Ravi Krishna',\n",
       "  'ashish kumar',\n",
       "  'Sri',\n",
       "  \"Teja'\",\n",
       "  'Kasi Viswanathan',\n",
       "  'krishna kathamrutham',\n",
       "  'Sri',\n",
       "  'Karthik Ravi',\n",
       "  'Sangramraj Pujari',\n",
       "  'Piotr Karaszewski',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Sandeep Sahu',\n",
       "  'harishkumar A',\n",
       "  'Rishu Sharma',\n",
       "  'Harshitha G',\n",
       "  'ksk test',\n",
       "  'Pratap Mamidi',\n",
       "  'Vamshi Krishna',\n",
       "  'Sangramraj Pujari',\n",
       "  'Rohit',\n",
       "  'Short Library',\n",
       "  'The Sadanand',\n",
       "  'Bhuvanesh prasad',\n",
       "  'Engineer Baaniya',\n",
       "  'Yash Patil',\n",
       "  'rahul shinde',\n",
       "  'Sagar Singh',\n",
       "  'mani deep',\n",
       "  'Vedantha S M',\n",
       "  'Ramandeep Virk',\n",
       "  'mohan barnwal',\n",
       "  \"Mr. Singh's\",\n",
       "  'ksk test',\n",
       "  'Narender Negi',\n",
       "  'A Vs',\n",
       "  'devika mohan',\n",
       "  'Ramya Chinnu',\n",
       "  'Sree Raam',\n",
       "  'Common Man',\n",
       "  'A Vs',\n",
       "  'Tanushree Nagar',\n",
       "  'Tejas Nandre',\n",
       "  'Saroj mahapatra',\n",
       "  'naveen kumar',\n",
       "  'Prathap Ganesh',\n",
       "  'Shamanth J',\n",
       "  'Tanushree Nagar',\n",
       "  'Akshay Dushyanth',\n",
       "  'Engineer Baaniya',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Drishti Nishchhal Sahu',\n",
       "  'Vishal Sharma',\n",
       "  'Alex Jolly',\n",
       "  'Saurabh Kr Pathak',\n",
       "  'sri ranjani',\n",
       "  'Harish B',\n",
       "  'Kiran Grandhe',\n",
       "  'ksk test',\n",
       "  'A Vs',\n",
       "  'Absar Usain',\n",
       "  'Onkar Londhe',\n",
       "  'Anek Singh',\n",
       "  'ashok gupta',\n",
       "  'Kannadiga in USA',\n",
       "  'Tejes Khandagale',\n",
       "  'Dataaholic',\n",
       "  'seymirpro',\n",
       "  'Senthilselvan',\n",
       "  'ksk test',\n",
       "  'A Vs',\n",
       "  'NIVEDITA',\n",
       "  'Onkar Londhe',\n",
       "  'Pawan Sharma',\n",
       "  'ksk test',\n",
       "  'Nallabotula Anusha',\n",
       "  'ramakrishna telugu',\n",
       "  'Sunny Raut',\n",
       "  'ksk test',\n",
       "  'random postings',\n",
       "  'LOKESH GUPTA',\n",
       "  'ramakrishna telugu',\n",
       "  'Onkar Londhe',\n",
       "  'Brewing Data',\n",
       "  'A Vs',\n",
       "  'vishal chhabra',\n",
       "  'prashanth928',\n",
       "  'Bilal',\n",
       "  'chandi priya',\n",
       "  'Gotham S',\n",
       "  'Ashwini Chandran',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Afsarahmed habib',\n",
       "  'ramakrishna telugu',\n",
       "  'Absar Usain',\n",
       "  'Manideep Gupta',\n",
       "  'Sonu Singh',\n",
       "  'gobi s',\n",
       "  'Tanishq Duggal',\n",
       "  'saikiran chittaluri',\n",
       "  'Ravi Krishna',\n",
       "  'ashish kumar',\n",
       "  'Sri',\n",
       "  \"Teja'\",\n",
       "  'Kasi Viswanathan',\n",
       "  'krishna kathamrutham',\n",
       "  'Sri',\n",
       "  'Karthik Ravi',\n",
       "  'Sangramraj Pujari',\n",
       "  'Piotr Karaszewski',\n",
       "  'Sangeetha Prabhagaran',\n",
       "  'Sandeep Sahu',\n",
       "  'harishkumar A',\n",
       "  'Rishu Sharma',\n",
       "  'Harshitha G',\n",
       "  'ksk test',\n",
       "  'Pratap Mamidi',\n",
       "  'Vamshi Krishna',\n",
       "  'Sangramraj Pujari',\n",
       "  'Rohit',\n",
       "  'Short Library',\n",
       "  'The Sadanand',\n",
       "  'Bhuvanesh prasad',\n",
       "  'Engineer Baaniya',\n",
       "  'Yash Patil',\n",
       "  'rahul shinde',\n",
       "  'Sagar Singh',\n",
       "  'mani deep',\n",
       "  'Vedantha S M',\n",
       "  'Ramandeep Virk',\n",
       "  'mohan barnwal',\n",
       "  \"Mr. Singh's\",\n",
       "  'ksk test',\n",
       "  'Narender Negi',\n",
       "  'A Vs',\n",
       "  'devika mohan',\n",
       "  'Ramya Chinnu',\n",
       "  'Sree Raam',\n",
       "  'Common Man',\n",
       "  'A Vs',\n",
       "  'Tanushree Nagar',\n",
       "  'Tejas Nandre',\n",
       "  'Saroj mahapatra',\n",
       "  'naveen kumar',\n",
       "  'Himanish Bhattacharjee',\n",
       "  'Himanish Bhattacharjee',\n",
       "  'Sravanthi',\n",
       "  'DataCoholic',\n",
       "  'Sowmya Reddy',\n",
       "  'India raja vlogs',\n",
       "  'Sowmya Reddy',\n",
       "  'Ichor Nichts',\n",
       "  'harsha royal',\n",
       "  'SKG',\n",
       "  'Harshal Raina',\n",
       "  'aritra mondal',\n",
       "  'Ravishankar Dwivedi',\n",
       "  'Rahul Pandit',\n",
       "  'Priyanka Gottipati',\n",
       "  'Tryfon Michalopoulos',\n",
       "  'kunuturu aravindreddy',\n",
       "  'Abdul Rehaman',\n",
       "  'Mrinmoy Das',\n",
       "  'ajit shendage',\n",
       "  'anup rai',\n",
       "  'Mahesh Gore',\n",
       "  'Dheerendra Jain',\n",
       "  'sampath kumar',\n",
       "  'Abhishek T',\n",
       "  'Awanish Kumar',\n",
       "  'Raghu S',\n",
       "  'venkadesan elangovan',\n",
       "  'krishna kishore namburi',\n",
       "  'Venkatesh Gannavarapu',\n",
       "  'Rajasekhar Reddy',\n",
       "  'Moin Khan',\n",
       "  'krish',\n",
       "  'Prathyusha Reddy',\n",
       "  'jagadish kumar',\n",
       "  'Umar Farook',\n",
       "  'Mr Geek',\n",
       "  'ankur runthala',\n",
       "  'kartik kudada',\n",
       "  'Atif Imam',\n",
       "  'AlgoTreding',\n",
       "  'zubair ahmed',\n",
       "  'Pratap Nayadkar',\n",
       "  'Akash chaudhary',\n",
       "  'Apna_Banaras',\n",
       "  'Archit R',\n",
       "  'Bhargavi Rudrakshula',\n",
       "  'Diptesh Roy',\n",
       "  'ankur runthala',\n",
       "  'Rajasekhar Reddy',\n",
       "  'AlgoTreding',\n",
       "  'Shankar Sr',\n",
       "  'All In One Tutorials',\n",
       "  'Mintu Choudhary',\n",
       "  'Narasimha Rao',\n",
       "  'Narasimha Rao',\n",
       "  'Rahul Verma',\n",
       "  'Sathish kumar',\n",
       "  'talla ravikumar',\n",
       "  'sampath kumar',\n",
       "  'satyanath parvatham',\n",
       "  'Antriksh Chandresh Gupta',\n",
       "  'I am Hetvik',\n",
       "  'vishal chhabra',\n",
       "  'Rohit Gade',\n",
       "  'satyanath parvatham',\n",
       "  'Ajinkya Hatolkar',\n",
       "  'skyfullofstars',\n",
       "  'Shb....',\n",
       "  'Akbar Munwar',\n",
       "  'Mosin Inamdar',\n",
       "  'Prabhath Kota',\n",
       "  'A Sathish',\n",
       "  'Siddhant Saxena',\n",
       "  'Datta B.',\n",
       "  'sai krishna',\n",
       "  'Sushmitha S',\n",
       "  'Uma Damineni',\n",
       "  'Abby',\n",
       "  'Anil kumar',\n",
       "  'Clever Studies',\n",
       "  'Kumar Shanu',\n",
       "  'Nanitha C',\n",
       "  'praveen raj',\n",
       "  'SKG',\n",
       "  'Harshal Raina',\n",
       "  'aritra mondal',\n",
       "  'Ravishankar Dwivedi',\n",
       "  'Rahul Pandit',\n",
       "  'Priyanka Gottipati',\n",
       "  'Tryfon Michalopoulos',\n",
       "  'kunuturu aravindreddy',\n",
       "  'Abdul Rehaman',\n",
       "  'Mrinmoy Das',\n",
       "  'ajit shendage',\n",
       "  'anup rai',\n",
       "  'Mahesh Gore',\n",
       "  'Dheerendra Jain',\n",
       "  'sampath kumar',\n",
       "  'Abhishek T',\n",
       "  'Awanish Kumar',\n",
       "  'Raghu S',\n",
       "  'venkadesan elangovan',\n",
       "  'krishna kishore namburi',\n",
       "  'Venkatesh Gannavarapu',\n",
       "  'Rajasekhar Reddy',\n",
       "  'Moin Khan',\n",
       "  'krish',\n",
       "  'Prathyusha Reddy',\n",
       "  'jagadish kumar',\n",
       "  'Umar Farook',\n",
       "  'Mr Geek',\n",
       "  'ankur runthala',\n",
       "  'kartik kudada',\n",
       "  'Atif Imam',\n",
       "  'AlgoTreding',\n",
       "  'zubair ahmed',\n",
       "  'Pratap Nayadkar',\n",
       "  'Akash chaudhary',\n",
       "  'Apna_Banaras',\n",
       "  'Archit R',\n",
       "  'Bhargavi Rudrakshula',\n",
       "  'Diptesh Roy',\n",
       "  'ankur runthala',\n",
       "  'Rajasekhar Reddy',\n",
       "  'AlgoTreding',\n",
       "  'Shankar Sr',\n",
       "  'All In One Tutorials',\n",
       "  'Mintu Choudhary',\n",
       "  'Narasimha Rao',\n",
       "  'Narasimha Rao',\n",
       "  'Rahul Verma',\n",
       "  'Sathish kumar',\n",
       "  'talla ravikumar',\n",
       "  'sampath kumar',\n",
       "  'satyanath parvatham',\n",
       "  'Antriksh Chandresh Gupta',\n",
       "  'I am Hetvik',\n",
       "  'vishal chhabra',\n",
       "  'Rohit Gade',\n",
       "  'satyanath parvatham',\n",
       "  'Ajinkya Hatolkar',\n",
       "  'skyfullofstars',\n",
       "  'Shb....',\n",
       "  'Akbar Munwar',\n",
       "  'Mosin Inamdar',\n",
       "  'Prabhath Kota',\n",
       "  'A Sathish',\n",
       "  'Siddhant Saxena',\n",
       "  'Datta B.',\n",
       "  'sai krishna',\n",
       "  'Sushmitha S',\n",
       "  'Uma Damineni',\n",
       "  'Abby',\n",
       "  'Anil kumar',\n",
       "  'Clever Studies',\n",
       "  'Kumar Shanu',\n",
       "  'Nanitha C',\n",
       "  'praveen raj',\n",
       "  'raj',\n",
       "  'Antriksh Chandresh Gupta',\n",
       "  'Devi prasad padhy',\n",
       "  'psYchE51',\n",
       "  'Naga Manickam',\n",
       "  'Praveen Kumar',\n",
       "  'Vishal Gupta',\n",
       "  'anubhav srivastava',\n",
       "  'Kiran Vijay',\n",
       "  'Love flaws ,Be bold and live life with respect',\n",
       "  'Keerthy M Ganesh',\n",
       "  'Ananth B',\n",
       "  'naveen chinde',\n",
       "  'Sonu Patel',\n",
       "  'Mani Tyagi',\n",
       "  'Me',\n",
       "  'Puneet Bhatia',\n",
       "  'Johnson Rajendran',\n",
       "  'anand kasyap',\n",
       "  'Shruthi V',\n",
       "  'SKG',\n",
       "  'Harshal Raina',\n",
       "  'aritra mondal',\n",
       "  'Ravishankar Dwivedi',\n",
       "  'Rahul Pandit',\n",
       "  'Priyanka Gottipati',\n",
       "  'Tryfon Michalopoulos',\n",
       "  'kunuturu aravindreddy',\n",
       "  'Abdul Rehaman',\n",
       "  'Mrinmoy Das',\n",
       "  'ajit shendage',\n",
       "  'anup rai',\n",
       "  'Mahesh Gore',\n",
       "  'Dheerendra Jain',\n",
       "  'sampath kumar',\n",
       "  'Abhishek T',\n",
       "  'Awanish Kumar',\n",
       "  'Raghu S',\n",
       "  'venkadesan elangovan',\n",
       "  'krishna kishore namburi',\n",
       "  'Venkatesh Gannavarapu',\n",
       "  'Rajasekhar Reddy',\n",
       "  'Moin Khan',\n",
       "  'krish',\n",
       "  'Prathyusha Reddy',\n",
       "  'jagadish kumar',\n",
       "  'Umar Farook',\n",
       "  'Mr Geek',\n",
       "  'ankur runthala',\n",
       "  'kartik kudada',\n",
       "  'Atif Imam',\n",
       "  'AlgoTreding',\n",
       "  'zubair ahmed',\n",
       "  'Pratap Nayadkar',\n",
       "  'Akash chaudhary',\n",
       "  'Apna_Banaras',\n",
       "  'Archit R',\n",
       "  'Bhargavi Rudrakshula',\n",
       "  'Diptesh Roy',\n",
       "  'ankur runthala',\n",
       "  'Rajasekhar Reddy',\n",
       "  'AlgoTreding',\n",
       "  'Shankar Sr',\n",
       "  'All In One Tutorials',\n",
       "  'Mintu Choudhary',\n",
       "  'Narasimha Rao',\n",
       "  'Narasimha Rao',\n",
       "  'Rahul Verma',\n",
       "  'Sathish kumar',\n",
       "  'talla ravikumar',\n",
       "  'sampath kumar',\n",
       "  'satyanath parvatham',\n",
       "  'Antriksh Chandresh Gupta',\n",
       "  'I am Hetvik',\n",
       "  'vishal chhabra',\n",
       "  'Rohit Gade',\n",
       "  'satyanath parvatham',\n",
       "  'Ajinkya Hatolkar',\n",
       "  'skyfullofstars',\n",
       "  'Shb....',\n",
       "  'Akbar Munwar',\n",
       "  'Mosin Inamdar',\n",
       "  'Prabhath Kota',\n",
       "  'A Sathish',\n",
       "  'Siddhant Saxena',\n",
       "  'Datta B.',\n",
       "  'sai krishna',\n",
       "  'Sushmitha S',\n",
       "  'Uma Damineni',\n",
       "  'Abby',\n",
       "  'Anil kumar',\n",
       "  'Clever Studies',\n",
       "  'Kumar Shanu',\n",
       "  'Nanitha C',\n",
       "  'praveen raj',\n",
       "  'raj',\n",
       "  'Antriksh Chandresh Gupta',\n",
       "  'Devi prasad padhy',\n",
       "  'psYchE51',\n",
       "  'Naga Manickam',\n",
       "  'Praveen Kumar',\n",
       "  'Vishal Gupta',\n",
       "  'anubhav srivastava',\n",
       "  'Kiran Vijay',\n",
       "  'Love flaws ,Be bold and live life with respect',\n",
       "  'Keerthy M Ganesh',\n",
       "  'Ananth B',\n",
       "  'naveen chinde',\n",
       "  'Sonu Patel',\n",
       "  'Mani Tyagi',\n",
       "  'Me',\n",
       "  'Puneet Bhatia',\n",
       "  'Johnson Rajendran',\n",
       "  'anand kasyap',\n",
       "  'Shruthi V',\n",
       "  'butchi raju',\n",
       "  'ramesh roshan',\n",
       "  'ramesh roshan',\n",
       "  'ramesh roshan',\n",
       "  'Diptesh Roy',\n",
       "  'rupesh singh',\n",
       "  'ratanUtube1',\n",
       "  'ratanUtube1',\n",
       "  'Komali G',\n",
       "  'PRASAD KAVURI',\n",
       "  'Somyya Chaudhary',\n",
       "  'Rohit Saini',\n",
       "  'Love Animals',\n",
       "  'Sri',\n",
       "  'Manish Pathak',\n",
       "  'Ankur',\n",
       "  'reema sharma',\n",
       "  'reema sharma',\n",
       "  'reema sharma',\n",
       "  'The Tech Travel Geeks | MR. Rai',\n",
       "  'The Tech Travel Geeks | MR. Rai',\n",
       "  ...],\n",
       " 'Comment_date': ['07/11/2023 12:17 PM',\n",
       "  '06/11/2023 12:25 PM',\n",
       "  '01/11/2023 04:12 PM',\n",
       "  '27/10/2023 04:12 PM',\n",
       "  '26/10/2023 02:57 AM',\n",
       "  '23/10/2023 05:27 AM',\n",
       "  '22/10/2023 05:37 PM',\n",
       "  '22/10/2023 01:23 PM',\n",
       "  '22/10/2023 01:01 PM',\n",
       "  '31/10/2023 02:39 PM',\n",
       "  '05/11/2023 09:17 AM',\n",
       "  '04/11/2023 01:38 PM',\n",
       "  '03/11/2023 02:00 PM',\n",
       "  '03/11/2023 05:51 AM',\n",
       "  '03/11/2023 03:10 AM',\n",
       "  '03/11/2023 01:05 AM',\n",
       "  '02/11/2023 05:21 PM',\n",
       "  '07/11/2023 06:10 PM',\n",
       "  '04/11/2023 04:46 AM',\n",
       "  '03/11/2023 03:33 PM',\n",
       "  '03/11/2023 02:52 PM',\n",
       "  '03/11/2023 01:52 PM',\n",
       "  '13/06/2023 02:25 AM',\n",
       "  '24/01/2023 06:02 AM',\n",
       "  '16/04/2023 04:33 AM',\n",
       "  '24/03/2023 07:56 AM',\n",
       "  '12/02/2023 03:27 PM',\n",
       "  '19/03/2023 06:53 AM',\n",
       "  '19/03/2023 07:11 AM',\n",
       "  '01/03/2023 04:50 PM',\n",
       "  '19/03/2023 05:21 AM',\n",
       "  '16/03/2023 04:50 PM',\n",
       "  '16/03/2023 04:50 PM',\n",
       "  '23/03/2023 05:53 PM',\n",
       "  '13/06/2023 02:25 AM',\n",
       "  '24/01/2023 06:02 AM',\n",
       "  '16/04/2023 04:33 AM',\n",
       "  '24/03/2023 07:56 AM',\n",
       "  '12/02/2023 03:27 PM',\n",
       "  '19/03/2023 06:53 AM',\n",
       "  '19/03/2023 07:11 AM',\n",
       "  '01/03/2023 04:50 PM',\n",
       "  '19/03/2023 05:21 AM',\n",
       "  '16/03/2023 04:50 PM',\n",
       "  '16/03/2023 04:50 PM',\n",
       "  '23/03/2023 05:53 PM',\n",
       "  '29/03/2023 12:37 PM',\n",
       "  '02/07/2023 06:22 PM',\n",
       "  '15/05/2023 02:15 PM',\n",
       "  '08/04/2023 10:29 AM',\n",
       "  '11/02/2023 06:46 PM',\n",
       "  '02/03/2023 01:16 PM',\n",
       "  '01/03/2023 04:40 PM',\n",
       "  '14/03/2023 04:45 PM',\n",
       "  '06/03/2023 05:32 AM',\n",
       "  '15/05/2023 02:15 PM',\n",
       "  '08/04/2023 10:29 AM',\n",
       "  '11/02/2023 06:46 PM',\n",
       "  '02/03/2023 01:16 PM',\n",
       "  '01/03/2023 04:40 PM',\n",
       "  '14/03/2023 04:45 PM',\n",
       "  '06/03/2023 05:32 AM',\n",
       "  '10/04/2023 09:20 AM',\n",
       "  '29/03/2023 12:40 PM',\n",
       "  '28/03/2023 04:50 AM',\n",
       "  '03/04/2023 04:55 PM',\n",
       "  '14/07/2023 07:50 PM',\n",
       "  '09/04/2023 10:57 AM',\n",
       "  '09/04/2023 07:00 AM',\n",
       "  '27/06/2023 02:51 PM',\n",
       "  '06/11/2022 02:02 PM',\n",
       "  '21/09/2023 07:16 PM',\n",
       "  '18/07/2022 05:13 AM',\n",
       "  '16/07/2022 05:21 PM',\n",
       "  '04/07/2022 10:13 PM',\n",
       "  '21/09/2023 07:48 PM',\n",
       "  '17/09/2023 06:08 AM',\n",
       "  '31/07/2023 06:31 PM',\n",
       "  '27/07/2022 10:43 PM',\n",
       "  '25/07/2022 04:45 AM',\n",
       "  '14/07/2022 04:23 PM',\n",
       "  '10/07/2022 11:21 AM',\n",
       "  '05/02/2023 09:49 AM',\n",
       "  '13/09/2022 11:02 AM',\n",
       "  '18/07/2022 05:14 AM',\n",
       "  '05/08/2022 03:05 AM',\n",
       "  '22/07/2022 05:01 PM',\n",
       "  '21/07/2022 11:43 AM',\n",
       "  '04/08/2022 04:28 PM',\n",
       "  '08/12/2022 03:08 PM',\n",
       "  '18/09/2022 04:51 PM',\n",
       "  '20/08/2022 01:56 AM',\n",
       "  '16/08/2022 06:19 PM',\n",
       "  '14/08/2022 11:08 PM',\n",
       "  '14/08/2022 05:57 AM',\n",
       "  '14/08/2022 02:13 AM',\n",
       "  '14/08/2022 02:12 AM',\n",
       "  '25/09/2023 02:51 PM',\n",
       "  '04/06/2023 04:26 PM',\n",
       "  '31/08/2022 07:46 AM',\n",
       "  '10/09/2023 11:52 AM',\n",
       "  '26/07/2023 01:25 PM',\n",
       "  '10/07/2023 06:16 PM',\n",
       "  '11/02/2023 07:29 PM',\n",
       "  '06/12/2022 04:47 PM',\n",
       "  '03/12/2022 07:34 PM',\n",
       "  '25/11/2022 06:45 AM',\n",
       "  '22/11/2022 07:35 AM',\n",
       "  '21/11/2022 09:57 AM',\n",
       "  '20/11/2022 05:52 PM',\n",
       "  '20/11/2022 02:15 PM',\n",
       "  '27/05/2023 02:58 AM',\n",
       "  '21/09/2023 07:16 PM',\n",
       "  '18/07/2022 05:13 AM',\n",
       "  '16/07/2022 05:21 PM',\n",
       "  '04/07/2022 10:13 PM',\n",
       "  '21/09/2023 07:48 PM',\n",
       "  '17/09/2023 06:08 AM',\n",
       "  '31/07/2023 06:31 PM',\n",
       "  '27/07/2022 10:43 PM',\n",
       "  '25/07/2022 04:45 AM',\n",
       "  '14/07/2022 04:23 PM',\n",
       "  '10/07/2022 11:21 AM',\n",
       "  '05/02/2023 09:49 AM',\n",
       "  '13/09/2022 11:02 AM',\n",
       "  '18/07/2022 05:14 AM',\n",
       "  '05/08/2022 03:05 AM',\n",
       "  '22/07/2022 05:01 PM',\n",
       "  '21/07/2022 11:43 AM',\n",
       "  '04/08/2022 04:28 PM',\n",
       "  '08/12/2022 03:08 PM',\n",
       "  '18/09/2022 04:51 PM',\n",
       "  '20/08/2022 01:56 AM',\n",
       "  '16/08/2022 06:19 PM',\n",
       "  '14/08/2022 11:08 PM',\n",
       "  '14/08/2022 05:57 AM',\n",
       "  '14/08/2022 02:13 AM',\n",
       "  '14/08/2022 02:12 AM',\n",
       "  '25/09/2023 02:51 PM',\n",
       "  '04/06/2023 04:26 PM',\n",
       "  '31/08/2022 07:46 AM',\n",
       "  '10/09/2023 11:52 AM',\n",
       "  '26/07/2023 01:25 PM',\n",
       "  '10/07/2023 06:16 PM',\n",
       "  '11/02/2023 07:29 PM',\n",
       "  '06/12/2022 04:47 PM',\n",
       "  '03/12/2022 07:34 PM',\n",
       "  '25/11/2022 06:45 AM',\n",
       "  '22/11/2022 07:35 AM',\n",
       "  '21/11/2022 09:57 AM',\n",
       "  '20/11/2022 05:52 PM',\n",
       "  '20/11/2022 02:15 PM',\n",
       "  '27/05/2023 02:58 AM',\n",
       "  '22/07/2023 08:00 PM',\n",
       "  '13/03/2023 12:28 PM',\n",
       "  '03/03/2023 05:59 AM',\n",
       "  '19/01/2023 12:32 PM',\n",
       "  '19/01/2023 07:57 AM',\n",
       "  '19/01/2023 06:16 AM',\n",
       "  '21/03/2023 04:34 AM',\n",
       "  '27/02/2023 01:44 PM',\n",
       "  '22/06/2023 04:50 PM',\n",
       "  '07/08/2022 12:47 PM',\n",
       "  '07/05/2022 01:59 PM',\n",
       "  '21/06/2022 05:50 AM',\n",
       "  '15/05/2022 05:52 PM',\n",
       "  '04/06/2022 05:59 PM',\n",
       "  '29/03/2023 10:37 AM',\n",
       "  '31/12/2022 11:47 AM',\n",
       "  '20/10/2022 03:28 PM',\n",
       "  '13/08/2023 01:05 AM',\n",
       "  '19/03/2023 03:28 PM',\n",
       "  '04/08/2022 01:01 PM',\n",
       "  '21/07/2022 12:20 PM',\n",
       "  '04/06/2022 05:36 PM',\n",
       "  '02/09/2022 11:02 AM',\n",
       "  '28/01/2022 05:11 AM',\n",
       "  '28/01/2022 05:10 AM',\n",
       "  '27/01/2022 09:58 AM',\n",
       "  '09/04/2023 03:19 PM',\n",
       "  '04/08/2022 02:59 PM',\n",
       "  '27/01/2022 11:37 AM',\n",
       "  '08/02/2022 06:54 PM',\n",
       "  '16/03/2022 06:54 PM',\n",
       "  '16/03/2022 05:26 AM',\n",
       "  '29/12/2021 06:00 PM',\n",
       "  '17/12/2021 12:42 AM',\n",
       "  '16/12/2021 04:49 PM',\n",
       "  '07/08/2022 04:21 AM',\n",
       "  '05/12/2021 06:01 AM',\n",
       "  '03/12/2021 04:13 AM',\n",
       "  '16/03/2022 05:26 AM',\n",
       "  '29/12/2021 06:00 PM',\n",
       "  '17/12/2021 12:42 AM',\n",
       "  '16/12/2021 04:49 PM',\n",
       "  '07/08/2022 04:21 AM',\n",
       "  '05/12/2021 06:01 AM',\n",
       "  '03/12/2021 04:13 AM',\n",
       "  '25/10/2021 10:04 AM',\n",
       "  '06/01/2022 06:15 PM',\n",
       "  '04/03/2023 08:35 AM',\n",
       "  '16/05/2022 03:45 AM',\n",
       "  '13/10/2021 04:51 AM',\n",
       "  '08/10/2022 07:01 AM',\n",
       "  '16/03/2022 05:26 AM',\n",
       "  '29/12/2021 06:00 PM',\n",
       "  '17/12/2021 12:42 AM',\n",
       "  '16/12/2021 04:49 PM',\n",
       "  '07/08/2022 04:21 AM',\n",
       "  '05/12/2021 06:01 AM',\n",
       "  '03/12/2021 04:13 AM',\n",
       "  '25/10/2021 10:04 AM',\n",
       "  '06/01/2022 06:15 PM',\n",
       "  '04/03/2023 08:35 AM',\n",
       "  '16/05/2022 03:45 AM',\n",
       "  '13/10/2021 04:51 AM',\n",
       "  '08/10/2022 07:01 AM',\n",
       "  '28/09/2021 11:47 AM',\n",
       "  '26/09/2021 07:49 AM',\n",
       "  '10/05/2023 07:49 AM',\n",
       "  '16/09/2021 09:31 AM',\n",
       "  '19/11/2021 03:09 PM',\n",
       "  '15/10/2022 04:53 PM',\n",
       "  '16/03/2022 05:26 AM',\n",
       "  '29/12/2021 06:00 PM',\n",
       "  '17/12/2021 12:42 AM',\n",
       "  '16/12/2021 04:49 PM',\n",
       "  '07/08/2022 04:21 AM',\n",
       "  '05/12/2021 06:01 AM',\n",
       "  '03/12/2021 04:13 AM',\n",
       "  '25/10/2021 10:04 AM',\n",
       "  '06/01/2022 06:15 PM',\n",
       "  '04/03/2023 08:35 AM',\n",
       "  '16/05/2022 03:45 AM',\n",
       "  '13/10/2021 04:51 AM',\n",
       "  '08/10/2022 07:01 AM',\n",
       "  '28/09/2021 11:47 AM',\n",
       "  '26/09/2021 07:49 AM',\n",
       "  '10/05/2023 07:49 AM',\n",
       "  '16/09/2021 09:31 AM',\n",
       "  '19/11/2021 03:09 PM',\n",
       "  '15/10/2022 04:53 PM',\n",
       "  '20/09/2023 09:13 AM',\n",
       "  '01/07/2023 07:41 AM',\n",
       "  '30/06/2023 02:19 AM',\n",
       "  '25/05/2023 10:37 AM',\n",
       "  '09/02/2023 01:49 PM',\n",
       "  '08/02/2023 10:07 PM',\n",
       "  '06/01/2023 03:21 PM',\n",
       "  '15/11/2022 07:37 PM',\n",
       "  '09/09/2021 06:28 PM',\n",
       "  '09/09/2021 06:28 PM',\n",
       "  '09/09/2021 06:27 PM',\n",
       "  '30/08/2021 12:48 PM',\n",
       "  '30/08/2021 12:42 PM',\n",
       "  '26/07/2023 04:11 AM',\n",
       "  '10/07/2023 07:27 AM',\n",
       "  '13/04/2023 04:46 PM',\n",
       "  '23/12/2022 04:22 PM',\n",
       "  '18/09/2022 11:03 AM',\n",
       "  '09/08/2022 02:42 AM',\n",
       "  '21/07/2022 08:06 PM',\n",
       "  '04/04/2022 02:28 PM',\n",
       "  '21/02/2022 07:25 AM',\n",
       "  '07/12/2021 01:16 PM',\n",
       "  '21/11/2021 05:48 AM',\n",
       "  '08/08/2021 07:11 AM',\n",
       "  '07/08/2021 03:24 PM',\n",
       "  '07/08/2021 02:08 PM',\n",
       "  '04/11/2022 01:56 PM',\n",
       "  '04/11/2022 01:56 PM',\n",
       "  '04/02/2023 02:34 PM',\n",
       "  '23/07/2022 05:01 AM',\n",
       "  '31/01/2021 07:11 PM',\n",
       "  '01/10/2020 03:07 PM',\n",
       "  '07/09/2020 06:03 PM',\n",
       "  '07/09/2020 04:15 PM',\n",
       "  '07/09/2020 03:51 PM',\n",
       "  '01/07/2021 02:23 PM',\n",
       "  '01/07/2021 03:11 AM',\n",
       "  '30/06/2021 02:45 PM',\n",
       "  '16/03/2023 05:50 PM',\n",
       "  '03/10/2021 05:22 PM',\n",
       "  '01/10/2020 09:43 AM',\n",
       "  '23/09/2020 02:29 AM',\n",
       "  '12/09/2020 05:34 PM',\n",
       "  '21/09/2022 04:25 PM',\n",
       "  '16/12/2021 03:21 AM',\n",
       "  '05/07/2021 06:55 PM',\n",
       "  '03/07/2021 07:30 AM',\n",
       "  '16/12/2021 03:55 AM',\n",
       "  '15/07/2021 02:05 PM',\n",
       "  '06/07/2021 02:35 PM',\n",
       "  '27/07/2021 04:12 AM',\n",
       "  '26/07/2021 04:23 PM',\n",
       "  '26/07/2021 04:08 PM',\n",
       "  '26/07/2021 03:58 PM',\n",
       "  '26/07/2021 03:45 PM',\n",
       "  '21/08/2021 09:02 PM',\n",
       "  '21/08/2021 05:18 PM',\n",
       "  '30/01/2023 01:30 PM',\n",
       "  '11/12/2022 05:45 AM',\n",
       "  '10/10/2022 04:27 AM',\n",
       "  '19/05/2022 06:55 PM',\n",
       "  '09/05/2022 10:48 AM',\n",
       "  '07/04/2022 06:00 PM',\n",
       "  '19/03/2022 03:54 AM',\n",
       "  '23/12/2022 12:49 PM',\n",
       "  '13/07/2021 06:12 PM',\n",
       "  '09/06/2021 07:58 AM',\n",
       "  '08/06/2021 03:50 AM',\n",
       "  '07/06/2021 03:13 PM',\n",
       "  '07/04/2023 08:24 PM',\n",
       "  '23/05/2022 12:22 PM',\n",
       "  '05/05/2022 05:03 PM',\n",
       "  '22/04/2022 03:33 PM',\n",
       "  '11/01/2022 05:36 PM',\n",
       "  '23/05/2021 11:03 AM',\n",
       "  '05/05/2021 07:11 AM',\n",
       "  '01/05/2021 12:49 PM',\n",
       "  '19/04/2021 04:15 PM',\n",
       "  '10/03/2023 12:29 AM',\n",
       "  '04/05/2021 02:13 PM',\n",
       "  '09/07/2022 12:29 PM',\n",
       "  '08/05/2021 02:13 PM',\n",
       "  '15/07/2023 06:39 AM',\n",
       "  '18/09/2023 09:12 PM',\n",
       "  '18/09/2023 09:11 PM',\n",
       "  '07/06/2023 03:25 PM',\n",
       "  '15/10/2022 02:32 PM',\n",
       "  '25/04/2022 04:50 PM',\n",
       "  '28/01/2022 05:12 PM',\n",
       "  '20/09/2021 03:16 PM',\n",
       "  '05/10/2022 09:55 PM',\n",
       "  '13/04/2022 03:09 PM',\n",
       "  '30/01/2023 01:30 PM',\n",
       "  '11/12/2022 05:45 AM',\n",
       "  '10/10/2022 04:27 AM',\n",
       "  '19/05/2022 06:55 PM',\n",
       "  '09/05/2022 10:48 AM',\n",
       "  '07/04/2022 06:00 PM',\n",
       "  '19/03/2022 03:54 AM',\n",
       "  '23/12/2022 12:49 PM',\n",
       "  '13/07/2021 06:12 PM',\n",
       "  '09/06/2021 07:58 AM',\n",
       "  '08/06/2021 03:50 AM',\n",
       "  '07/06/2021 03:13 PM',\n",
       "  '07/04/2023 08:24 PM',\n",
       "  '23/05/2022 12:22 PM',\n",
       "  '05/05/2022 05:03 PM',\n",
       "  '22/04/2022 03:33 PM',\n",
       "  '11/01/2022 05:36 PM',\n",
       "  '23/05/2021 11:03 AM',\n",
       "  '05/05/2021 07:11 AM',\n",
       "  '01/05/2021 12:49 PM',\n",
       "  '19/04/2021 04:15 PM',\n",
       "  '10/03/2023 12:29 AM',\n",
       "  '04/05/2021 02:13 PM',\n",
       "  '09/07/2022 12:29 PM',\n",
       "  '08/05/2021 02:13 PM',\n",
       "  '15/07/2023 06:39 AM',\n",
       "  '18/09/2023 09:12 PM',\n",
       "  '18/09/2023 09:11 PM',\n",
       "  '07/06/2023 03:25 PM',\n",
       "  '15/10/2022 02:32 PM',\n",
       "  '25/04/2022 04:50 PM',\n",
       "  '28/01/2022 05:12 PM',\n",
       "  '20/09/2021 03:16 PM',\n",
       "  '05/10/2022 09:55 PM',\n",
       "  '13/04/2022 03:09 PM',\n",
       "  '07/07/2023 02:07 PM',\n",
       "  '05/02/2023 08:25 PM',\n",
       "  '02/04/2022 05:03 PM',\n",
       "  '07/03/2022 09:08 AM',\n",
       "  '04/06/2021 03:48 PM',\n",
       "  '04/06/2021 03:21 PM',\n",
       "  '29/11/2022 01:12 PM',\n",
       "  '31/07/2022 08:45 AM',\n",
       "  '29/06/2021 03:10 PM',\n",
       "  '22/06/2021 08:22 AM',\n",
       "  '29/06/2021 05:59 AM',\n",
       "  '08/07/2021 02:54 AM',\n",
       "  '10/07/2021 08:18 PM',\n",
       "  '10/07/2021 02:05 PM',\n",
       "  '01/02/2022 06:54 AM',\n",
       "  '12/07/2021 05:12 PM',\n",
       "  '30/01/2023 01:30 PM',\n",
       "  '11/12/2022 05:45 AM',\n",
       "  '10/10/2022 04:27 AM',\n",
       "  '19/05/2022 06:55 PM',\n",
       "  '09/05/2022 10:48 AM',\n",
       "  '07/04/2022 06:00 PM',\n",
       "  '19/03/2022 03:54 AM',\n",
       "  '23/12/2022 12:49 PM',\n",
       "  '13/07/2021 06:12 PM',\n",
       "  '09/06/2021 07:58 AM',\n",
       "  '08/06/2021 03:50 AM',\n",
       "  '07/06/2021 03:13 PM',\n",
       "  '07/04/2023 08:24 PM',\n",
       "  '23/05/2022 12:22 PM',\n",
       "  '05/05/2022 05:03 PM',\n",
       "  '22/04/2022 03:33 PM',\n",
       "  '11/01/2022 05:36 PM',\n",
       "  '23/05/2021 11:03 AM',\n",
       "  '05/05/2021 07:11 AM',\n",
       "  '01/05/2021 12:49 PM',\n",
       "  '19/04/2021 04:15 PM',\n",
       "  '10/03/2023 12:29 AM',\n",
       "  '04/05/2021 02:13 PM',\n",
       "  '09/07/2022 12:29 PM',\n",
       "  '08/05/2021 02:13 PM',\n",
       "  '15/07/2023 06:39 AM',\n",
       "  '18/09/2023 09:12 PM',\n",
       "  '18/09/2023 09:11 PM',\n",
       "  '07/06/2023 03:25 PM',\n",
       "  '15/10/2022 02:32 PM',\n",
       "  '25/04/2022 04:50 PM',\n",
       "  '28/01/2022 05:12 PM',\n",
       "  '20/09/2021 03:16 PM',\n",
       "  '05/10/2022 09:55 PM',\n",
       "  '13/04/2022 03:09 PM',\n",
       "  '07/07/2023 02:07 PM',\n",
       "  '05/02/2023 08:25 PM',\n",
       "  '02/04/2022 05:03 PM',\n",
       "  '07/03/2022 09:08 AM',\n",
       "  '04/06/2021 03:48 PM',\n",
       "  '04/06/2021 03:21 PM',\n",
       "  '29/11/2022 01:12 PM',\n",
       "  '31/07/2022 08:45 AM',\n",
       "  '29/06/2021 03:10 PM',\n",
       "  '22/06/2021 08:22 AM',\n",
       "  '29/06/2021 05:59 AM',\n",
       "  '08/07/2021 02:54 AM',\n",
       "  '10/07/2021 08:18 PM',\n",
       "  '10/07/2021 02:05 PM',\n",
       "  '01/02/2022 06:54 AM',\n",
       "  '12/07/2021 05:12 PM',\n",
       "  '16/07/2021 03:20 PM',\n",
       "  '09/07/2022 05:20 AM',\n",
       "  '25/03/2022 03:09 PM',\n",
       "  '23/07/2021 12:51 AM',\n",
       "  '13/02/2023 06:24 AM',\n",
       "  '05/10/2022 06:38 PM',\n",
       "  '26/09/2022 02:19 PM',\n",
       "  '25/03/2022 03:11 PM',\n",
       "  '02/08/2021 09:41 AM',\n",
       "  '11/04/2023 07:01 PM',\n",
       "  '17/11/2022 09:35 PM',\n",
       "  '06/03/2023 02:42 AM',\n",
       "  '16/01/2023 04:18 PM',\n",
       "  '10/08/2021 01:44 AM',\n",
       "  '09/08/2021 03:58 PM',\n",
       "  '09/08/2021 03:58 PM',\n",
       "  '12/09/2021 07:31 AM',\n",
       "  '19/08/2021 02:33 PM',\n",
       "  '19/02/2021 11:45 AM',\n",
       "  '01/04/2021 03:45 PM',\n",
       "  '02/11/2023 03:55 AM',\n",
       "  '02/08/2023 05:51 AM',\n",
       "  '06/05/2023 12:46 PM',\n",
       "  '22/12/2022 04:52 PM',\n",
       "  '16/11/2022 05:53 AM',\n",
       "  '24/08/2022 10:29 AM',\n",
       "  '28/05/2022 03:12 AM',\n",
       "  '02/05/2022 06:00 PM',\n",
       "  '01/02/2022 06:30 PM',\n",
       "  '13/01/2022 08:17 AM',\n",
       "  '10/11/2021 05:35 AM',\n",
       "  '31/05/2021 04:32 PM',\n",
       "  '11/04/2021 06:43 PM',\n",
       "  '26/02/2021 04:15 AM',\n",
       "  '31/01/2021 07:43 PM',\n",
       "  '20/12/2020 09:55 AM',\n",
       "  '18/12/2022 04:58 PM',\n",
       "  '26/09/2022 06:57 AM',\n",
       "  '04/04/2022 06:07 PM',\n",
       "  '21/07/2021 03:34 PM',\n",
       "  '23/05/2021 05:53 PM',\n",
       "  '23/05/2021 10:53 AM',\n",
       "  '18/05/2021 07:04 PM',\n",
       "  '05/04/2021 11:21 PM',\n",
       "  '26/02/2021 04:16 AM',\n",
       "  '31/01/2021 08:32 PM',\n",
       "  '07/11/2020 04:21 PM',\n",
       "  '18/12/2022 05:04 PM',\n",
       "  '22/07/2021 07:31 PM',\n",
       "  '26/02/2021 04:59 AM',\n",
       "  '25/02/2021 06:21 PM',\n",
       "  '18/02/2021 03:14 PM',\n",
       "  '15/05/2021 06:45 AM',\n",
       "  '26/02/2021 09:40 PM',\n",
       "  '12/05/2021 11:13 AM',\n",
       "  '03/05/2021 12:49 PM',\n",
       "  '18/02/2021 06:05 PM',\n",
       "  '19/12/2022 04:44 AM',\n",
       "  '02/12/2022 04:48 PM',\n",
       "  '31/01/2021 09:11 PM',\n",
       "  '24/01/2021 02:40 PM',\n",
       "  '06/02/2023 07:04 AM',\n",
       "  '13/03/2022 04:47 AM',\n",
       "  '29/10/2021 04:50 PM',\n",
       "  '12/09/2021 11:26 PM',\n",
       "  '24/06/2021 12:42 PM',\n",
       "  '31/08/2022 05:08 AM',\n",
       "  '02/02/2022 11:30 AM',\n",
       "  '18/02/2021 06:54 PM',\n",
       "  '20/12/2020 03:50 PM',\n",
       "  '02/11/2023 03:55 AM',\n",
       "  '02/08/2023 05:51 AM',\n",
       "  '06/05/2023 12:46 PM',\n",
       "  '22/12/2022 04:52 PM',\n",
       "  '16/11/2022 05:53 AM',\n",
       "  '24/08/2022 10:29 AM',\n",
       "  '28/05/2022 03:12 AM',\n",
       "  '02/05/2022 06:00 PM',\n",
       "  '01/02/2022 06:30 PM',\n",
       "  '13/01/2022 08:17 AM',\n",
       "  '10/11/2021 05:35 AM',\n",
       "  '31/05/2021 04:32 PM',\n",
       "  '11/04/2021 06:43 PM',\n",
       "  '26/02/2021 04:15 AM',\n",
       "  '31/01/2021 07:43 PM',\n",
       "  '20/12/2020 09:55 AM',\n",
       "  '18/12/2022 04:58 PM',\n",
       "  '26/09/2022 06:57 AM',\n",
       "  '04/04/2022 06:07 PM',\n",
       "  '21/07/2021 03:34 PM',\n",
       "  '23/05/2021 05:53 PM',\n",
       "  '23/05/2021 10:53 AM',\n",
       "  '18/05/2021 07:04 PM',\n",
       "  '05/04/2021 11:21 PM',\n",
       "  '26/02/2021 04:16 AM',\n",
       "  '31/01/2021 08:32 PM',\n",
       "  '07/11/2020 04:21 PM',\n",
       "  '18/12/2022 05:04 PM',\n",
       "  '22/07/2021 07:31 PM',\n",
       "  '26/02/2021 04:59 AM',\n",
       "  '25/02/2021 06:21 PM',\n",
       "  '18/02/2021 03:14 PM',\n",
       "  '15/05/2021 06:45 AM',\n",
       "  '26/02/2021 09:40 PM',\n",
       "  '12/05/2021 11:13 AM',\n",
       "  '03/05/2021 12:49 PM',\n",
       "  '18/02/2021 06:05 PM',\n",
       "  '19/12/2022 04:44 AM',\n",
       "  '02/12/2022 04:48 PM',\n",
       "  '31/01/2021 09:11 PM',\n",
       "  '24/01/2021 02:40 PM',\n",
       "  '06/02/2023 07:04 AM',\n",
       "  '13/03/2022 04:47 AM',\n",
       "  '29/10/2021 04:50 PM',\n",
       "  '12/09/2021 11:26 PM',\n",
       "  '24/06/2021 12:42 PM',\n",
       "  '31/08/2022 05:08 AM',\n",
       "  '02/02/2022 11:30 AM',\n",
       "  '18/02/2021 06:54 PM',\n",
       "  '20/12/2020 03:50 PM',\n",
       "  '06/07/2021 05:39 PM',\n",
       "  '05/02/2023 11:23 AM',\n",
       "  '04/08/2022 12:24 AM',\n",
       "  '04/06/2022 09:49 PM',\n",
       "  '26/06/2021 03:43 PM',\n",
       "  '07/01/2021 04:36 AM',\n",
       "  '05/01/2021 02:32 PM',\n",
       "  '13/11/2022 02:14 PM',\n",
       "  '08/08/2021 08:19 AM',\n",
       "  '11/01/2021 05:46 PM',\n",
       "  '23/03/2023 04:02 AM',\n",
       "  '13/11/2022 03:07 PM',\n",
       "  '12/03/2021 10:31 AM',\n",
       "  '15/01/2021 02:54 PM',\n",
       "  '17/03/2023 06:11 AM',\n",
       "  '24/08/2022 09:23 AM',\n",
       "  '13/03/2022 04:06 AM',\n",
       "  '11/07/2021 05:26 AM',\n",
       "  '04/06/2021 11:54 AM',\n",
       "  '20/04/2021 09:00 AM',\n",
       "  '06/03/2021 06:27 AM',\n",
       "  '04/02/2021 12:45 AM',\n",
       "  '20/01/2021 03:54 AM',\n",
       "  '19/01/2021 05:07 PM',\n",
       "  '05/06/2023 01:32 AM',\n",
       "  '16/03/2023 09:15 PM',\n",
       "  '08/03/2023 10:07 AM',\n",
       "  '05/12/2022 01:15 AM',\n",
       "  '15/11/2022 01:49 PM',\n",
       "  '06/11/2022 05:51 AM',\n",
       "  '05/11/2022 12:46 PM',\n",
       "  '22/10/2022 01:57 PM',\n",
       "  '29/06/2022 08:50 AM',\n",
       "  '22/04/2022 03:03 PM',\n",
       "  '15/01/2022 12:10 AM',\n",
       "  '27/11/2021 07:43 AM',\n",
       "  '24/04/2021 08:39 AM',\n",
       "  '06/03/2021 06:42 AM',\n",
       "  '10/02/2021 10:53 AM',\n",
       "  '31/01/2021 07:01 PM',\n",
       "  '13/12/2022 03:12 AM',\n",
       "  '05/12/2021 03:24 PM',\n",
       "  '30/09/2021 03:23 PM',\n",
       "  '24/07/2021 01:03 PM',\n",
       "  '31/01/2021 06:30 PM',\n",
       "  '20/03/2023 11:56 AM',\n",
       "  '03/11/2022 11:25 AM',\n",
       "  '17/04/2021 06:09 AM',\n",
       "  '28/01/2021 04:00 PM',\n",
       "  '02/11/2023 03:55 AM',\n",
       "  '02/08/2023 05:51 AM',\n",
       "  '06/05/2023 12:46 PM',\n",
       "  '22/12/2022 04:52 PM',\n",
       "  '16/11/2022 05:53 AM',\n",
       "  '24/08/2022 10:29 AM',\n",
       "  '28/05/2022 03:12 AM',\n",
       "  '02/05/2022 06:00 PM',\n",
       "  '01/02/2022 06:30 PM',\n",
       "  '13/01/2022 08:17 AM',\n",
       "  '10/11/2021 05:35 AM',\n",
       "  '31/05/2021 04:32 PM',\n",
       "  '11/04/2021 06:43 PM',\n",
       "  '26/02/2021 04:15 AM',\n",
       "  '31/01/2021 07:43 PM',\n",
       "  '20/12/2020 09:55 AM',\n",
       "  '18/12/2022 04:58 PM',\n",
       "  '26/09/2022 06:57 AM',\n",
       "  '04/04/2022 06:07 PM',\n",
       "  '21/07/2021 03:34 PM',\n",
       "  '23/05/2021 05:53 PM',\n",
       "  '23/05/2021 10:53 AM',\n",
       "  '18/05/2021 07:04 PM',\n",
       "  '05/04/2021 11:21 PM',\n",
       "  '26/02/2021 04:16 AM',\n",
       "  '31/01/2021 08:32 PM',\n",
       "  '07/11/2020 04:21 PM',\n",
       "  '18/12/2022 05:04 PM',\n",
       "  '22/07/2021 07:31 PM',\n",
       "  '26/02/2021 04:59 AM',\n",
       "  '25/02/2021 06:21 PM',\n",
       "  '18/02/2021 03:14 PM',\n",
       "  '15/05/2021 06:45 AM',\n",
       "  '26/02/2021 09:40 PM',\n",
       "  '12/05/2021 11:13 AM',\n",
       "  '03/05/2021 12:49 PM',\n",
       "  '18/02/2021 06:05 PM',\n",
       "  '19/12/2022 04:44 AM',\n",
       "  '02/12/2022 04:48 PM',\n",
       "  '31/01/2021 09:11 PM',\n",
       "  '24/01/2021 02:40 PM',\n",
       "  '06/02/2023 07:04 AM',\n",
       "  '13/03/2022 04:47 AM',\n",
       "  '29/10/2021 04:50 PM',\n",
       "  '12/09/2021 11:26 PM',\n",
       "  '24/06/2021 12:42 PM',\n",
       "  '31/08/2022 05:08 AM',\n",
       "  '02/02/2022 11:30 AM',\n",
       "  '18/02/2021 06:54 PM',\n",
       "  '20/12/2020 03:50 PM',\n",
       "  '06/07/2021 05:39 PM',\n",
       "  '05/02/2023 11:23 AM',\n",
       "  '04/08/2022 12:24 AM',\n",
       "  '04/06/2022 09:49 PM',\n",
       "  '26/06/2021 03:43 PM',\n",
       "  '07/01/2021 04:36 AM',\n",
       "  '05/01/2021 02:32 PM',\n",
       "  '13/11/2022 02:14 PM',\n",
       "  '08/08/2021 08:19 AM',\n",
       "  '11/01/2021 05:46 PM',\n",
       "  '23/03/2023 04:02 AM',\n",
       "  '13/11/2022 03:07 PM',\n",
       "  '12/03/2021 10:31 AM',\n",
       "  '15/01/2021 02:54 PM',\n",
       "  '17/03/2023 06:11 AM',\n",
       "  '24/08/2022 09:23 AM',\n",
       "  '13/03/2022 04:06 AM',\n",
       "  '11/07/2021 05:26 AM',\n",
       "  '04/06/2021 11:54 AM',\n",
       "  '20/04/2021 09:00 AM',\n",
       "  '06/03/2021 06:27 AM',\n",
       "  '04/02/2021 12:45 AM',\n",
       "  '20/01/2021 03:54 AM',\n",
       "  '19/01/2021 05:07 PM',\n",
       "  '05/06/2023 01:32 AM',\n",
       "  '16/03/2023 09:15 PM',\n",
       "  '08/03/2023 10:07 AM',\n",
       "  '05/12/2022 01:15 AM',\n",
       "  '15/11/2022 01:49 PM',\n",
       "  '06/11/2022 05:51 AM',\n",
       "  '05/11/2022 12:46 PM',\n",
       "  '22/10/2022 01:57 PM',\n",
       "  '29/06/2022 08:50 AM',\n",
       "  '22/04/2022 03:03 PM',\n",
       "  '15/01/2022 12:10 AM',\n",
       "  '27/11/2021 07:43 AM',\n",
       "  '24/04/2021 08:39 AM',\n",
       "  '06/03/2021 06:42 AM',\n",
       "  '10/02/2021 10:53 AM',\n",
       "  '31/01/2021 07:01 PM',\n",
       "  '13/12/2022 03:12 AM',\n",
       "  '05/12/2021 03:24 PM',\n",
       "  '30/09/2021 03:23 PM',\n",
       "  '24/07/2021 01:03 PM',\n",
       "  '31/01/2021 06:30 PM',\n",
       "  '20/03/2023 11:56 AM',\n",
       "  '03/11/2022 11:25 AM',\n",
       "  '17/04/2021 06:09 AM',\n",
       "  '28/01/2021 04:00 PM',\n",
       "  '07/02/2021 05:30 PM',\n",
       "  '06/02/2021 05:18 AM',\n",
       "  '05/02/2021 03:10 PM',\n",
       "  '03/04/2023 05:14 PM',\n",
       "  '13/08/2022 05:04 PM',\n",
       "  '12/06/2021 04:33 PM',\n",
       "  '15/08/2022 05:33 AM',\n",
       "  '27/07/2023 03:26 PM',\n",
       "  '13/06/2021 03:15 AM',\n",
       "  '11/11/2022 11:36 AM',\n",
       "  '04/11/2022 04:56 PM',\n",
       "  '14/05/2022 03:08 AM',\n",
       "  '17/06/2021 05:36 PM',\n",
       "  '16/06/2021 04:13 PM',\n",
       "  '16/06/2021 03:29 PM',\n",
       "  '21/09/2023 10:38 AM',\n",
       "  '12/04/2023 04:50 PM',\n",
       "  '11/09/2022 12:24 PM',\n",
       "  '08/08/2022 05:45 AM',\n",
       "  '17/06/2022 09:18 AM',\n",
       "  '26/01/2022 01:22 PM',\n",
       "  '23/12/2021 06:31 AM',\n",
       "  '24/10/2021 05:26 AM',\n",
       "  '16/09/2021 09:03 AM',\n",
       "  '15/03/2021 06:35 PM',\n",
       "  '14/02/2021 04:20 PM',\n",
       "  '10/01/2021 01:38 PM',\n",
       "  '16/12/2020 06:46 PM',\n",
       "  '23/11/2020 04:47 AM',\n",
       "  '15/11/2020 02:03 PM',\n",
       "  '12/11/2020 07:28 PM',\n",
       "  '05/11/2020 04:14 AM',\n",
       "  '06/12/2021 07:29 AM',\n",
       "  '30/11/2021 08:11 PM',\n",
       "  '23/06/2021 07:18 AM',\n",
       "  '22/11/2021 07:56 PM',\n",
       "  '18/08/2021 02:25 PM',\n",
       "  '06/06/2021 08:29 PM',\n",
       "  '19/05/2021 08:13 AM',\n",
       "  '19/12/2020 09:33 PM',\n",
       "  '02/12/2020 02:50 PM',\n",
       "  '01/12/2020 02:06 PM',\n",
       "  '18/09/2022 03:19 AM',\n",
       "  '28/11/2021 04:36 PM',\n",
       "  '05/11/2021 07:48 AM',\n",
       "  '22/10/2021 07:08 AM',\n",
       "  '15/09/2021 04:58 PM',\n",
       "  '19/07/2021 11:18 PM',\n",
       "  '08/06/2021 09:05 PM',\n",
       "  '05/05/2021 02:10 PM',\n",
       "  '09/04/2021 05:20 AM',\n",
       "  '07/03/2021 04:48 PM',\n",
       "  '11/01/2021 03:53 PM',\n",
       "  '07/12/2020 06:41 PM',\n",
       "  '06/12/2020 01:49 PM',\n",
       "  '06/12/2020 01:47 PM',\n",
       "  '06/12/2020 07:16 AM',\n",
       "  '08/08/2022 05:19 PM',\n",
       "  '02/06/2022 12:30 PM',\n",
       "  '23/09/2021 05:59 PM',\n",
       "  '22/06/2021 10:09 AM',\n",
       "  '08/05/2021 02:10 PM',\n",
       "  '14/03/2021 05:43 PM',\n",
       "  '22/01/2021 12:07 PM',\n",
       "  '15/07/2023 03:54 PM',\n",
       "  '01/10/2022 02:20 PM',\n",
       "  '27/09/2022 03:15 PM',\n",
       "  '26/08/2022 08:09 AM',\n",
       "  '20/08/2022 07:17 AM',\n",
       "  '14/07/2022 07:23 PM',\n",
       "  '06/06/2022 06:52 PM',\n",
       "  '02/02/2022 05:38 AM',\n",
       "  '07/05/2021 01:17 PM',\n",
       "  '09/03/2021 04:12 AM',\n",
       "  '23/12/2020 05:03 PM',\n",
       "  '15/09/2023 06:09 AM',\n",
       "  '25/10/2022 08:04 AM',\n",
       "  '28/07/2022 02:34 AM',\n",
       "  '09/04/2022 07:52 PM',\n",
       "  '09/03/2021 12:42 AM',\n",
       "  '06/01/2021 12:08 PM',\n",
       "  '06/01/2021 11:55 AM',\n",
       "  '24/07/2023 09:20 AM',\n",
       "  '26/01/2021 11:58 AM',\n",
       "  '11/11/2022 11:36 AM',\n",
       "  '04/11/2022 04:56 PM',\n",
       "  '14/05/2022 03:08 AM',\n",
       "  '17/06/2021 05:36 PM',\n",
       "  '16/06/2021 04:13 PM',\n",
       "  '16/06/2021 03:29 PM',\n",
       "  '21/09/2023 10:38 AM',\n",
       "  '12/04/2023 04:50 PM',\n",
       "  '11/09/2022 12:24 PM',\n",
       "  '08/08/2022 05:45 AM',\n",
       "  '17/06/2022 09:18 AM',\n",
       "  '26/01/2022 01:22 PM',\n",
       "  '23/12/2021 06:31 AM',\n",
       "  '24/10/2021 05:26 AM',\n",
       "  '16/09/2021 09:03 AM',\n",
       "  '15/03/2021 06:35 PM',\n",
       "  '14/02/2021 04:20 PM',\n",
       "  '10/01/2021 01:38 PM',\n",
       "  '16/12/2020 06:46 PM',\n",
       "  '23/11/2020 04:47 AM',\n",
       "  '15/11/2020 02:03 PM',\n",
       "  '12/11/2020 07:28 PM',\n",
       "  '05/11/2020 04:14 AM',\n",
       "  '06/12/2021 07:29 AM',\n",
       "  '30/11/2021 08:11 PM',\n",
       "  '23/06/2021 07:18 AM',\n",
       "  '22/11/2021 07:56 PM',\n",
       "  '18/08/2021 02:25 PM',\n",
       "  '06/06/2021 08:29 PM',\n",
       "  '19/05/2021 08:13 AM',\n",
       "  '19/12/2020 09:33 PM',\n",
       "  '02/12/2020 02:50 PM',\n",
       "  '01/12/2020 02:06 PM',\n",
       "  '18/09/2022 03:19 AM',\n",
       "  '28/11/2021 04:36 PM',\n",
       "  '05/11/2021 07:48 AM',\n",
       "  '22/10/2021 07:08 AM',\n",
       "  '15/09/2021 04:58 PM',\n",
       "  '19/07/2021 11:18 PM',\n",
       "  '08/06/2021 09:05 PM',\n",
       "  '05/05/2021 02:10 PM',\n",
       "  '09/04/2021 05:20 AM',\n",
       "  '07/03/2021 04:48 PM',\n",
       "  '11/01/2021 03:53 PM',\n",
       "  '07/12/2020 06:41 PM',\n",
       "  '06/12/2020 01:49 PM',\n",
       "  '06/12/2020 01:47 PM',\n",
       "  '06/12/2020 07:16 AM',\n",
       "  '08/08/2022 05:19 PM',\n",
       "  '02/06/2022 12:30 PM',\n",
       "  '23/09/2021 05:59 PM',\n",
       "  '22/06/2021 10:09 AM',\n",
       "  '08/05/2021 02:10 PM',\n",
       "  '14/03/2021 05:43 PM',\n",
       "  '22/01/2021 12:07 PM',\n",
       "  '15/07/2023 03:54 PM',\n",
       "  '01/10/2022 02:20 PM',\n",
       "  '27/09/2022 03:15 PM',\n",
       "  '26/08/2022 08:09 AM',\n",
       "  '20/08/2022 07:17 AM',\n",
       "  '14/07/2022 07:23 PM',\n",
       "  '06/06/2022 06:52 PM',\n",
       "  '02/02/2022 05:38 AM',\n",
       "  '07/05/2021 01:17 PM',\n",
       "  '09/03/2021 04:12 AM',\n",
       "  '23/12/2020 05:03 PM',\n",
       "  '15/09/2023 06:09 AM',\n",
       "  '25/10/2022 08:04 AM',\n",
       "  '28/07/2022 02:34 AM',\n",
       "  '09/04/2022 07:52 PM',\n",
       "  '09/03/2021 12:42 AM',\n",
       "  '06/01/2021 12:08 PM',\n",
       "  '06/01/2021 11:55 AM',\n",
       "  '24/07/2023 09:20 AM',\n",
       "  '26/01/2021 11:58 AM',\n",
       "  '01/05/2022 08:47 AM',\n",
       "  '31/05/2021 03:37 PM',\n",
       "  '21/01/2021 11:02 PM',\n",
       "  '25/07/2022 01:44 AM',\n",
       "  '27/04/2021 03:27 PM',\n",
       "  '19/03/2021 04:28 PM',\n",
       "  '03/02/2021 04:47 PM',\n",
       "  '22/12/2021 04:36 PM',\n",
       "  '25/05/2021 06:35 PM',\n",
       "  '07/05/2022 06:53 AM',\n",
       "  '10/06/2021 03:48 PM',\n",
       "  '31/05/2021 06:30 PM',\n",
       "  '23/04/2023 01:41 PM',\n",
       "  '24/03/2023 09:14 PM',\n",
       "  '04/11/2022 07:16 PM',\n",
       "  '30/05/2022 06:37 AM',\n",
       "  '17/12/2021 06:00 PM',\n",
       "  '18/07/2021 07:31 PM',\n",
       "  '10/06/2021 05:06 AM',\n",
       "  '09/06/2021 05:29 PM',\n",
       "  '11/11/2022 11:36 AM',\n",
       "  '04/11/2022 04:56 PM',\n",
       "  '14/05/2022 03:08 AM',\n",
       "  '17/06/2021 05:36 PM',\n",
       "  '16/06/2021 04:13 PM',\n",
       "  '16/06/2021 03:29 PM',\n",
       "  '21/09/2023 10:38 AM',\n",
       "  '12/04/2023 04:50 PM',\n",
       "  '11/09/2022 12:24 PM',\n",
       "  '08/08/2022 05:45 AM',\n",
       "  '17/06/2022 09:18 AM',\n",
       "  '26/01/2022 01:22 PM',\n",
       "  '23/12/2021 06:31 AM',\n",
       "  '24/10/2021 05:26 AM',\n",
       "  '16/09/2021 09:03 AM',\n",
       "  '15/03/2021 06:35 PM',\n",
       "  '14/02/2021 04:20 PM',\n",
       "  '10/01/2021 01:38 PM',\n",
       "  '16/12/2020 06:46 PM',\n",
       "  '23/11/2020 04:47 AM',\n",
       "  '15/11/2020 02:03 PM',\n",
       "  '12/11/2020 07:28 PM',\n",
       "  '05/11/2020 04:14 AM',\n",
       "  '06/12/2021 07:29 AM',\n",
       "  '30/11/2021 08:11 PM',\n",
       "  '23/06/2021 07:18 AM',\n",
       "  '22/11/2021 07:56 PM',\n",
       "  '18/08/2021 02:25 PM',\n",
       "  '06/06/2021 08:29 PM',\n",
       "  '19/05/2021 08:13 AM',\n",
       "  '19/12/2020 09:33 PM',\n",
       "  '02/12/2020 02:50 PM',\n",
       "  '01/12/2020 02:06 PM',\n",
       "  '18/09/2022 03:19 AM',\n",
       "  '28/11/2021 04:36 PM',\n",
       "  '05/11/2021 07:48 AM',\n",
       "  '22/10/2021 07:08 AM',\n",
       "  '15/09/2021 04:58 PM',\n",
       "  '19/07/2021 11:18 PM',\n",
       "  '08/06/2021 09:05 PM',\n",
       "  '05/05/2021 02:10 PM',\n",
       "  '09/04/2021 05:20 AM',\n",
       "  '07/03/2021 04:48 PM',\n",
       "  '11/01/2021 03:53 PM',\n",
       "  '07/12/2020 06:41 PM',\n",
       "  '06/12/2020 01:49 PM',\n",
       "  '06/12/2020 01:47 PM',\n",
       "  '06/12/2020 07:16 AM',\n",
       "  '08/08/2022 05:19 PM',\n",
       "  '02/06/2022 12:30 PM',\n",
       "  '23/09/2021 05:59 PM',\n",
       "  '22/06/2021 10:09 AM',\n",
       "  '08/05/2021 02:10 PM',\n",
       "  '14/03/2021 05:43 PM',\n",
       "  '22/01/2021 12:07 PM',\n",
       "  '15/07/2023 03:54 PM',\n",
       "  '01/10/2022 02:20 PM',\n",
       "  '27/09/2022 03:15 PM',\n",
       "  '26/08/2022 08:09 AM',\n",
       "  '20/08/2022 07:17 AM',\n",
       "  '14/07/2022 07:23 PM',\n",
       "  '06/06/2022 06:52 PM',\n",
       "  '02/02/2022 05:38 AM',\n",
       "  '07/05/2021 01:17 PM',\n",
       "  '09/03/2021 04:12 AM',\n",
       "  '23/12/2020 05:03 PM',\n",
       "  '15/09/2023 06:09 AM',\n",
       "  '25/10/2022 08:04 AM',\n",
       "  '28/07/2022 02:34 AM',\n",
       "  '09/04/2022 07:52 PM',\n",
       "  '09/03/2021 12:42 AM',\n",
       "  '06/01/2021 12:08 PM',\n",
       "  '06/01/2021 11:55 AM',\n",
       "  '24/07/2023 09:20 AM',\n",
       "  '26/01/2021 11:58 AM',\n",
       "  '01/05/2022 08:47 AM',\n",
       "  '31/05/2021 03:37 PM',\n",
       "  '21/01/2021 11:02 PM',\n",
       "  '25/07/2022 01:44 AM',\n",
       "  '27/04/2021 03:27 PM',\n",
       "  '19/03/2021 04:28 PM',\n",
       "  '03/02/2021 04:47 PM',\n",
       "  '22/12/2021 04:36 PM',\n",
       "  '25/05/2021 06:35 PM',\n",
       "  '07/05/2022 06:53 AM',\n",
       "  '10/06/2021 03:48 PM',\n",
       "  '31/05/2021 06:30 PM',\n",
       "  '23/04/2023 01:41 PM',\n",
       "  '24/03/2023 09:14 PM',\n",
       "  '04/11/2022 07:16 PM',\n",
       "  '30/05/2022 06:37 AM',\n",
       "  '17/12/2021 06:00 PM',\n",
       "  '18/07/2021 07:31 PM',\n",
       "  '10/06/2021 05:06 AM',\n",
       "  '09/06/2021 05:29 PM',\n",
       "  '10/08/2023 04:56 AM',\n",
       "  '25/06/2021 11:51 AM',\n",
       "  '25/06/2021 11:47 AM',\n",
       "  '25/06/2021 08:35 AM',\n",
       "  '23/06/2021 10:16 PM',\n",
       "  '23/06/2021 06:46 PM',\n",
       "  '23/06/2021 06:30 PM',\n",
       "  '23/06/2021 06:28 PM',\n",
       "  '21/07/2021 01:35 AM',\n",
       "  '12/07/2021 05:20 PM',\n",
       "  '06/06/2022 11:23 AM',\n",
       "  '14/01/2022 07:51 PM',\n",
       "  '28/02/2023 07:08 PM',\n",
       "  '12/11/2022 08:32 AM',\n",
       "  '30/08/2022 07:00 PM',\n",
       "  '22/06/2022 04:57 PM',\n",
       "  '18/06/2022 03:41 AM',\n",
       "  '18/06/2022 03:40 AM',\n",
       "  '18/06/2022 03:38 AM',\n",
       "  '20/12/2022 01:12 PM',\n",
       "  '20/12/2022 12:28 PM',\n",
       "  ...]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentsdetails = Commentsd(videoids)\n",
    "commentsdetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f02531ea-0e37-43bc-acb9-2b0abc8d61c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no comments\n"
     ]
    }
   ],
   "source": [
    "cmt_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId='FxofzoACcP0',\n",
    "    maxResults=50,\n",
    "    # pageToken=token\n",
    ")\n",
    "try:\n",
    "    reponse = cmt_response.execute()\n",
    "except:\n",
    "    print('no comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc7e34-86d2-40d9-b846-cb4b1392a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9038208-1564-4abb-8d36-38680e0af52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d54fb-106a-4351-86d9-c7d9f18a573d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a9674-7bab-43e6-8460-07c92900bb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f008e52-4d45-4c47-90db-a829e39276a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
